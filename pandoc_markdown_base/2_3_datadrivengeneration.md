<!-- ich kriege ein Problem - ich muss extrem oft bei required algorithms and techniques auf den algorithm vorgreifen und das macht's echt awkward (später werden wir xyz gebrauchen)..... => BASE IDEA OF THE ALGORITHM MUSS SCHON VOR DER METHODS SECTION, FOR REQUIRED ALGORITHMS, STEHEN!!! -->



Now the standard problem with conceptual spaces is that they would have to be manually generated, which of courses is a lot of work, which is where the work of \textcite{Derrac2015} comes in - to generate them in a data-driven fashion.

That Gärdenfors is often critizised as being inprecise or smth in the definition (providing no benefit bc just like before you had to crate knowledge bases now you have to create CS), but tbh hat er pretty specific instructions on how to create them computationally:
* \cite{Gardenfors2000a} also said stuff like "Dimensionality Reduction from the high-dimensional input (neurons) eg using MDS into a euclidian space, and then geometric reasoning on that" including some examples of kinds of reasoning, so actually the exact algorithm \cite{Derrac2015} did was extremely naheliegend (put some obvious NLP modelling to that like \cite{Turney2010} explained and you're pretty much exactly at their algorithm) 
	* He even has a chapter "conceptual aspects", where he suggests vector space models, dimensionality reduction algorithms, ANN architectures, ... (for all of the 3 levels)
	* ....but even more reason to make me think that it may have been their error to keep the enforcement that the MDS-result must be a euclidian space, when afterwards they have the additional step of using their rankings anyway! (...which btw brings me to the question what that does to distances?!)


#### Excursion: A CS for the subconceptual level with ANNs

* The Information received by the receptors is too rich and too unstructured, \q{What is needed is some way of transforming and organizing the input into a mode that can be ahndled on the conpcetual or symbolic level. This basically involves finding a more *economic* form of representation: going from the subconceptual to the conceptual level usually involves a *reduction of the number of dimensions* that are represented} (p221)
* \eg MDS, Shephard's algorithm of "start high-dim and then sucessively reduce dimensionality until no furhther dimensions can be limitnated without a substantial disagreement between the rank order generated by the metric assignment and the original rank order" (often no more than 2-3D)
* But we an also think about ANNs - Concreteley \q{Kohonen's (1988, 1995) *self-organizing maps*.}, that automatically \q{*reduce the representational complexity* of the input} (221) (=question "How can one generalize from single observations to general laws" on subconceptual level)
	* Self-organizing map is an ANN (with most often 2-3D array of neurons as output), that does the connections such that \q{similarities occuring among different input vectos are [...] *preserved* in the mapping, in the sense that input vectors with common features are mapped onto *neighboring* neurons in the output map. The degree of similarity between two input vectors is determined by some (instrinsitc) *distance* measure}, of which he suggested many (sec 2.4). Preserve most topological relations while making it lower-dim. 
	* That maps highdim regions to point-embeddings, and THAT IS GENERALIZATION (answer to question 2)

### What assumptions are we dropping

* euclidian metric, feddisch. (generally, a CS is not necessarily of euclidian metric, gärdenfors brings some examples of weirder metrics/topological structures)
* The Gropuing into several low-dimensional subspaces per domain is a lot weaker: actually we'd have to embed entities into small spaces of only one domain! This is only kinda picked up again later by \cite{Alshaikh2020}
    * \cite{Alshaikh2020}: "When representing a particular entity in a conceptual space, we need to specify which domains it belongs to, and for each of these domains we need to provide a corresponding vector." 
* Modelling verbs and actions in CS is a whole new chapter, but we're disregarding that
* Modified Betweeness, not as strict but rather levels of betweeeness


#### On types and tokens

Unlike many ther NLP approaches that rely on embedding (see \autoref{sec:embeddings}), in a Conceptual Space, natural language terms are not modelled as points or vectors, but as convex regions. A point in such a region is one specific instance of such a concept - you could say that regions denote \textbf{types}, with the individual points corresponding to their \textbf{tokens}. 

According to \cite{Derrac2015}, using regions instead of points has some clear advantages:
* It allows \q{to distinguish borderline instances of a concept from more prototypical instances, by taking the view that instances which are closer to the center of a region are more typical} \cite{Derrac2015} (they cite \cite{Gardenfors2000a})
* Concept Subsumption ("every pizzeria is a restaurant"), mutual exclusiveness (no restaurant can also be a beach), overlapping concepts (some bars serve wine but not all, some establishments which serve wine are bars but not all) 
* (their [41] says that "Region based models have been shown to outperform point based models in some natural language processing tasks")


In their algorithm, \cite{Derrac2015} drop this assumption and work with vectors instead of regions.
<!-- \q{In this paper, we essentially view point based representations as coarse-grained approximations of conceptual spaces, where points correspond to fine-grained categories instead of specific instances, while convex regions are used to model higher-level categories} -->
While this may seem to stand in strong contrast to an important component of the theory, 


* Computational Complexity on regions is vastly higher than for poitns or vectors
* "learning accurate region boundaries for a given concept would require a prohibitive amount of data" \cite{Derrac2015}
* Gärdenfors himself said when discussing if self-orgazinizing maps are useful that that is a GOOD THING, because that IS GENERALIZATION
* If you'd want regions, a good approach would be to just generate the type from its token. In the case of educational resources, every instance of a course is a token and thus a point/vector, and you can build your region "Introductory classes to Computer Science" by the minimal complex region that encompasses all tokens of "Informatik A" and "Introduction to Algorithmen" etc


\textbf{Conceptual Space in our Case = Euclidian space with interpretable dimensions}

### Okay, let's get to \cite{Derrac2015}

For that, the authors look at three different domains: movies, wines and places. For each of these domains, they collected many samples (like movies) together with descriptions from places where people can leave them (like reviews from IMDB). A representation of a movie is then generated from the bag-of-words of the descriptions of the individual movies, leading to a very high-dimensional, very sparse representation for all movies. 
To make the representations less sparse and more meaningful, the words in the BOW are subsequently PPMI-weighted, which weights words that appear often in the description of a particular movie while being infrequent in the corpus overall higher while setting the representation of stopwords to almost zero. 
This PPMI-weighted BOW is however not yet a euclidian space yet, which is why the authors subsequently use multidimensional scaling (MDS). MDS is a diminsionality reduction technique that attempts to create a euclidian space of lower dimensionality than the original one in which the individual distances of the items are preserved as well as possible. 

With such a space, the concepts of betweeness already makes sense, but so far, the dimensions are not interpretable. So how does one automatically find such directions? In the case of movies, good dimensions may be "scariness", "funniness", "epicness", "family-friendlyness" etc. 
To find these dimensions, the authors look for these words (as well as similar words thanks to clustering) in the reviews. Then the movies are grouped into those that contain the words from the cluster often enough vs those that don't. A support-vector-machine subsquently finds a hyperplane that best divides the two groups (eg. scary and non-scary), and the orthogonal of that hyperplane is used as one axis of the new coordinate basis. 




#### Problems with this / CS generally

* Question of how to identify and describe domains not remotely answered
* Reasioning with regions is computationally extremely demanding
* We say we're dealing with POINTS but we're constantly doing cosine similarity, isn't the important difference between points and vectors that cosine would be relevant for vectors, but euclidian(/..) distance for points?! 





Thesis goals: We want to automatically generate conceptual spaces for the domain of educational resources to generate explainable recommendation. So far, we established  that Conceptual Spaces are a good framework for that. The part that's missing is the automatic generation. The work of \cite{Derrac2015} is great because it basically does what Gärdenfors suggested using classical AI algorithms, and \cite{Ager2018} and \cite{Alshaikh2019} provided some useful additions for it without changing the main logic. So, we'll work with their algorithm, also only making small improvemenents. So the two main areas of work are implementing the original algorithm, and changing small details of it where most appropriate such that it works well for the domain we're interested in.


TALK ABOUT that actually, in CS concepts (=types) are regions, BUT we have only one-instance-per, so TOKENS, so it's kiiinda reasonable that we have points! IF we would have the collection of "ALL Computer Science 1 Courses" it would be different

% \cite{Alshaikh2019} geht drauf ein warum man infoGAN und VAEs für bilder als pretty much sowas betrachten kann

%Wie funktioniert die Idee des data-driven generieren 

% Base idea: [Derrac and Schockaert, 2015] proposed an unsupervised method which uses text descriptions of the considered entities to identify se- mantic features that can be characterized as directions. Their core assumption is that words describing semantically mean- ingful features can be identified by learning for each candi- date word w a linear classifier which separates the embed- dings of entities that have w in their description from the oth- ers. The performance of the classifier for w then tells us to what extent w describes a semantically meaningful feature. 
% This method trains for each word w in the vocab- ulary a linear classifier which predicts from the embedding of an entity whether w occurs in its description. The words w1, ..., wn for which this classifier performs sufficiently well are then used as basic features. To assess classifier perfor- mance, Cohen’s Kappa score, which can be seen as a correc- tion of classification accuracy to deal with class imbalance, is used. Each of the basic features w is associated with a cor- responding vector dw (i.e. the normal vector of the separat- ing hyperplane learned by the classifier). These directions are subsequently clustered, which serves to reduce the total num- ber of features.
% Zum Thema points vs regions: [CS] where properties and concepts are represented using convex regions, while specific instances of a concept are represented as points. This has a num- ber of important advantages. First, it allows us to distinguish borderline instances of a concept from more prototypical instances, by taking the view that instances which are closer to the center of a region are more typical [9]. A second advantage is that using regions makes it clear whether one concept subsumes another (e.g. every pizzeria is a restaurant), whether two concepts are mutually exclusive (e.g. no restaurant can also be a beach), or whether they are overlapping (e.g. some bars serve wine but not all, some establishments which serve wine are bars but not all). Region based models have been shown to outperform point based models in some natural language processing tasks [41] On the other hand, using regions is computationally more demanding, and learning accurate region boundaries for a given concept would require a prohibitive amount of data. In this paper, we essentially view point based representations as coarse-grained approximations of conceptual spaces, where points correspond to fine-grained categories instead of specific instances, while convex regions are used to model higher-level categories
%...ansonsten hätte ich immernoch die frage ob wir überhaupt points consideren oder nur vektoren, UND warum wie cosine distance consideren und nicht öfter mal euclidian distance, I mean warum ist unser space metric?!


% TODO: Have to write here:
% * that in a CS the axes correspond to human concepts, "concepts" meaning attributes and what-was-the-other-again, according to CS lingo corresponding to nouns and adjectives yadda yadda, darauf referenzier ich mich im Text



* was für teile der CS definition wir behalten und was wir droppen
    * we are only dealing with one domain (movies, placetypes, courses, ..) at a time 
        * this is like CS, but what's missing is a sort of categorization at first.. however that one is never talked about anyway
    * supposed to be regions, but we use vectors.  [TYPES are, which are MADE UP FROM TOKENS]
        * MUCH more computationally efficient
        * the domain is different than the theoretical idea of Gärdenfors - especially stuff like movies and courses, what ARE they? are they type or token? Rather both at once - the region of the course XYZ is composed of only one token, at least until we take into account a much bigger dataset that would allow us to do reasoning on "the set of all introductory computer science courses" or something. (For placetypes however we ARE dealing with types!) 
    * we ARE dealing with a mostly metric space	
        * ..however we use cosine distance instead of euclidian


Am ende soll rauskommen: A \textbf{feature-based representation}