## Relation to VSMs

Dass Word Embeddings ja relativ nah an CS sind - For example, a well-known property of word embeddings is that many syntactic and semantic relationships can be captured in terms of word vector differences [Mikolov et al., 2013].

To understand what a CS is, let's start with vector space models in general. \Glspl{vsm} represent words or generally \emph{\glspl{entity}} (such as educational resources) together with their associated properties (\eg if they are \emph{advanced}) and relevant concepts (\eg their \emph{faculty}). \Glspl{vsm} are used for information retrieval and \gls{nlp} \cite{deerwester, Lowe} since long before the dawn of modern neural embeddings like \gls{word2vec} \cite{Mikolov2013}. 

\Glspl{cs} are similar in principle, instead of embedding all natural language words (including verbs and adjectives), in \glspl{cs} there is an explicit disctinction between \emph{entities} (tokens, modelled as vectors) and their \emph{properties} and \emph{concepts} (types, modelled as regions). The domain of a \gls{cs} does not include all kinds of words or concepts, but only concepts of a certain domain (like movies or university courses). Where the regularities in the VSMs (see \autoref{eq:w2vregularity}) are only implicit, a CS explicitly models meaningful and interpretable human concepts and properties as dimensions\footnote{Which means that in a conceptual space of humans, \emph{man} may be a unit vector}, so in contrast to arbitrary dimensions that only depend on the random initial setup, there is a clearly interpretable direction for the gender, and the space itself has a clearly defined metric that allows much more geometric and arithmetic reasoning, such as \eg betweeness. Furthermore, CS are organized into multiple low-dimensionsional spaces for different facets of the domain, such that in each of those only a small set of highly correlated properties/concepts is relevant\footnote{A prime example for this is the color-domain, which may consist of the attributes \emph{hue}, \emph{saturation} and \emph{value}.}.
<!-- from \cite{Alshaikh2019}: \q{For instance, in a conceptual space of movies, we may have facets such as genre, language, geographic location, etc. Each facet is associated with its own vector space, which intuitively captures similarity \wrt the corresponding facet. Most of these facet spaces tend to be low-dimensional [...]. This clearly differentiates them from traditional semantic spaces, which often have hundreds of dimensions}) -->
Importantly, a concept in a Conceptual Spaces is not modelled as a vector or point, but as a convex region. <!-- TODO: is this mentioned? (which allows for easy extraction of is-a and part-of relations or prototypical examples vs edge examples, but makes the generation computationally vastly more expensive)--> 

<!-- 
## Do I explain well enough?

* "criterion C defines concepts as regions of conceptual spaces" \cite[111]{Gardenfors2000a}
* properties: *A property is a convex region in some domain*
* information organized in spatial structures with dimensions (color, size, shape, ...)
    * dimensions have topological or geometric structures
    * dimensions sorted into domains (h+s+v = color)
    * dimensions are human-interpretable (measurable qualities)
* Gives an extended notion of what **similarity** is (see reasoning-section)
* related to prototype theory
-->

<!-- 

#### Excursion: A CS for the subconceptual level with ANNs

* The Information received by the receptors is too rich and too unstructured, \q{What is needed is some way of transforming and organizing the input into a mode that can be ahndled on the conpcetual or symbolic level. This basically involves finding a more *economic* form of representation: going from the subconceptual to the conceptual level usually involves a *reduction of the number of dimensions* that are represented} (p221)
* \eg MDS, Shephard's algorithm of "start high-dim and then sucessively reduce dimensionality until no furhther dimensions can be limitnated without a substantial disagreement between the rank order generated by the metric assignment and the original rank order" (often no more than 2-3D)
* But we an also think about ANNs - Concreteley \q{Kohonen's (1988, 1995) *self-organizing maps*.}, that automatically \q{*reduce the representational complexity* of the input} (221) (=question "How can one generalize from single observations to general laws" on subconceptual level)
	* Self-organizing map is an ANN (with most often 2-3D array of neurons as output), that does the connections such that \q{similarities occuring among different input vectos are [...] *preserved* in the mapping, in the sense that input vectors with common features are mapped onto *neighboring* neurons in the output map. The degree of similarity between two input vectors is determined by some (instrinsitc) *distance* measure}, of which he suggested many (sec 2.4). Preserve most topological relations while making it lower-dim. 
	* That maps highdim regions to point-embeddings, and THAT IS GENERALIZATION (answer to question 2)

-->