In this section, I will outline the architecture that was developed in order to achieve the aforementioned results. Getting this architecture right was subject to a lot of trial-and-error and the end-result may look quite cumbersome, but [yadda yadda it needed to be done because of the size of the datasets and resulting runtime, and that I need to run it for many parameter-combinations but many intermediate resuls are shared, which leads to a much shorter runtime overall etc etc]

* One of the main goals of this thesis was to create a better architecture than the shit that was available from the papers I tried to re-do here (DESC15 didn't have any code, one of the others has a link to the repo but it's empty, the last one has >40 unnamed command-line-args!), and I think that was successful: This codebase contains everything and finally fulfills code-standards! It is a proper python-package (that you can run with `python -m derive_conceptualspace`), there is a working Docker-container that can be installed with one command and run on any VM even in a thousand years (fixed dependencies etc, reference a reproducibility paper!), it has workflow-management tools (reference snakemake), it can easily be installed on a grid (for SLURM I refer to https://github.com/frankier/singslurm2, but as our university still uses the sun grid engine, there are `.sge` files that allow to run the snakemake-workflow there), etc etc etc!
* Wie toll das ganze ist dass ich halt mit snakemake zig parameter-kombinationen auf dem cluster ausführen kann und mir dann mit `generate-conceptualspace show-data-info` die dabei generierten outputs (like the metrics for the clusters) angucken kann, und wenn mir die gefallen öffne ich den ganzen kram ebenfalls nochmal in nem jupyter-notebook (wo ich dank config-hash fix erkenne obs die selbe config ist), um mir dort dann bspw die überschneidungen der configs anzuschauen, haach dieser workflow

* I have MANY different parameters & param-combinations that I need to try out
* The pipeline is divisible into distinct steps
* As I often need to try different settings where only at the 8th step the relevant config is different, generating temporary files makes sense, such that I can run the stuff only once up until the 7th step and twice only for the step where it's necessary
* So this way, when I run the 8th step for the pipeline from the command-line, it figures out if all dependency are there. If they are, I can execute what I want to execute, and if not, it can automatically (using the build-system) create all dependencies - but if the config I demand differs only in the third step from something that is already there, the build system automatically only executes steps 4-7.
	* General stuff about snakemake, that the main thing about is is automatic dependency resolvement (which means I can just tell it "hey I need this file" and it creates it, which is good if I need to automatically create dependencies if I want to run step 8 with certain settings), and that it's great if you want to run multiple things at once with optimal usage of CPU/RAM and also very nice for cluster execution
		* speaking of cluster execution: Of course there's also a docker-container, so if you don't want to look at any of the code and just execute for a given dataset, you can run it on any PC with docker with the following command: [...] and for a list of all commands, it is referred to appendix xyz which lists a shitton of ways (all snakemake ones, examples of how to include a config, examples of how to look at results using the `generate-conceptualspace show-data-info` command, examples of how to look at results and intermediate results and plots in jupyterlab, ...)
	* ...but that you have to abuse it a lot to get where I want it to be (have rules that don't rely on files in the same file with rules that do, no good way to debug, apparently small community (no SO answers etc (mit quelle wie viele Fragen es gibt!!!)), the fact that I need nondynamic filenames that are set from the start of the execution)
* The build-system. Allows to build everything from the env-vars (default), to build EVERY possible combination (until a specific step), to build from config-file (where you can have lists for values, and it automatically creates all files necessary for the product of these combinations, to build for a filename, ..)

* SO, if you want to run something, you specify the parameters you want to have (using cmd-line-args, env-vars, a configfile, ... (with explicit way of resolving priorities and making sure that eg. already-used-settings are not changed lateron, ..)), and it automatically resolves which previous files it needs. Then, when loading these files in, you get a bunch of new settings from the dependencies, so again we need to resolve priorities, check for inconsistencies, etc.
	* then there are also settings (debug, randomseed) that ARE allowed to be different from now to the dependencies

* The JsonPersister!! All outputs, all created figures, all settings (including date of their use) etc is all logged, and all intermediary files also note down which dependencies THEY needed (incl. their date of creation and commit), such that going through these files is really easy, and there is of course also a command that allows to go through these files and look at their outputs etc, and of course it's also easy to load these into jupyter (in fact, that's why we need a context that is basically the same but minimally different for click, jupyter and snakemake, as all have different requirements (click: sub-commands and cmd-args (btw, why is it practical that subcommands), jupyter: automatically and easily loading as much as possible, snakemake: config-files work differently and having to do stuff using environment-variables))
* That configurations are only suggested, and their real value is (according to the priorities) only determined when they are needed, and that it fails if an already-used configuration should be overwritten with something different, or if a demanded config differs with what a dependency needed (though only IF it needed it), ...
* How I usually call it (in SIMPLE, with EXAMPLES), also von wegen ich provide einfach den pfad zu nem env-file und in dem env-file steht dann nochmal welche config er lesen soll, so ist drauf geachtet dass ich die selben settings für alle steps der pipeline hab (what wouldn't be the case if I used cmd-args), and that I can this way have fixed configs for the suggested params of eg the papers I rely on 
* PLOTS from snakemake and also from the json-persister!

* Very short subsection of this: Architecture of the fachbereich-classifier using sacred and my code (max 1/2 page!)