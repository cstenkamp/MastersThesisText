%TODO: of course we lose quality a bit, but it's only for XYZ percent of the data, and BLEU (commonly used translation metric) as well as side-by-side translation score of it is good enough

% gtranslate: talk about performance for supported languages here https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html - most descriptions are in german english or spanish, all of which are supported.
% For those langauges, it is Neural Machine Translation using Transformers (https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

% GNMT (https://arxiv.org/abs/1609.08144) is their OLD syste(2016). Since that, much progress on langauges like german or spanish (the relevant ones!) that have much training data. The currently used model has a transformer encoder and an RNN decoder (IMPORTANT PAPER: https://arxiv.org/abs/1804.09849). All relevant languagepairs use an embedding-based model (not a dictionary one, see https://arxiv.org/pdf/1807.11906.pdf)

% BLEU score, the default metric for translation, is over 50 (COMPARE TO HUMAN)
%  A popular metric for automatic quality evaluation of machine translation systems is the BLEU score, which is based on the similarity between a system’s translation and reference translations that were generated by people

% SO, https://arxiv.org/abs/1609.08144 is the orig one,  https://arxiv.org/abs/1804.09849 is the improvement.

%As is well-known, BLEU score does not fully capture the quality of a translation. For that reason we also carry out side-by-side (SxS) evaluations where we have human raters evaluate and compare the quality of two translations presented side by side for a given source sentence. Side-by-side scores range from 0 to 6, with a score of 0 meaning “completely nonsense translation”, and a score of 6 meaning “perfect translation: the meaning of the translation is completely consistent with the source, and the grammar is correct”. A translation is given a score of 4 if “the sentence retains most of the meaning of the source sentence, but may have some grammar mistakes”, and a translation is given a score of 2 if “the sentence preserves some of the meaning of the source sentence but misses significant parts”. These scores are generated by human raters who are fluent in both languages and hence often capture translation quality better than BLEU scores
