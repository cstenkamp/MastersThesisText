### Regarding Translation:

* of course we lose quality a bit, but it's only for XYZ percent of the data, and BLEU (commonly used translation metric) as well as side-by-side translation score of it is good enough
* gtranslate: talk about performance for supported languages here https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html - most descriptions are in German English or Spanish, all of which are supported.
* For those langauges, it is Neural Machine Translation using Transformers (https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
* GNMT (https://arxiv.org/abs/1609.08144) is their OLD syste(2016). Since that, much progress on langauges like German or Spanish (the relevant ones!) that have much training data. The currently used model has a transformer encoder and an RNN decoder (IMPORTANT PAPER: https://arxiv.org/abs/1804.09849). All relevant languagepairs use an embedding-based model (not a dictionary one, see https://arxiv.org/pdf/1807.11906.pdf)
* BLEU score, the default metric for translation, is over 50 (COMPARE TO HUMAN)
*  A popular metric for automatic quality evaluation of machine translation systems is the BLEU score, which is based on the similarity between a system’s translation and reference translations that were generated by people
* SO, https://arxiv.org/abs/1609.08144 is the orig one,  https://arxiv.org/abs/1804.09849 is the improvement.
* As is well-known, BLEU score does not fully capture the quality of a translation. For that reason we also carry out side-by-side (SxS) evaluations where we have human raters evaluate and compare the quality of two translations presented side by side for a given source sentence. Side-by-side scores range from 0 to 6, with a score of 0 meaning “completely nonsense translation”, and a score of 6 meaning “perfect translation: the meaning of the translation is completely consistent with the source, and the grammar is correct”. A translation is given a score of 4 if “the sentence retains most of the meaning of the source sentence, but may have some grammar mistakes”, and a translation is given a score of 2 if “the sentence preserves some of the meaning of the source sentence but misses significant parts”. These scores are generated by human raters who are fluent in both languages and hence often capture translation quality better than BLEU scores




## Left to do here (Details per step):


### Preprocessing



### Create Doc-Term-Mat & Embedding

* Possible Sanity-check here: look at the close descriptions 
    * in distance-matrix
    * in embedding

* Grains of salt: MDS-implementation of scikit-learn seems broken unless we initialize from something. Also of course MDS and the other embedding-algorithms have arguments we must decide (most importantly we decide for metric MDS)

### Extract & Postprocess Candidates

* Due to the aforementioned reasons, using KeyBERT requires a postprocessing of the candidate-terms, as the extracted phrases may not literally occur in the processed descriptions - hence the **postprocess_candidates** step which I need but which doesn't exist in DESC15 or the others. In my implementation however this additionally creates a mapping of keyphrases that can be considered equal. The numeric methods for they keyword-extraction shouldn't require this step, so there it's only a sanity-check double-checking if the keywords are actually in the documents they are extracted from.
    * Afterwards, the candidate-terms are postprocessed - this is especially relevant for the KeyBERT-methods, where the extracted candidates may not literally occur in the texts (especially 2+-grams), so this step applies the same processing as done for the descriptions to the candidates and tries to match them to n-grams actually occuring in the text. Those where the matching shouldn't work (or that become longer than the demanded max-n-gram because they would also contain stopwords or smth) are dropped. If a lot of processing needed to be done, a mapping of how it literally occured in the text to how it is now in the bag-of-ngrams is saved as well. For the distributional methods, nothing of this is necessary as the keywords are literally extracted. 
* extracting by quantification:
    * sklearn is used to extract using tf-idf, the ppmi implementation is manual work as I didn't find a library for that. Did I mention we're using scipy sparse csr? good stuff, but even when doing so we're easily eating 20GB RAM
    * In case of the quantifications, we extract either all those whose value is bigger than a fixed threshold, or a relative threshold of being in the top X% of all keyphrase-quantifications of all documents, while not extracing more than another absolute and/or relative threshold per-document. The keyphrases in the top Y% of all ones will be extracted anyway, even if it goes above the aforementioned threshold (Y >> X). Another threshold allows to extract a minimal number of the best-scoring keyphrases per-document anyway.


### Create Candidate SVM


\begin{figure}[H]
	\begin{center}
	  \includegraphics[width=0.9\textwidth]{3dplot_hyperplane_and_orthogonal}
	  \caption[Visual representation of the Hyperplane of an SVM splitting a dataset]{ \label{fig:3d_hyperplane_ortho} Visual representation of the Hyperplane of a Support-Vector-Machine splitting a dataset, as well as it's orthogonal and the orthogonal projection of a set of samples onto the plane. For an interactive version of this plot, visit  {\small \url{https://nbviewer.org/github/cstenkamp/derive_conceptualspaces/blob/main/notebooks/text_referenced_plots/hyperplane_orthogonal_3d.ipynb?flush_cache}}}
	\end{center}
\end{figure}


* To be able to look at the best one lateron, we save a bunch of metrics here, comparing the ranked-data, digitized data, you name it.

* Regarding Kappa-Weighting-Algorithm:
    * Yet another point where \cite{Derrac2015} are really low on information what parameters they used.
        * Sklearn allows different weighting types\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\#sklearn.metrics.cohen_kappa_score}}, when we try them [...] - TODO: explain what that changes respectively!!}, and as this plot: ![kappa_weighting_funcs](graphics/figures/which_weigthing_algo.png){#fig:which_weighting_algo} TODO: also generally write about if Kappa is a good choice (see eg \url{https://en.wikipedia.org/wiki/Cohen%27s_kappa})




## Configs per Step:

### Preprocessing 

* Translate-Policy (translate, original language, only-one-language)
* PP-Components: sentwise_merge, add_title, add_subtitle, remove_htmltags, sent_tokenize, convert_lower, remove_stopwords, lemmatize, remove_diacritics, remove_punctuation
* Another config is if this should all be done by Sklearn's sent-tokenizer, in which case some settings (like lemmatizing or sent-tokenizing) are not available (still relevant: stopwords, max_ngram, remove_htmltags, convert_lower, remove_diacritics, only_partnered)
* MIN_WORDS_PER_DESC

### Create Dissimilarity Matrix

* max-ngram
* Possible Quantifications: ppmi, tf-idf, tf, count, binary

### Create Embedding

* General Technique (either Bo-Embedding + Distance Matrix + Dimensionality-Reduction or Straight Document Embedding)
* If BoW+Distance+Dimensionality-Reduction:
    * DISSIM_MAT_ONLY_PARTNERED
    * NGRAMS_IN_EMBEDDING (and if true MAX_NGRAM)
    * quantification_measure (ppmi, tf-idf, tf, count, binary)
    * dissim_measure (mean_ang_dist, ...)
    * embed-algo (isomap, tsne, mds)
    * embed-dimensions (3, 20, 50, 100, 200)

### Extract & Process Candidates & Create DTM


* Extraction-Method (one-of-the-KeyBERTs vs one-of-the-quantifications vs all)
* If all:
    * min-doc-freq
    * if df or stf is used (use_n_docs_count)
    * max-ngram
* If by-quantification:
    * extraction_method (tfidf, tf, ppmi)
    * min-doc-freq 
    * if df or stf is used (use_n_docs_count)
    * max-ngram
    * minval_abs or minval_perc (these are per-all-docs), maxperdoc_abs and maxperdoc_rel (per-doc), forcetake_perc, minperdoc
* Then regardless of the extraction_method:
    * candidate_min_term_count (and use_n_docs_count)
    * quantification for the doc-temr-matrix (`dcm_quant_measure`)

### Creating Candidate SVMs

* `classifier` (SVM, SVM-with-squared-hinge)
* (later: which metric to be used)
