% Appendix B
% https://tex.stackexchange.com/questions/152829/how-can-i-highlight-yaml-code-in-a-pretty-way-with-listings

% \newgeometry{
% 	a4paper,
% 	top=21mm,
% 	bottom=11mm,
% 	inner=24mm,
% 	outer=9mm,
% } %bindingoffset=.5cm

%\newgeometry{
%	a4paper, inner=1.9cm, outer=1.9cm, bindingoffset=1.3cm, top=1.5cm, bottom=1.5cm, 
%} %bindingoffset=.5cm



\chapter{Implementation Details} % Main appendix title

A main goal of this thesis is to provide a code base that makes it as simple as possible to get started with \gencite{Derrac2015} algorithm to derive rudimentary conceptual spaces for any kind of dataset. In order to achieve this, documenting some implementation details and design decisions is crucial.
% TODO: something a la "es ist aber zu detailliert für den hauptteil und zerstört den lesefluss, deswegen ist der aufbau halt so dass der Hauptteil/der methods-section sich möglichst kurz fasst, wie halt die methods-section von nem Paper, und ebendieser appendix für diejenigen gedacht ist die den spezifischen Algorithmus genauer wissen wollen ODER den code nutzen wollen ODER sich einfach fragen warum dinge so sind wie sie sind. Also I HAVE to cite some of the used techniques as per their licences.
This appendix goes into more detail for selected components of the algorithm.

\label{AppendixB} 

\section{Algorithm Details}

\subsection*{Preprocessing}

\subsubsection*{Language-Detection and Translation}
\label{ap:translating}

To check the languages of the entities, the \codeother{langdetect}\footnote{\url{https://pypi.org/project/langdetect/}, \textcite{nakatani2010langdetect}} library is used, which is a direct port of a java library that claims to have 99.8\% accuracy on longer texts \cite{nakatani2010langdetect}. 
\newline

Depending on the translation-policy, it is possible to either take only those entities of the demanded language, ignore it and consider all entities in their original language, or enforce the demanded language by translating all entities from their original language to the demanded one. The accompaning code for this thesis contains extensive code to do that using the \emph{Google Cloud Translation API}\footnote{\url{https://cloud.google.com/translate}}. Many descriptions of the SIDDATA-dataset were translated using this technique\footnote{As, however, only 500.000 characters per google-account and month can be translated \href{https://cloud.google.com/translate/pricing}{free of charge}, the translation-process for the descriptions is still in progress.}. As of now, Google's Cloud Translation Service uses an embedding-based neural model of a hybrid architecture that has a transformer encoder, followed by an RNN decoder \cite{Chen2018}. All of the languages detected in the SIDDATA-dataset are supported by the system - translating between the languages german, english and spanish, which make up \todoparagraph{HOWMANY} percent of the SIDDATA-descriptions, is what the system is particularly optimized for. 
\todoparagraph{write short about their percentage, bleu score etc}
%TODO: of course we lose quality a bit, but it's only for XYZ percent of the data, and BLEU (commonly used translation metric) as well as side-by-side translation score of it is good enough

% gtranslate: talk about performance for supported languages here https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html - most descriptions are in german english or spanish, all of which are supported.
% For those langauges, it is Neural Machine Translation using Transformers (https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

% GNMT (https://arxiv.org/abs/1609.08144) is their OLD syste(2016). Since that, much progress on langauges like german or spanish (the relevant ones!) that have much training data. The currently used model has a transformer encoder and an RNN decoder (IMPORTANT PAPER: https://arxiv.org/abs/1804.09849). All relevant languagepairs use an embedding-based model (not a dictionary one, see https://arxiv.org/pdf/1807.11906.pdf)

% BLEU score, the default metric for translation, is over 50 (COMPARE TO HUMAN)
%  A popular metric for automatic quality evaluation of machine translation systems is the BLEU score, which is based on the similarity between a system’s translation and reference translations that were generated by people

% SO, https://arxiv.org/abs/1609.08144 is the orig one,  https://arxiv.org/abs/1804.09849 is the improvement.

%As is well-known, BLEU score does not fully capture the quality of a translation. For that reason we also carry out side-by-side (SxS) evaluations where we have human raters evaluate and compare the quality of two translations presented side by side for a given source sentence. Side-by-side scores range from 0 to 6, with a score of 0 meaning “completely nonsense translation”, and a score of 6 meaning “perfect translation: the meaning of the translation is completely consistent with the source, and the grammar is correct”. A translation is given a score of 4 if “the sentence retains most of the meaning of the source sentence, but may have some grammar mistakes”, and a translation is given a score of 2 if “the sentence preserves some of the meaning of the source sentence but misses significant parts”. These scores are generated by human raters who are fluent in both languages and hence often capture translation quality better than BLEU scores


% #########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

\section{Used Software}

% * Meckern über used software:
% * nltk's sent_tokenize(*, language=german) trennt sogar "...am Ende des 2. Semesters", oder, even worse, "Relevante Probleme wie z.B. Lautierungsregeln", like seriously?!
% * MDS has bugs
% * Regarding PPMI: DESC15 say it's divided py p_e* = sum_t'(pet'), whereas here [https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus] they say it's the product
% dass PPMI und auch viele andere Sachen von der größe des Datasets abhängen und exporbitant RAM verbrauchen

% #########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

\section{Configurations to run the configs for the cited papers with the accompaning code-base}
\label{ap:yamls_for_origalgos}

% \vspace{-0.8cm}

% \lstinputlisting[language=, firstline=29]{codes/dqn.txt}

\subsection{\textcite{Derrac2015}}

\begin{lstlisting}[language=yaml, caption={YAML for \textcite{Derrac2015}}]
    pp_components:          mfautcsdp
    translate_policy:       translate
    quantification_measure: ppmi
    dissim_measure:         norm_ang_dist
    embed_algo:             mds
    embed_dimensions:       [20, 50, 100, 200]
    extraction_method:      pp_keybert
    max_ngram:              5                   
    dcm_quant_measure:      count
    classifier:             SVM
    kappa_weights:          quadratic
    classifier_succmetric:  [kappa_count2rank_onlypos, kappa_rank2rank_onlypos_min] 
    prim_lambda:            0.5
    sec_lambda:             0.1
    __perdataset__:
      placetypes:
        extraction_method:  all 
        pp_components:      none
\end{lstlisting}

\subsection{\textcite{Ager2018}}

\begin{lstlisting}[language=yaml, caption={YAML for \textcite{Ager2018}}]
    max_ngram:              1
    classifier_succmetric:  [cohen_kappa, accuracy, ndcg]
    dcm_quant_measure:      ppmi    
\end{lstlisting}


\subsection{\textcite{Alshaikh2020}}

\begin{lstlisting}[language=yaml, caption={YAML for \textcite{Alshaikh2020}}]
    TODO: do
\end{lstlisting}