% Appendix B
% https://tex.stackexchange.com/questions/152829/how-can-i-highlight-yaml-code-in-a-pretty-way-with-listings

% \newgeometry{
% 	a4paper,
% 	top=21mm,
% 	bottom=11mm,
% 	inner=24mm,
% 	outer=9mm,
% } %bindingoffset=.5cm

%\newgeometry{
%	a4paper, inner=1.9cm, outer=1.9cm, bindingoffset=1.3cm, top=1.5cm, bottom=1.5cm, 
%} %bindingoffset=.5cm



\chapter{Implementation Details} % Main appendix title

A main goal of this thesis is to provide a code base that makes it as simple as possible to get started with \gencite{Derrac2015} algorithm to derive rudimentary conceptual spaces for any kind of dataset. In order to achieve this, documenting some implementation details and design decisions is crucial.
% TODO: something a la "es ist aber zu detailliert für den hauptteil und zerstört den lesefluss, deswegen ist der aufbau halt so dass der Hauptteil/der methods-section sich möglichst kurz fasst, wie halt die methods-section von nem Paper, und ebendieser appendix für diejenigen gedacht ist die den spezifischen Algorithmus genauer wissen wollen ODER den code nutzen wollen ODER sich einfach fragen warum dinge so sind wie sie sind. Also I HAVE to cite some of the used techniques as per their licences.
This appendix goes into more detail for selected components of the algorithm.

\label{AppendixB} 

\section{Algorithm Details}

\subsection*{Preprocessing}

see \autoref{sec:algo_preproc}

\subsubsection*{Language-Detection and Translation}
\label{ap:translating}

To check the languages of the entities, the \codeother{langdetect}\footnote{\url{https://pypi.org/project/langdetect/}, \textcite{nakatani2010langdetect}} library is used, which is a direct port of a java library that claims to have 99.8\% accuracy on longer texts \cite{nakatani2010langdetect}. 
\newline

Depending on the translation-policy, it is possible to either take only those entities of the demanded language, ignore it and consider all entities in their original language, or enforce the demanded language by translating all entities from their original language to the demanded one. The accompaning code for this thesis contains extensive code to do that using the \emph{Google Cloud Translation API}\footnote{\url{https://cloud.google.com/translate}}. Many descriptions of the SIDDATA-dataset were translated using this technique\footnote{As, however, only 500.000 characters per google-account and month can be translated \href{https://cloud.google.com/translate/pricing}{free of charge}, the translation-process for the descriptions is still in progress.}. As of now, Google's Cloud Translation Service uses an embedding-based neural model of a hybrid architecture that has a transformer encoder, followed by an RNN decoder \cite{Chen2018}. All of the languages detected in the SIDDATA-dataset are supported by the system - translating between the languages german, english and spanish, which make up \todoparagraph{HOWMANY} percent of the SIDDATA-descriptions, is what the system is particularly optimized for. 
\todoparagraph{write short about their percentage, bleu score etc}

\includeMD{pandoc_generated_latex/6_1_implementationdetails}

\subsection*{Candidate Extraction}

see \autoref{sec:extract_cands}

\subsubsection*{KeyBERT}
\label{ap:details_keybert}

The \emph{KeyBERT}-algorithm\footnote{\label{fnote:keybertgibhut}\fullcite{MaartenGr2021}} \cite{grootendorst2020keybert} is one of the techniques used to select phrases of the text-corpus as candidates for \gls{feature}-directions. 

KeyBERT is a keyword-extraction technique \q{that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document}\footnoteref{fnote:keybertgibhut}. \Gls{bert} is a neural language representation model that is able to embed both words and documents. Its embeddings are obtained by training a multi-layer bidirectional transformer encoder \gls{ann} architecture on a task in which a masked word must be predicted from the its bidirectional context as well subsequent fine-tuning tasks \cite{Devlin2019}. To extract keywords, the KeyBERT algorithm embeds both the document as well the \glspl{ngram} it contains using BERT and returns those phrases whose embedding ist most similar to the document-embedding according to the cosine-similiarity\footnoteref{fnote:keybertgibhut}.

The KeyBERT-model was incorporated to extract key-phrases for this codebase in two ways: 

\paragraph{KeyBERT-original} runs the algorithm on the unprocessed original texts. This is reasonable, as this is what BERT-embeddings are trained on, however it has the disadvantage that it requires a lot of post-processing to match the extracted phrases to the processed descriptions (which \eg may contain only lemmas or have their \glspl{stopword} removed)
\paragraph{KeyBERT-preprocessed} alleviates this problems by running the algorithm on already preprocessed texts. This may however lead to worse results, as the algorithm was trained on unprocessed natural sentences.

In practice, though both variants extracted different phrases, the results for either of the technqiues did not differ significantly.

% #########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

\section{Used Software}

\includeMD{pando_generated_latex/6_2_usedsoftware}

% #########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

\section{Configurations to run \mainalgos}
\label{ap:yamls_for_origalgos}

% \vspace{-0.8cm}

% \lstinputlisting[language=, firstline=29]{codes/dqn.txt}

\subsection{\textcite{Derrac2015}}

\begin{lstlisting}[language=yaml, caption={YAML for \textcite{Derrac2015}}]
    pp_components:          mfautcsdp
    translate_policy:       translate
    quantification_measure: ppmi
    dissim_measure:         norm_ang_dist
    embed_algo:             mds
    embed_dimensions:       [20, 50, 100, 200]
    extraction_method:      pp_keybert
    max_ngram:              5                   
    dcm_quant_measure:      count
    classifier:             SVM
    kappa_weights:          quadratic
    classifier_succmetric:  [kappa_count2rank_onlypos, kappa_rank2rank_onlypos_min] 
    prim_lambda:            0.5
    sec_lambda:             0.1
    __perdataset__:
      placetypes:
        extraction_method:  all 
        pp_components:      none
\end{lstlisting}

\subsection{\textcite{Ager2018}}

\begin{lstlisting}[language=yaml, caption={YAML for \textcite{Ager2018}}]
    max_ngram:              1
    classifier_succmetric:  [cohen_kappa, accuracy, ndcg]
    dcm_quant_measure:      ppmi    
\end{lstlisting}


\subsection{\textcite{Alshaikh2020}}

\begin{lstlisting}[language=yaml, caption={YAML for \textcite{Alshaikh2020}}]
    TODO: do
\end{lstlisting}