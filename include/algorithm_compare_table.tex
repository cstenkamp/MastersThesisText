


\begin{landscape}
	\begin{table}[]
		%Preprocessing siehe other table
		\resizebox{\textwidth}{!}{%
			\begin{tabular}{llll}
			& \cite{Derrac2015} & \cite{Ager2018} & \cite{Alshaikh2020} \\
			\textbf{Step 1: Generate Vector Spaces from Descriptions} & 
			\specialcell[l]{MDS trained with the angular differences $\frac{2}{\pi}* arccos\left(\frac{\vec{v}_{e_i}*\vec{v}_{e_j}}{\norm{\vec{v}_{e_i}}*\norm{\vec{v}_{e_j}}}\right)$ \\
			between the PPMI weighted BoW Vectors (all terms) \\
			Spaces of dim 20, 50, 100 and 200 \\
			Previous Experiments also considered SVD and Isomap} &
			\specialcell[l]{ \tabitem like \cite{Derrac2015} \\ \tabitem PCA from PPMI weighted BoW vectors (no quadratic complexity) \\ \tabitem Doc2Vec\footnote{TODO: Explain and cite!} Document Embeddings \\ \tabitem thresholded\footnote{\label{foot:threshold}Only words that occur more than 2 times (15 for movies-dataset)}, averaged pre-trained GloVe word embeddings \\ \tabitem thresholded\footref{foot:threshold}, PPMI-weighted-averaged pre-trained GloVe word embeddings \\
			Number of dimensions one of (50, 100, 200)} &
			\specialcell[l]{ \tabitem Document Embeddings } \\ \midrule
			\specialcell[l]{ \textbf{Step 2: Generate Candidate Words} \\ \textbf{for Feature Directions} } & 
			\specialcell[l]{ All sufficiently frequent\footnote{For the placetypes-dataset: all tags that co-occur with at least 50 place types} adjectives, nouns, adjective phrases and noun phrases \\
			(using POS-Tagger and Chunker from openNLP) } &  
			\specialcell[l]{All sufficiently frequent words\footnote{For the thresholds, see table \ref{tab:all_datasets}} (use PPMI in a later step so possibly PPMI?!) \\ weighted Logitistic Regression Classifier for Vector Direction} &
			\\ \midrule
			\textbf{Step 3: Filter Candidate Feature Directions} &                   
			\specialcell[l]{ linear SVM for all candidates (pos samples: $\forall e: c \in e$) with pos/neg-count-ratio as instance cost \\
			Only take candidates where the correlation according to Cohen's Kappa between the ranking induced by the SVM's hyperplane and count(t, e) is $\geq \lambda$ (0.5/0.1)\\
			also tried Spearman's and Kendall's correlation coeffcients }&
			\specialcell[l]{ Classifier-Performance as measured by \\ \tabitem Cohen's Kappa (compared to the ranking induced by the PPMI) \\ \tabitem Accuracy (binary)\\ \tabitem Normalized Discounted Cumulative Gain (NDCG) (TODO: binary? ranking?) } 
			& "we used logistic regression classifiers instead of SVMs, which we found to perform similarly but were faster to train."
			\\ \midrule
			\textbf{Step 4: Merge Feature Directions} &   
			\specialcell[l]{ According to \cite{Alshaikh2020} "a variant of \textit{k}-means" \\ Cluster centers: Select Term with highest $\kappa$, then i=2*ndims times select the term from $T^{0.5}$ minimizing $max_{j<i}cos(\vec[m]{v_{t_j}},\vec[m]{v_t})$ \\ 
			Others: assign all terms from $T^{0.1}$ to the closest cluster and define $\vec[m]{v^*_i} = \frac{1}{|C_i|} \sum_{t\in C_i} \vec[m]{v_t}$ as cluster direction (average direction of cluster's elements)} &
			\specialcell[l]{Input-ndims for clustering algorithm one of (500, 1000, 2000) \\
			Number of clusters one of (ndims, 2*ndims) \\
			Centroid of the cluster computed as $v_{C_j} = \frac{1}{|C_j|}\sum_{w_l \in C_j} v_l$, provided $\vec[m]{v_w}$ all normalized} & \specialcell[l]{  \textbullet\, \textbf{sub, ortho, primary}:   
			Affinity propagation instead of \textit{k}-means (no need to specify the ndims, helps with the issue that there are some non-informative clusters in \cite{Derrac2015}'s algorithm) \\ ~~ n-dims for this not directly configurable, only over preference parameter relative to median $\mu$, tried for (0.7$\mu$, 0.9$\mu$, $\mu$, 1.1$\mu$, 1.3$\mu$) \\  \textbullet\, \textbf{AHC}: Agglomerative Hierachical Clustering to cluster word directions with distance cut-offs  \\  \textbullet\, also tried Hierachical LDA \\ Cluster direction (AHC and Affinity Propagation): normal vector of the hyperplane of a linear classifier separating entities whose description contains at least one of the words from the cluster from the others  } %TODO find a short-term notation for "entities whose description contains at least one of the words from XYZ" ("for a Cluster C, we write pos_C and neg_C for the set of positively and negatively classified entities")
			\\  \midrule
			\textbf{Step 5: Post-Processing} &                   
			None &                 
			TODO describe fine-tuning! & \specialcell[l]{
			Perform steps 1-4 a second time (only for positively classified entities), such that there are primary features (domains) and sub-features \\ representation kept flat (values for the sub-features is same dot-product as for domains) \\ \textbullet\, \textbf{sub}: sub-features extracted equal to first-order-features \\ \textbullet\, \textbf{ortho}: sub-feature directions orthogonal to corresponding primary feature direction (enforce complementary information) \\ ~~ by computing orthogonal decomposition of feature w.r.t. its domain (pg. 4, equation 1) \\ \textbullet\, also tried to combine mother-domain with sub-feature, but that performed poorly \\ \textbullet\, \textbf{primary}: model with only primary features}
			\end{tabular}
		}
		\caption{Compared algorithms from \cite{Derrac2015} \cite{Ager2018} \cite{Alshaikh2020}}
		\label{tab:compared_algos}
	\end{table}
\end{landscape}

% Alshaikh2020 say: "It may seem counter-intuitive to use binary classifiers to learn representations of ordinal features. However, the occurrence or non- occurrence of a word in the description is binary, and this is the most important available signal. We experimented with statistics such as pointwise mutual information, which did not lead to better results." -> does that mean DCM_QUANT_MEASURE or QUANTIFICATION_MEASURE or both? and also does that mean they use binary or count as this measure?

% Alshaikh2020 use: sub, ortho, primary, random (coordinates uniformly random), 
