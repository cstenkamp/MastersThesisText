\usepackage{amsfonts}
\usepackage{siunitx}
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}

%TODO: If I don't want to have a COMPLETE pagelist, see https://tex.stackexchange.com/q/246818/108199

% =========================================================
% UNITS

% \newglossaryentry{m}{
%     name=\ensuremath{m},
%     description={Meter},
%     unit={\si{m}},
%     type=units
% }

% \newglossaryentry{energyconsump}{
%     name=\ensuremath{P},
%     description={Power},
%     unit={\si{kW}},
%     type=units
% }

% =========================================================
% SYMBOLS

% \newglossaryentry{symb:Pi}{
%     name=\ensuremath{\pi},
%     description={geometric value},
%     type=symbols,
%     sort=P
% }

% \newglossaryentry{integers}{
%     name=\ensuremath{\Z},
%     type=symbols,
%     description={the ring of integers}, 
%     sort=Z
% }

% \newglossaryentry{rationals}{
%     name=\ensuremath{\Q},
%     type=symbols,
%     description={the field of rational numbers}, 
%     sort=Q,
%     % nonumberlist,
% } 

\newglossaryentry{vector-space}{
    name=\ensuremath{V},
    type=symbols,
    description={a vector space}, 
    sort=V
}

% =========================================================
% DEFINITIONS

\newglossaryentry{ngram}{
    type=defs,
    name=n-gram,
    description={
        n-grams are sequences of consecutive words of length $n$. For example, the text "I eat lunch" contains the 1-grams ["I", "eat", "lunch"], the 2-grams ["I eat", "eat lunch"] and the 3-gram ["I eat lunch"]. In the scope of this thesis, the term \textbf{phrase} also refers to n-grams.
        }
}


\newglossaryentry{stopword}{
    type=defs,
    name=stopword,
    description={
        TODO: do
        }
}

\newglossaryentry{lemma}{
    type=defs,
    name=lemma,
    description={
        The \textbf{lemma} of a word is the canonical, base form of a set of words belonging to the same lexeme. \textbf{Lemmatizing} a word refers to the process of finding this base form for (possibly inflected) words. For example, the lemma of the words \textit{going, went, gone} is \textit{go}.
        }
}

\newglossaryentry{doctermmat}{
    type=defs,
    name=document-term \mbox{matrix},
    plural=document-term matrices,
    description={
        A document-term matrix encodes the frequency of terms (words, n-grams or other) for a collection of texts in a matrix. The (often very sparse) matrix has a rows represending the documents and columns corresponding to terms, the individual values encoding the pure counts, frequencies or quantifications of all combinations of document and term. \todoparagraph{one component for every term occurring in the corpus}
        }
}


\newglossaryentry{dissimmat}{
    type=defs,
    name=dissimilarity matrix,
    plural=dissimilarity matrices,
    description={
        \hspace{0.2em} A square matrix where both rows and columns represent entities, the cells being to their pairwise dissimilarities as calculated by an arbitrary distance function. For metric distances, distance matrices are mirrored along their main diagonal, which is made up solely from zeros. Also called \textbf{distance matrix}.
        }
}


\newglossaryentry{word2vec}{
    type=defs,
    name=Word2Vec,
    description={
        word2vec is the most famous of a family of \emph{neural language models} \cite{Le2014}. Here we'll use it as \textit{Pars pro Toto} for all of these or rather the respectively most appropriate one. \todoparagraph{Write more about it, rather in a section!}
        }
}

\newglossaryentry{doc2vec}{
    type=defs,
    name=Doc2Vec,
    description={
        \todoparagraph{The term Doc2Vec is not originally used by the authors, only \emph{Paragraph Vectors}, but that's what it's generally called}
        }
}



\newglossaryentry{tsne}{
    type=defs,
    name=t-SNE,
    description={
        t-SNE is a dimensionality-reduction algorithm often used to visualize high-dimensional data in two or three dimensions. It converts distances between data point to probabilities and minimizes the pairwise Kullback-Leibler-divergence between the joint probabilities of the original data and their respective embeddings. % https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html
        }
}

\newglossaryentry{acc}{
    type=defs,
    name=accuracy,
    description={
        \todoparagraph{TODO!}
        }
}

\newglossaryentry{f1}{
    type=defs,
    name=F-1 score,
    description={
        \todoparagraph{TODO!}
        }
}

\newglossaryentry{unsupervised}{
    type=defs,
    name=unsupervised learning,
    description={
        \todoparagraph{TODO!}
        }
}

\newglossaryentry{cos}{
    type=defs,
    name=cosine distance,
    description={
        \todoparagraph{TODO!}
        }
}

\newglossaryentry{normangdist}{
    type=defs,
    name=Normalized Angular Distance,
    description={
        \todoparagraph{TODO!}
        }
}





% =========================================================
% CUSTOM TERMS


\newglossaryentry{rank}{
    type=customs,
    name=rank,
    description={
        \todoparagraph{Rank here means} ordered-by-value, specifically the value of an entity with respect to a semantic direction. Relevant in the \gls{fbr}
    }
}

\newglossaryentry{fbr}{
    type=customs,
    name=feature-based representation,
    description={
        In the context of the given algorithm, the \textbf{feature-based representation} of an \gls{entity} is its representation as a feature-vector, where each dimension corresponds to a semantic \gls{feature} and its value is the entity's respective \gls{rank} for that feature.
    }
}


\newglossaryentry{quant}{
    type=customs,
    name=quantificiation,
    description={
        \hspace{1.2em} In the scope of this thesis, the term \textbf{quantification} refers to the relative score for an n-gram in a document, depending on its frequency as well as other frequencies, as calculated by one of the \nameref{sec:word_count_techniques}, also called \textbf{quantification measures}. %TODO: to lower-case see https://tex.stackexchange.com/questions/445404/capitalization-variants-of-nameref 
        }
}

\newglossaryentry{entity}{
    type=customs,
    name=entity,
    plural=entities,
    description={
        An entity is a single sample from the handled corpus. Depending on the context, this term may also refer to its associated text (which may, depending on the considered dataset, be the course-description, picture-tags, concatenated-reviews, \dots).
        }
}


\newglossaryentry{param}{
    type=customs,
    name=hyperparameter,
    description={
        \todoparagraph{When I refer to hyperparameters, I mean not only stuff like the number of dimensions, but also which specific algorithm is used in a step!!}
        }
}


\newglossaryentry{cand}{
    type=customs,
    name=candidate-phrase,
    description={
        \todoparagraph{Candidateterm, Candidate, Candidate-phrase, potato, potahto}
        }
}

\newglossaryentry{feature}{
    type=customs,
    name=feature,
    description={
        \todoparagraph{Feature, the semantic human-interpretable dimensions, words or groups of words that are supposed to represent a concept or attribute, used as dimension in the embedding}
        }
}
