\usepackage{amsfonts}
\usepackage{siunitx}
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}

%TODO: If I don't want to have a COMPLETE pagelist, see https://tex.stackexchange.com/q/246818/108199

% =========================================================
% UNITS

% \newglossaryentry{m}{
%     name=\ensuremath{m},
%     description={Meter},
%     unit={\si{m}},
%     type=units
% }

% \newglossaryentry{energyconsump}{
%     name=\ensuremath{P},
%     description={Power},
%     unit={\si{kW}},
%     type=units
% }

% =========================================================
% SYMBOLS

% \newglossaryentry{symb:Pi}{
%     name=\ensuremath{\pi},
%     description={geometric value},
%     type=symbols,
%     sort=P
% }

% \newglossaryentry{integers}{
%     name=\ensuremath{\Z},
%     type=symbols,
%     description={the ring of integers}, 
%     sort=Z
% }

% \newglossaryentry{rationals}{
%     name=\ensuremath{\Q},
%     type=symbols,
%     description={the field of rational numbers}, 
%     sort=Q,
%     % nonumberlist,
% } 

% \newglossaryentry{vector-space}{
%     name=\ensuremath{V},
%     type=symbols,
%     description={a vector space}, 
%     sort=V
% }

% =========================================================
% DEFINITIONS

\newglossaryentry{ngram}{
    type=defs,
    name=n-gram,
    description={
        n-grams are sequences of consecutive words of length $n$. For example, the text "I eat lunch" contains the 1-grams ["I", "eat", "lunch"], the 2-grams ["I eat", "eat lunch"] and the 3-gram ["I eat lunch"]. In the scope of this thesis, the term \textbf{phrase} also refers to n-grams.
        }
}


\newglossaryentry{stopword}{
    type=defs,
    name=stop word,
    description={
        \textbf{Stop words} are words that are very common to a language and thus of low discriminative power for differentiating between individual texts of a corpus. Stop word lists are used as negative dictionary to remove them before text processing. There are no universally agreed-upon stop word lists.
        }
}

\newglossaryentry{lemma}{
    type=defs,
    name=lemma,
    description={
        The \textbf{lemma} of a word is the canonical, base form of a set of words belonging to the same lexeme. \textbf{Lemmatizing} a word refers to the process of finding this base form for (possibly inflected) words. For example, the lemma of the words \textit{going, went, gone} is \textit{go}.
        }
}

\newglossaryentry{doctermmat}{
    type=defs,
    name=document-term \mbox{matrix},
    plural=document-term matrices,
    description={
        A document-term matrix encodes the frequency of terms (words, n-grams or other) for a collection of texts in a matrix. The (often very sparse) matrix has a rows represending the documents and columns corresponding to terms, the individual values encoding the pure counts, frequencies or quantifications of all combinations of document and term. 
        }
}


\newglossaryentry{dissimmat}{
    type=defs,
    name=dissimilarity matrix,
    plural=dissimilarity matrices,
    description={
        \hspace{0.2em} A square matrix where both rows and columns represent entities, the cells being to their pairwise dissimilarities as calculated by an arbitrary distance function. For metric distances, distance matrices are mirrored along their main diagonal, which is made up solely from zeros. Also called \textbf{distance matrix}.
        }
}


\newglossaryentry{word2vec}{
    type=defs,
    name=Word2Vec,
    description={
        word2vec is the most famous of a family of \emph{neural language models} \cite{Mikolov2013}. These models are trained on large corpora of texts to predict a word from its surrounding words or vice versa. Each word is represented as a vector, and the training ensures that semantically similar words end up with similar vector representations. This helps with many NLP tasks, as it counters problems of synonymy and polysemy by considering context. The difference between vectors carry semantic meaning, however unlike conceptual spaces they are not domain-spefic and embed all natural language words in a high-dimensional space of arbitrary dimensions.}
}

\newglossaryentry{doc2vec}{
    type=defs,
    name=Doc2Vec,
    description={
        doc2vec (or \emph{Paragraph Vectors}) refers to a technique by \cite{Le2014} that represents a document by a dense vector that is trained to predict the word occuring in it, analaogous to the training of word2vec. In contrast to bag-of-words-representations of texts, it considers word order and semantics of the words, which often leads to substantial improvements \eg in classification and information retrieval tasks. Nowadays, there are many better performance models based on \textit{transformer} \gls{ann} architectures such as \gls{bert} \cite{Devlin2019} wich base on similar training techniques but outperform in such tasks.
        }
}



\newglossaryentry{tsne}{
    type=defs,
    name=t-SNE,
    description={
        t-SNE is a dimensionality-reduction algorithm often used to visualise high-dimensional data in two or three dimensions. It converts distances between data point to probabilities and minimizes the pairwise Kullback-Leibler-divergence between the joint probabilities of the original data and their respective embeddings. % https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html
        }
}

\newglossaryentry{acc}{
    type=defs,
    name=accuracy,
    description={
        $\frac{TP+TN}{TP+TN+FP+FN}$
        }
}

\newglossaryentry{f1}{
    type=defs,
    name=F-1 score,
    description={
        $2*\frac{precision*recall}{percision+recall} = \frac{TP}{TP+\frac{1}{2}(FP+FN)}$
        }
}


% \DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
% \usepackage{fourier,esvect}
% \newcommand{\cross}[2]{\bigl[ \vv{#1},\vv{#2\vphantom{#1}} \bigr]}
% \newcommand\z{\vphantom{{}'}}

\newglossaryentry{cos}{
    type=defs,
    name=cosine distance,
    description={
        $\cos(\theta )={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|\|\mathbf {B} \|}$
        }
}





% =========================================================
% CUSTOM TERMS


\newglossaryentry{rank}{
    type=customs,
    name=rank,
    description={
        A ranking of a set up numbers refers to their respective index when  ordered-by-value. In this work it refers specifically to the value of an entity with respect to a semantic direction. Relevant in the \gls{fbr}.
    }
}

\newglossaryentry{fbr}{
    type=customs,
    name=feature-based representation,
    description={
        In the context of the given algorithm, the \textbf{feature-based representation} of an \gls{entity} is its representation as a feature-vector, where each dimension corresponds to a semantic feature and its value is the entity's respective \gls{rank} for that feature.
    }
}


\newglossaryentry{quant}{
    type=customs,
    name=quantificiation,
    description={
        \hspace{1em} In the scope of this thesis, the term \textbf{quantification} refers to the relative score for an n-gram in a document, depending on its frequency as well as other frequencies, as calculated by one of the \nameref{sec:word_count_techniques}, also called \textbf{quantification measures}. %TODO: to lower-case see https://tex.stackexchange.com/questions/445404/capitalization-variants-of-nameref 
        }
}

\newglossaryentry{entity}{
    type=customs,
    name=entity,
    plural=entities,
    description={
        An entity is a single sample from the handled corpus. Depending on the context, this term may also refer to its associated text (which may, depending on the considered dataset, be the course-description, picture-tags, concatenated-reviews, \dots).
        }
}


\newglossaryentry{param}{
    type=customs,
    name=hyperparameter,
    description={
        \hspace{0.55em}  When it is referred to \textit{hyperparameter} in this work, it does not only refer to scalars like the dimensionality of an embedding, but also \eg which specific algorithm is used in a step of the algorithm.
        }
}


