Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop
Chris' script ran over it.

@inproceedings{Guo,
abstract = {This paper presents an effective approach for parallel corpus mining using bilingual sentence embeddings. Our embedding models are trained to produce similar representations exclusively for bilingual sentence pairs that are translations of each other. This is achieved using a novel training method that introduces hard negatives consisting of sentences that are not translations but have some degree of semantic similarity. The quality of the resulting embeddings are evaluated on parallel corpus reconstruction and by assessing machine translation systems trained on gold vs. mined sentence pairs. We find that the sentence embeddings can be used to reconstruct the United Nations Parallel Corpus (Ziemski et al., 2016) at the sentence-level with a precision of 48.9% for en-fr and 54.9% for en-es. When adapted to document-level matching, we achieve a parallel document matching accuracy that is comparable to the significantly more computationally intensive approach of Uszkoreit et al. (2010). Using reconstructed parallel data, we are able to train NMT models that perform nearly as well as models trained on the original data (within 1-2 BLEU).},
archivePrefix = {arXiv},
arxivId = {1807.11906},
author = {Guo, Mandy and Shen, Qinlan and Yang, Yinfei and Ge, Heming and Cer, Daniel and Abrego, Gustavo Hernandez and Stevens, Keith and Constant, Noah and Sung, Yun Hsuan and Strope, Brian and Kurzweil, Ray},
booktitle = {WMT 2018 - 3rd Conference on Machine Translation, Proceedings of the Conference},
doi = {10.18653/v1/w18-6317},
eprint = {1807.11906},
file = {:home/chris/Documents/Mendeley Desktop/Guo et al/Guo et al. - Unknown - Effective Parallel Corpus Mining using Bilingual Sentence Embeddings.pdf:pdf},
isbn = {9781948087810},
pages = {165--176},
title = {{Effective Parallel Corpus Mining using Bilingual Sentence Embeddings}},
volume = {1},
year = {2018}
}

@article{Lowe,
abstract = {This paper adds some theory to the growing literature of semantic space models. We motivate semantic space models from the perspective of distributional linguistics and show how an explicit mathematical formulation can provide a better understanding of existing models and suggest changes and improvements. In addition to pro- viding a theoretical framework for current models, we consider the implications of statistical aspects of language data that have not been addressed in the psychological modeling literature. Statistical approaches to language must deal principally with count data, and this data will typically have a highly skewed frequency distribution due to Zipf's law. We consider the consequences of these facts for the construction of semantic space models, and present methods for removing frequency biases from se- mantic space models.},
author = {Lowe, Will},
file = {:home/chris/Documents/Mendeley Desktop/Lowe/Lowe - 2001 - Towards a Theory of Semantic Space.pdf:pdf},
issn = {0305-0009},
journal = {Proceedings of the Twenty-Third Annual Conference of the Cognitive Science Society},
pages = {576--581},
pmid = {3980601},
title = {{Towards a Theory of Semantic Space}},
year = {2001}
}

@inproceedings{nothman-etal-2018-stop,
abstract = {Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. {``}hasn{'}t{''} but not {``}hadn{'}t{''}) and inclusions ({``}computer{''}), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.},
address = {Melbourne, Australia},
author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
booktitle = {Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})},
doi = {10.18653/v1/W18-2502},
pages = {7--12},
publisher = {Association for Computational Linguistics},
title = {{Stop Word Lists in Free Open-source Software Packages}},
url = {https://aclanthology.org/W18-2502},
year = {2018}
}

@article{Carmel2009,
abstract = {This work investigates cluster labeling enhancement by utilizing Wikipedia, the free on-line encyclopedia. We describe a general framework for cluster labeling that extracts candidate labels from Wikipedia in addition to important terms that are extracted directly from the text. The "labeling quality" of each candidate is then evaluated by several independent judges and the top evaluated candidates are recommended for labeling. Our experimental results reveal that the Wikipedia labels agree with manual labels associated by humans to a cluster, much more than with significant terms that are extracted directly from the text. We show that in most cases even when human's associated label appears in the text, pure statistical methods have difficulty in identifying them as good descriptors. Furthermore, our experiments show that for more than 85% of the clusters in our test collection, the manual label (or an inflection, or a synonym of it) appears in the top five labels recommended by our system. Copyright 2009 ACM.},
author = {Carmel, David and Roitman, Haggai and Zwerdling, Naama},
doi = {10.1145/1571941.1571967},
file = {:home/chris/Documents/Mendeley Desktop/Carmel, Roitman, Zwerdling/Carmel, Roitman, Zwerdling - 2009 - Enhancing cluster labeling using wikipedia.pdf:pdf},
isbn = {9781605584836},
journal = {Proceedings - 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009},
keywords = {Cluster labeling,Wikipedia},
pages = {139--146},
title = {{Enhancing cluster labeling using wikipedia}},
year = {2009}
}

@inproceedings{maas-EtAl:2011:ACL-HLT2011,
address = {Portland, Oregon, USA},
author = {Maas, Andrew L and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
pages = {142--150},
publisher = {Association for Computational Linguistics},
title = {{Learning Word Vectors for Sentiment Analysis}},
url = {http://www.aclweb.org/anthology/P11-1015},
year = {2011}
}

@techreport{Derrac2015,
abstract = {Commonsense reasoning patterns such as interpolation and a fortiori inference have proven useful for dealing with gaps in structured knowledge bases. An important diculty in applying these reasoning patterns in practice is that they rely on fine-grained knowledge of how di↵erent concepts and entities are semantically related. In this paper, we show how the required semantic relations can be learned from a large collection of text documents. To this end, we first induce a conceptual space from the text documents, using multi-dimensional scaling. We then rely on the key insight that the required semantic relations correspond to qualitative spatial relations in this conceptual space. Among others, in an entirely unsupervised way, we identify salient directions in the conceptual space which correspond to interpretable relative properties such as 'more fruity than' (in a space of wines), resulting in a symbolic and interpretable representation of the conceptual space. To evaluate the quality of our semantic relations, we show how they can be exploited by a number of commonsense reasoning based classifiers. We experimentally show that these classifiers can outperform standard approaches, while being able to provide intuitive explanations of classification decisions. A number of crowdsourcing experiments provide further insights into the nature of the extracted semantic relations.},
author = {Derrac, Joaqu{\'{i}}n and Schockaert, Steven},
file = {:home/chris/Documents/Mendeley Desktop/Derrac, Schockaert/Derrac, Schockaert - 2015 - Inducing semantic relations from conceptual spaces a data-driven approach to plausible reasoning.pdf:pdf},
keywords = {Commonsense reasoning Email addresses: jderrac@csc,Conceptual spaces,Dimensionality reduction,Qualitative spatial relations,sschockaert@cscardiffacuk (Steven Schockaert)},
title = {{Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning}},
url = {http://dbpedia.org/About},
year = {2015}
}

@inproceedings{Alshaikh2020,
abstract = {Conceptual spaces are geometric meaning representations in which similar entities are represented by similar vectors. They are widely used in cognitive science, but there has been relatively little work on learning such representations from data. In particular, while standard representation learning methods can be used to induce vector space embeddings from text corpora, these differ from conceptual spaces in two crucial ways. First, the dimensions of a conceptual space correspond to salient semantic features, known as quality dimensions, whereas the dimensions of learned embeddings typically lack any clear interpretation. This has been partially addressed in previous work, which has shown that it is possible to identify directions in learned vector spaces which capture semantic features. Second, conceptual spaces are normally organised into a set of domains, each of which is associated with a separate vector space. In contrast, learned embeddings represent all entities in a single vector space. Our hypothesis in this paper is that such single-space representations are sub-optimal for learning quality dimensions, due to the fact that semantic features are often only relevant to a subset of the entities. We show that this issue can be mitigated by identifying features in a hierarchical fashion. Intuitively, the top-level features split the vector space into domains, allowing us to subsequently identify domain-specific quality dimensions.},
annote = {-Conceptual Spaces have two advantages over vector-spaces: 1) they have interpretable dimensions, 2) they are have sets of domains, each with their own vector-space
-identify features in hierachical fashion. Top-Level features split vector space into domains, and inside that you have domain-specific quality dimensions.
Improvements over Schockhart 2015!},
author = {Alshaikh, Rana and Bouraoui, Zied and Schockaert, Steven},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2020/494},
file = {:home/chris/Documents/Mendeley Desktop/Alshaikh, Bouraoui, Schockaert/Alshaikh, Bouraoui, Schockaert - 2020 - Learning Conceptual Spaces with Disentangled Facets.pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
keywords = {Humans and AI: Cognitive Modeling,Machine Learning: Interpretability,Natural Language Processing: Natural Language Proc},
pages = {3573--3579},
title = {{Learning Conceptual Spaces with Disentangled Facets}},
volume = {2021-Janua},
year = {2020}
}

@article{Alshaikh2019,
abstract = {Conceptual spaces are geometric representations of meaning that were proposed by G{\"{a}}rdenfors (2000). They share many similarities with the vector space embeddings that are commonly used in natural language processing. However, rather than representing entities in a single vector space, conceptual spaces are usually decomposed into several facets, each of which is then modelled as a relatively low-dimensional vector space. Unfortunately, the problem of learning such conceptual spaces has thus far only received limited attention. To address this gap, we analyze how, and to what extent, a given vector space embedding can be decomposed into meaningful facets in an unsupervised fashion. While this problem is highly challenging, we show that useful facets can be discovered by relying on word embeddings to group semantically related features.},
author = {Alshaikh, Rana and Bouraoui, Zied and Schockaert, Steven},
doi = {10.18653/v1/k19-1013},
file = {:home/chris/Documents/Mendeley Desktop/Alshaikh, Bouraoui, Schockaert/Alshaikh, Bouraoui, Schockaert - 2019 - Learning conceptual spaces with disentangled facets.pdf:pdf},
isbn = {9781950737727},
journal = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},
pages = {131--139},
publisher = {Association for Computational Linguistics},
title = {{Learning conceptual spaces with disentangled facets}},
url = {https://aclanthology.org/K19-1013},
year = {2019}
}

@inproceedings{Ager2018,
abstract = {In this paper we consider semantic spaces consisting of objects from some particular domain (e.g. IMDB movie reviews). Various authors have observed that such semantic spaces often model salient features (e.g. how scary a movie is) as directions. These feature directions allow us to rank objects according to how much they have the corresponding feature, and can thus play an important role in interpretable classifiers, recommendation systems, or entity-oriented search engines, among others. Methods for learning semantic spaces, however, are mostly aimed at modelling similarity. In this paper, we argue that there is an inherent trade-off between capturing similarity and faithfully modelling features as directions. Following this observation, we propose a simple method to fine-tune existing semantic spaces, with the aim of improving the quality of their feature directions. Crucially, our method is fully unsupervised, requiring only a bag-of-words representation of the objects as input.},
address = {Stroudsburg, PA, USA},
author = {Ager, Thomas and Ku{\v{z}}elka, Ondřej and Schockaert, Steven},
booktitle = {CoNLL 2018 - 22nd Conference on Computational Natural Language Learning, Proceedings},
doi = {10.18653/v1/k18-1051},
file = {:home/chris/Documents/Mendeley Desktop/Ager, Ku{\v{z}}elka, Schockaert/Ager, Ku{\v{z}}elka, Schockaert - 2018 - Modelling salient features as directions in fine-tuned semantic spaces.pdf:pdf},
isbn = {9781948087728},
pages = {530--540},
publisher = {Association for Computational Linguistics},
title = {{Modelling salient features as directions in fine-tuned semantic spaces}},
url = {http://aclweb.org/anthology/K18-1051},
year = {2018}
}

@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonyms that are in turn linked through semantic relations that determine word definitions. {\textcopyright} 1995, ACM. All rights reserved.},
author = {Miller, George A.},
doi = {10.1145/219717.219748},
file = {:home/chris/Documents/Mendeley Desktop/Miller/Miller - 1995 - WordNet A Lexical Database for English.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
month = {nov},
number = {11},
pages = {39--41},
publisher = {ACM
		PUB27
		New York, NY, USA},
title = {{WordNet: A Lexical Database for English}},
url = {https://dl.acm.org/doi/abs/10.1145/219717.219748},
volume = {38},
year = {1995}
}

@book{Gardenfors2000,
abstract = {Within cognitive science, two approaches currently dominate the problem of modeling representations. The symbolic approach views cognition as computation involving symbolic manipulation. Connectionism, a special case of associationism, models associations using artificial neuron networks. Peter G{\"{a}}rdenfors offers his theory of conceptual representations as a bridge between the symbolic and connectionist approaches.},
author = {G{\"{a}}rdenfors, Peter},
booktitle = {Conceptual Spaces},
doi = {10.7551/mitpress/2076.001.0001},
publisher = {MIT Press},
title = {{Conceptual Spacess: The Geometry of Thought}},
url = {https://direct.mit.edu/books/book/2532/Conceptual-SpacesThe-Geometry-of-Thought},
year = {2000}
}

@inproceedings{loper-bird-2002-nltk,
address = {Philadelphia, Pennsylvania, USA},
author = {Loper, Edward and Bird, Steven},
booktitle = {Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics},
doi = {10.3115/1118108.1118117},
pages = {63--70},
publisher = {Association for Computational Linguistics},
title = {{{NLTK} : The Natural Language Toolkit}},
url = {https://aclanthology.org/W02-0109},
year = {2002}
}

@book{Gardenfors2000a,
author = {G{\"{a}}rdenfors, Peter},
doi = {10.7551/mitpress/2076.001.0001},
isbn = {0-262-07199-1},
pages = {318},
publisher = {Bradford Books},
title = {{Conceptual Spaces: The Geometry of Thought}},
year = {2000}
}

@article{Molder2021a,
abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way. Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid.Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
author = {M{\"{o}}lder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and Tomkins-Tinch, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and K{\"{o}}ster, Johannes},
doi = {10.12688/F1000RESEARCH.29032.1},
file = {:home/chris/Documents/Mendeley Desktop/M{\"{o}}lder et al/M{\"{o}}lder et al. - 2021 - Sustainable data analysis with Snakemake(3).pdf:pdf},
issn = {2046-1402},
journal = {F1000Research},
keywords = {Lee S: Methodology, Software,Letcher B: Methodology, Software, Writing-Review &,Nahnsen S: Conceptualization,Rahmann S: Supervision, Writing-Review & Editing,Sochat V: Methodology, Software, Writing-Review &,Tomkins-Tinch CH: Methodology, Software, Writing-R,Twardziok SO: Methodology, Software,Wilm A: Methodology, Software, Writing-Review & Ed,adaptability,data analysis,reproducibility,scalability,sustainability,transparency,workflow management},
month = {jan},
pages = {33},
publisher = {F1000 Research Ltd},
title = {{Sustainable data analysis with Snakemake}},
url = {https://f1000research.com/articles/10-33},
volume = {10},
year = {2021}
}

@inproceedings{Henrich,
abstract = {This paper introduces GernEdiT (short for: GermaNet Editing Tool), a new graphical user interface for the lexicographers and developers of GermaNet, the German version of the Princeton WordNet.},
address = {Valetta, Malta},
author = {Henrich, Verena and Hinrichs, Erhard},
booktitle = {Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010)},
file = {:home/chris/Documents/Mendeley Desktop/Henrich, Hinrichs/Henrich, Hinrichs - 2010 - GernEdiT-The GermaNet Editing Tool.pdf:pdf},
pages = {2228--2235},
title = {{GernEdiT-The GermaNet Editing Tool}},
url = {http://wordnet.princeton.edu/man/grind.1WN.html},
year = {2010}
}

@article{Gardenfors2004,
abstract = {The Semantic Web is not semantic. It is good for syllogistic reasoning, but there is much more to semantics than syllogisms. I argue that the current Semantic Web is too dependent on symbolic representations of information structures, which limits its representational capacity. As a remedy, I propose conceptual spaces as a tool for expressing more of the semantics. Conceptual spaces are built up from quality dimensions that have geometric or topological structures. With the aid of the dimensions, similarities between objects can easily be represented and it is argued that similarity is a central aspect of semantic content. By sorting the dimensions into domains, I define properties and concepts and show how prototype effects of concepts can be treated with the aid of conceptual spaces. I present an outline of how one can reconstruct most of the taxonomies and other meta-data that are explicitly coded in the current Semantic Web and argue that inference engines on the symbolic level will become largely superfluous. As an example of the semantic power of conceptual spaces, I show how concept combinations can be analysed in a much richer and more accurate way than in the classical logical approach.},
annote = {First description of CS: "Conceptual spaces are built up from quality dimensions that have geometric or topological structures. With the aid of the dimensions, similarities between objects can easily be represented and it is argued that similarity is a central aspect of semantic content. By sorting the dimensions into domains, I define properties and concepts and show how prototype effects of concepts can be treated with the aid of conceptual spaces."},
author = {G{\"{a}}rdenfors, P},
file = {:home/chris/Documents/Mendeley Desktop/G{\"{a}}rdenfors/G{\"{a}}rdenfors - 2004 - How to make the semantic web more semantic.pdf:pdf},
journal = {Formal Ontology in Information Systems. IOS Press},
pages = {17--36},
title = {{How to make the semantic web more semantic}},
url = {http://yaxu.org/tmp/Gardenfors04.pdf},
year = {2004}
}

@article{Turney2010,
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
archivePrefix = {arXiv},
arxivId = {1003.1141v1},
author = {Turney, Peter D and Pantel, Patrick},
eprint = {1003.1141v1},
file = {:home/chris/Documents/Mendeley Desktop/Turney, Pantel/Turney, Pantel - 2010 - From Frequency to Meaning Vector Space Models of Semantics.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {141--188},
title = {{From Frequency to Meaning: Vector Space Models of Semantics}},
url = {http://www.natcorp.ox.ac.uk/.},
volume = {37},
year = {2010}
}

@inproceedings{Mikolov2013a,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.48550/arxiv.1310.4546},
eprint = {1310.4546},
file = {:home/chris/Documents/Mendeley Desktop/Mikolov et al/Mikolov et al. - 2013 - Distributed representations ofwords and phrases and their compositionality.pdf:pdf},
issn = {10495258},
keywords = {()},
month = {oct},
publisher = {Neural information processing systems foundation},
title = {{Distributed representations ofwords and phrases and their compositionality}},
url = {https://arxiv.org/abs/1310.4546v1},
year = {2013}
}

@article{Gardenfors2001,
abstract = {Understanding the process of categorization is a primary research goal in artificial intelligence. The conceptual space framework provides a flexible approach to modeling context-sensitive categorization via a geometrical representation designed for modeling and managing concepts. In this paper we show how algorithms developed in computational geometry, and the Region Connection Calculus can be used to model important aspects of categorization in conceptual spaces. In particular, we demonstrate the feasibility of using existing geometric algorithms to build and manage categories in conceptual spaces, and we show how the Region Connection Calculus can be used to reason about categories and other conceptual regions.},
author = {G{\"{a}}rdenfors, Peter and Williams, Mary Anne},
file = {:home/chris/Documents/Mendeley Desktop/G{\"{a}}rdenfors, Williams/G{\"{a}}rdenfors, Williams - 2001 - Reasoning about categories in conceptual spaces.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {385--392},
title = {{Reasoning about categories in conceptual spaces}},
year = {2001}
}

@article{Cohn1997,
abstract = {This paper surveys the work of the qualitative spatial reasoning group at the University of Leeds. The group has developed a number of logical calculi for representing and reasoning with qualitative spatial relations over regions. We motivate the use of regions as the primary spatial entity and show how a rich language can be built up from surprisingly few primitives. This language can distinguish between convex and a variety of concave shapes and there is also an extension which handles regions with uncertain boundaries. We also present a variety of reasoning techniques, both for static and dynamic situations. A number of possible application areas are briefly mentioned.},
annote = {Gelesen: Kapitel 1},
author = {Cohn, Anthony G. and Bennett, Brandon and Gooday, John and Gotts, Nicholas Mark},
doi = {10.1023/A:1009712514511},
file = {:home/chris/Documents/Mendeley Desktop/Cohn et al/Cohn et al. - 1997 - Qualitative Spatial Representation and Reasoning with the Region Connection Calculus.pdf:pdf},
issn = {13846175},
journal = {GeoInformatica},
keywords = {Qualitative spatial reasoning,Shape,Spatial logics,Topology,Vague boundaries},
number = {3},
pages = {275--316},
title = {{Qualitative Spatial Representation and Reasoning with the Region Connection Calculus}},
volume = {1},
year = {1997}
}

@inproceedings{Chen2018,
abstract = {The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then outperformed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English→French and English→German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1804.09849},
author = {Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Shazeer, Noam and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Schuster, Mike and Chen, Zhifeng and Wu, Yonghui and Hughes, Macduff},
booktitle = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
doi = {10.18653/v1/p18-1008},
eprint = {1804.09849},
file = {:home/chris/Documents/Mendeley Desktop/Chen et al/Chen et al. - 2018 - The best of both worlds Combining recent advances in neural machine translation.pdf:pdf},
isbn = {9781948087322},
pages = {76--86},
title = {{The best of both worlds: Combining recent advances in neural machine translation}},
url = {https://github.com/tensorflow/tensor2tensor},
volume = {1},
year = {2018}
}

@misc{nakatani2010langdetect,
author = {Shuyo, Nakatani},
title = {{Language Detection Library for Java}},
url = {http://code.google.com/p/language-detection/},
year = {2010},
howpublished = {\url{http://code.google.com/p/language-detection/}}
}

@inproceedings{hamp-feldweg-1997-germanet,
author = {Hamp, Birgit and Feldweg, Helmut},
booktitle = {Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications},
title = {{{G}erma{N}et - a Lexical-Semantic Net for {G}erman}},
url = {https://aclanthology.org/W97-0802 https://aclanthology.org/W97-0802.pdf},
year = {1997}
}

@article{VISR12,
abstract = {This article introduces the tag genome, a data structure that extends the traditional tagging model to provide enhanced forms of user interaction. Just as a biological genome encodes an organism based on a sequence of genes, the tag genome encodes an item in an information space based on its relationship to a common set of tags. We present a machine learning approach for computing the tag genome, and we evaluate several learning models on a ground truth dataset provided by users. We describe an application of the tag genome called Movie Tuner which enables users to navigate from one item to nearby items along dimensions represented by tags. We present the results of a 7-week field trial of 2,531 users of Movie Tuner and a survey evaluating users subjective experience. Finally, we outline the broader space of applications of the tag genome. {\textcopyright} 2012 ACM.},
annote = {This is source 13 from DESC15, listed there as
"One exception is [13], which proposes a critique based movie recommender. Using a supervised method, their system allows users to specify, for instance, that they want “a film like this one, but grit- tier”. Similarly, [14] proposes a critique based image search engine, based on a supervised method that learns the degree to which visual attributes apply to images, e.g. “I want to buy shoes like these, but shinier”"},
author = {Vig, Jesse and Sen, Shilad and Riedl, John},
doi = {10.1145/2362394.2362395},
file = {:home/chris/Documents/Mendeley Desktop/Vig, Sen, Riedl/Vig, Sen, Riedl - 2012 - The tag genome Encoding community knowledge to support novel interaction.pdf:pdf},
issn = {21606463},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {Tagging,conversational recommenders,data mining,information retrieval,machine learning,recommender systems},
number = {3},
title = {{The tag genome: Encoding community knowledge to support novel interaction}},
volume = {2},
year = {2012}
}

@article{Mikolov:Regularities,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.},
author = {Mikolov, Tomas and Yih, Wen Tau and Zweig, Geoffrey},
file = {:home/chris/Documents/Mendeley Desktop/Mikolov, Yih, Zweig/Mikolov, Yih, Zweig - 2013 - Linguistic Regularities in Continuous Space Word Representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of the 2nd Workshop on Computational Linguistics for Literature, CLfL 2013 at the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2013},
pages = {746--751},
title = {{Linguistic Regularities in Continuous Space Word Representations}},
url = {http://research.microsoft.com/en-},
year = {2013}
}
