Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop
Chris' script ran over it.

@article{Fiorini2013,
author = {Fiorini, Sandro and G{\"{a}}rdenfors, Peter and Abel, Mara},
doi = {10.1007/s10339-013-0585-x},
file = {:home/chris/Documents/Mendeley Desktop/Fiorini, G{\"{a}}rdenfors, Abel/Fiorini, G{\"{a}}rdenfors, Abel - 2013 - Representing part-whole relations in conceptual spaces.pdf:pdf},
journal = {Cognitive processing},
month = {oct},
title = {{Representing part-whole relations in conceptual spaces}},
volume = {15},
year = {2013}
}

@article{Alshaikh2019,
abstract = {Conceptual spaces are geometric representations of meaning that were proposed by G{\"{a}}rdenfors (2000). They share many similarities with the vector space embeddings that are commonly used in natural language processing. However, rather than representing entities in a single vector space, conceptual spaces are usually decomposed into several facets, each of which is then modelled as a relatively low-dimensional vector space. Unfortunately, the problem of learning such conceptual spaces has thus far only received limited attention. To address this gap, we analyze how, and to what extent, a given vector space embedding can be decomposed into meaningful facets in an unsupervised fashion. While this problem is highly challenging, we show that useful facets can be discovered by relying on word embeddings to group semantically related features.},
author = {Alshaikh, Rana and Bouraoui, Zied and Schockaert, Steven},
doi = {10.18653/v1/k19-1013},
file = {:home/chris/Documents/Mendeley Desktop/Alshaikh, Bouraoui, Schockaert/Alshaikh, Bouraoui, Schockaert - 2019 - Learning conceptual spaces with disentangled facets.pdf:pdf},
isbn = {9781950737727},
journal = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},
pages = {131--139},
publisher = {Association for Computational Linguistics},
title = {{Learning conceptual spaces with disentangled facets}},
url = {https://aclanthology.org/K19-1013},
year = {2019}
}

@book{Gardenfors2000,
abstract = {Within cognitive science, two approaches currently dominate the problem of modeling representations. The symbolic approach views cognition as computation involving symbolic manipulation. Connectionism, a special case of associationism, models associations using artificial neuron networks. Peter G{\"{a}}rdenfors offers his theory of conceptual representations as a bridge between the symbolic and connectionist approaches.},
author = {G{\"{a}}rdenfors, Peter},
booktitle = {Conceptual Spaces},
doi = {10.7551/mitpress/2076.001.0001},
publisher = {MIT Press},
title = {{Conceptual Spacess: The Geometry of Thought}},
url = {https://direct.mit.edu/books/book/2532/Conceptual-SpacesThe-Geometry-of-Thought},
year = {2000}
}

@inproceedings{Ager2018,
abstract = {In this paper we consider semantic spaces consisting of objects from some particular domain (e.g. IMDB movie reviews). Various authors have observed that such semantic spaces often model salient features (e.g. how scary a movie is) as directions. These feature directions allow us to rank objects according to how much they have the corresponding feature, and can thus play an important role in interpretable classifiers, recommendation systems, or entity-oriented search engines, among others. Methods for learning semantic spaces, however, are mostly aimed at modelling similarity. In this paper, we argue that there is an inherent trade-off between capturing similarity and faithfully modelling features as directions. Following this observation, we propose a simple method to fine-tune existing semantic spaces, with the aim of improving the quality of their feature directions. Crucially, our method is fully unsupervised, requiring only a bag-of-words representation of the objects as input.},
address = {Stroudsburg, PA, USA},
author = {Ager, Thomas and Ku{\v{z}}elka, Ondřej and Schockaert, Steven},
booktitle = {CoNLL 2018 - 22nd Conference on Computational Natural Language Learning, Proceedings},
doi = {10.18653/v1/k18-1051},
file = {:home/chris/Documents/Mendeley Desktop/Ager, Ku{\v{z}}elka, Schockaert/Ager, Ku{\v{z}}elka, Schockaert - 2018 - Modelling salient features as directions in fine-tuned semantic spaces.pdf:pdf},
isbn = {9781948087728},
pages = {530--540},
publisher = {Association for Computational Linguistics},
title = {{Modelling salient features as directions in fine-tuned semantic spaces}},
url = {http://aclweb.org/anthology/K18-1051},
year = {2018}
}

@misc{grootendorst2020keybert,
author = {Grootendorst, Maarten},
doi = {10.5281/zenodo.4461265},
publisher = {Zenodo},
title = {{KeyBERT: Minimal keyword extraction with BERT.}},
url = {https://doi.org/10.5281/zenodo.4461265},
year = {2020},
howpublished = {\url{https://doi.org/10.5281/zenodo.4461265}}
}

@article{stockmann2005,
abstract = {Dieser Beitrag beschreibt die Konzeption, den Funktionsumfang und Erfahrungswerte der Open-Source-eLearning-Plattform Stud.IP. Der Funktionsumfang umfasst f{\"{u}}r jede einzelne Veranstaltung Ablaufpl{\"{a}}ne, das Hochladen von Hausarbeiten, Diskussionsforen, pers{\"{o}}nliche Homepages, Chatr{\"{a}}ume u.v.a. Ziel ist es hierbei, eine Infrastruktur des Lehrens und Lernens anzubieten, die dem Stand der Technik entspricht. Wissenschaftliche Einrichtungen finden zudem eine leistungsstarke Umgebung zur Verwaltung ihres Personals, Pflege ihrer Webseiten und der automatischer Erstellung von Veranstaltungs- oder Personallisten vor. Betreiber k{\"{o}}nnen auf ein verl{\"{a}}ssliches Supportsystem zugreifen, dass sie an der Weiterentwicklung durch die Entwickler- und Betreiber-Community teilhaben l{\"{a}}sst.},
author = {Stockmann, Ralf and Berg, Alexander},
issn = {1860-7470},
journal = {eleed},
keywords = {Kursplattform; Kursverwaltung; Learnmanagement; Le},
number = {1},
title = {{Stud.IP}},
url = {http://nbn-resolving.de/urn:nbn:de:0009-5-845},
volume = {1},
year = {2005}
}

@article{VISR12,
abstract = {This article introduces the tag genome, a data structure that extends the traditional tagging model to provide enhanced forms of user interaction. Just as a biological genome encodes an organism based on a sequence of genes, the tag genome encodes an item in an information space based on its relationship to a common set of tags. We present a machine learning approach for computing the tag genome, and we evaluate several learning models on a ground truth dataset provided by users. We describe an application of the tag genome called Movie Tuner which enables users to navigate from one item to nearby items along dimensions represented by tags. We present the results of a 7-week field trial of 2,531 users of Movie Tuner and a survey evaluating users subjective experience. Finally, we outline the broader space of applications of the tag genome. {\textcopyright} 2012 ACM.},
annote = {This is source 13 from DESC15, listed there as
"One exception is [13], which proposes a critique based movie recommender. Using a supervised method, their system allows users to specify, for instance, that they want “a film like this one, but grit- tier”. Similarly, [14] proposes a critique based image search engine, based on a supervised method that learns the degree to which visual attributes apply to images, e.g. “I want to buy shoes like these, but shinier”"},
author = {Vig, Jesse and Sen, Shilad and Riedl, John},
doi = {10.1145/2362394.2362395},
file = {:home/chris/Documents/Mendeley Desktop/Vig, Sen, Riedl/Vig, Sen, Riedl - 2012 - The tag genome Encoding community knowledge to support novel interaction.pdf:pdf},
issn = {21606463},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {Tagging,conversational recommenders,data mining,information retrieval,machine learning,recommender systems},
number = {3},
title = {{The tag genome: Encoding community knowledge to support novel interaction}},
volume = {2},
year = {2012}
}

@inproceedings{Schurz2021,
abstract = {Digital Study Assistants (DSA) aim to support individual learning processes by designing them appropriately and efficiently based on recommendations. In this paper we present a prototype of a DSA for students in higher education of three German universities. The digital data driven DSA is integrated into the local learning management system and consists of recommender modules with a certain kind of recommendation for a specific purpose, e.g., recommending Academic Contacts that fit an expressed academic interest. The modules implemented so far use a wide range of methods: Classic rule-based Artificial Intelligence (AI) or Neural Networks, that can detect complex features and patterns in large data sets. To evaluate the current prototype of the DSA we used a mixed methods design approach with concurrently collected user data and qualitative data. A first insight in the user data suggests that recommender modules providing personalized recommendations are more likely to be used by students. A focus group discussion with students confirmed these findings with the suggestion to make the DSA more personal, individual, interactive, supportive, and user-friendly. In conclusion we present ideas for the further development of the prototype based on these findings.},
annote = {-gelesen-
Am Anfang stehen 3 possible quellen zu educational resources, rest egal},
author = {Schurz, Katharina and Schrumpf, Johannes and Weber, Felix and L{\"{u}}bcke, Maren and Seyfeli, Funda and Wannemacher, Klaus},
booktitle = {18th International Conference on Cognition and Exploratory Learning in Digital Age, CELDA 2021},
doi = {10.33965/celda2021_202108l006},
file = {:home/chris/Documents/Mendeley Desktop/Schurz et al/Schurz et al. - 2021 - TOWARDS A USER FOCUSED DEVELOPMENT OF A DIGITAL STUDY ASSISTANT THROUGH A MIXED METHODS DESIGN.pdf:pdf},
isbn = {9789898704337},
keywords = {AI in education,Digital Study Assistant (DSA),Higher education,Individual learning process,Innovative Learning Management Systems (LMS),Mixed methods design,Recommender systems},
number = {Celda},
pages = {45--52},
title = {{TOWARDS A USER FOCUSED DEVELOPMENT OF A DIGITAL STUDY ASSISTANT THROUGH A MIXED METHODS DESIGN}},
year = {2021}
}

@article{Gardenfors2001,
abstract = {Understanding the process of categorization is a primary research goal in artificial intelligence. The conceptual space framework provides a flexible approach to modeling context-sensitive categorization via a geometrical representation designed for modeling and managing concepts. In this paper we show how algorithms developed in computational geometry, and the Region Connection Calculus can be used to model important aspects of categorization in conceptual spaces. In particular, we demonstrate the feasibility of using existing geometric algorithms to build and manage categories in conceptual spaces, and we show how the Region Connection Calculus can be used to reason about categories and other conceptual regions.},
author = {G{\"{a}}rdenfors, Peter and Williams, Mary Anne},
file = {:home/chris/Documents/Mendeley Desktop/G{\"{a}}rdenfors, Williams/G{\"{a}}rdenfors, Williams - 2001 - Reasoning about categories in conceptual spaces.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {385--392},
title = {{Reasoning about categories in conceptual spaces}},
year = {2001}
}

@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable
posterior distributions, and large datasets? We introduce a stochastic
variational inference and learning algorithm that scales to large datasets and,
under some mild differentiability conditions, even works in the intractable
case. Our contributions is two-fold. First, we show that a reparameterization
of the variational lower bound yields a lower bound estimator that can be
straightforwardly optimized using standard stochastic gradient methods. Second,
we show that for i.i.d. datasets with continuous latent variables per
datapoint, posterior inference can be made especially efficient by fitting an
approximate inference model (also called a recognition model) to the
intractable posterior using the proposed lower bound estimator. Theoretical
advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
doi = {10.48550/arxiv.1312.6114},
eprint = {1312.6114},
file = {:home/chris/Documents/Mendeley Desktop/Kingma, Welling/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Auto-Encoding Variational Bayes}},
url = {https://arxiv.org/abs/1312.6114v10},
year = {2013}
}

@inproceedings{Maas2011,
address = {Portland, Oregon, USA},
author = {Maas, Andrew L and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
pages = {142--150},
publisher = {Association for Computational Linguistics},
title = {{Learning Word Vectors for Sentiment Analysis}},
url = {http://www.aclweb.org/anthology/P11-1015},
year = {2011}
}

@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I.},
doi = {10.1016/b978-0-12-411519-4.00006-9},
file = {:home/chris/Documents/Mendeley Desktop/Blei, Ng, Jordan/Blei, Ng, Jordan - 2003 - Latent Dirichlet allocation.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
number = {4-5},
pages = {993--1022},
title = {{Latent Dirichlet allocation}},
volume = {3},
year = {2003}
}

@article{Olcott2012,
abstract = {This reflection examines some of the continuing and emerging issues in the open educational resources (OER) field. These include blending OER with university management structures; formal and non-formal OER; the need for sustainable OER business models; and expanding awareness, adoption, and use of OER. In the future, research will need to examine the concept of open educational practices (OEP) and OER issues relevant to faculty incentives and career advancement in the university. The author suggests there is no silver bullet solution to the "open" road ahead. Proprietary and open content will coexist in the education sector. OER are not a panacea for resolving all the range of global education issues and divides. OER are, however, a valuable resource that must be developed and sustained. OER may ultimately be the genuine equalizer for education and for empowering social inclusion in a pluralistic, multicultural, and imperfect world. {\textcopyright} 2012 Copyright Open and Distance Learning Association of Australia, Inc.},
author = {Olcott, Don},
doi = {10.1080/01587919.2012.700561},
issn = {01587919},
journal = {Distance Education},
keywords = {OER,emerging issues,management,open educational practices,universities},
month = {aug},
number = {2},
pages = {283--290},
publisher = {Routledge},
title = {{OER perspectives: Emerging issues for universities}},
url = {https://www.tandfonline.com/doi/abs/10.1080/01587919.2012.700561},
volume = {33},
year = {2012}
}

@inproceedings{Alshaikh2021,
abstract = {Various methods have already been proposed for learning entity embeddings from text descriptions. Such embeddings are commonly used for inferring properties of entities, for recommendation and entity-oriented search, and for injecting background knowledge into neural architectures, among others. Entity embeddings essentially serve as a compact encoding of a similarity relation, but similarity is an inherently multi-faceted notion. By representing entities as single vectors, existing methods leave it to downstream applications to identify these different facets, and to select the most relevant ones. In this paper, we propose a model that instead learns several vectors for each entity, each of which intuitively captures a different aspect of the considered domain. We use a mixture-of-experts formulation to jointly learn these facet-specific embeddings. The individual entity embeddings are learned using a variant of the GloVe model, which has the advantage that we can easily identify which properties are modelled well in which of the learned embeddings. This is exploited by an associated gating network, which uses pre-trained word vectors to encourage the properties that are modelled by a given embedding to be semantically coherent, i.e. to encourage each of the individual embeddings to capture a meaningful facet.},
author = {Alshaikh, Rana and Bouraoui, Zied and Jeawak, Shelan and Schockaert, Steven},
doi = {10.18653/v1/2020.coling-main.449},
file = {:home/chris/Documents/Mendeley Desktop/Alshaikh et al/Alshaikh et al. - 2021 - A Mixture-of-Experts Model for Learning Multi-Facet Entity Embeddings.pdf:pdf},
pages = {5124--5135},
publisher = {Online},
title = {{A Mixture-of-Experts Model for Learning Multi-Facet Entity Embeddings}},
year = {2021}
}

@article{Mikolov2013,
annote = {https://arxiv.org/pdf/1301.3781v3.pdf
Contents
* 1 Introduction
1.1 Goals of the Paper
1.2 Previous Work
2 Model Architectures
2.1 Feedforward Neural Net Language Model (NNLM)
2.2 Recurrent Neural Net Language Model (RNNLM)
2.3 Parallel Training of Neural Networks
3 New Log-linear Models
3.1 Continuous Bag-of-Words Model
3.2 Continuous Skip-gram Model
4 Results
4.1 Task Description
4.2 Maximization of Accuracy
4.3 Comparison of Model Architectures
4.4 Large Scale Parallel Training of Models
4.5 Microsoft Research Sentence Completion Challenge
5 Examples of the Learned Relationships
6 Conclusion
7 Follow-Up Work----------1301.3781v3.pdf
Contents
* 1 Introduction
1.1 Goals of the Paper
1.2 Previous Work
2 Model Architectures
2.1 Feedforward Neural Net Language Model (NNLM)
2.2 Recurrent Neural Net Language Model (RNNLM)
2.3 Parallel Training of Neural Networks
3 New Log-linear Models
3.1 Continuous Bag-of-Words Model
3.2 Continuous Skip-gram Model
4 Results
4.1 Task Description
4.2 Maximization of Accuracy
4.3 Comparison of Model Architectures
4.4 Large Scale Parallel Training of Models
4.5 Microsoft Research Sentence Completion Challenge
5 Examples of the Learned Relationships
6 Conclusion
7 Follow-Up Work},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {:home/chris/Documents/Mendeley Desktop/Mikolov et al/Mikolov et al. - 2013 - Efficient estimation of word representations in vector space.pdf:pdf},
journal = {arXiv preprint arXiv:1301.3781},
keywords = {Folder - Deepdream,Folder - Deepdream - NLP},
mendeley-tags = {Folder - Deepdream,Folder - Deepdream - NLP},
title = {{Efficient estimation of word representations in vector space}},
url = {http://arxiv.org/abs/1301.3781 https://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}

@inproceedings{Goodfellow2014,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N and Weinberger, K Q},
publisher = {Curran Associates, Inc.},
title = {{Generative Adversarial Nets}},
url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
volume = {27},
year = {2014}
}

@inproceedings{Mikolov2013a,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.48550/arxiv.1310.4546},
eprint = {1310.4546},
file = {:home/chris/Documents/Mendeley Desktop/Mikolov et al/Mikolov et al. - 2013 - Distributed representations ofwords and phrases and their compositionality.pdf:pdf},
issn = {10495258},
keywords = {()},
month = {oct},
publisher = {Neural information processing systems foundation},
title = {{Distributed representations ofwords and phrases and their compositionality}},
url = {https://arxiv.org/abs/1310.4546v1},
year = {2013}
}

@inproceedings{nothman-etal-2018-stop,
abstract = {Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. {``}hasn{'}t{''} but not {``}hadn{'}t{''}) and inclusions ({``}computer{''}), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.},
address = {Melbourne, Australia},
author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
booktitle = {Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})},
doi = {10.18653/v1/W18-2502},
pages = {7--12},
publisher = {Association for Computational Linguistics},
title = {{Stop Word Lists in Free Open-source Software Packages}},
url = {https://aclanthology.org/W18-2502},
year = {2018}
}

@article{Hernandez-Conde2017,
author = {Hern{\'{a}}ndez-Conde, Jos{\'{e}}},
doi = {10.1007/s11229-016-1123-z},
file = {:home/chris/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Conde/Hern{\'{a}}ndez-Conde - 2017 - A case against convexity in conceptual spaces.pdf:pdf},
journal = {Synthese},
title = {{A case against convexity in conceptual spaces}},
volume = {194},
year = {2017}
}

@article{Smith2017,
abstract = {Amazon is well-known for personalization and recommendations, which help customers discover items they might otherwise not have found. In this update to their original paper, the authors discuss some of the changes as Amazon has grown.},
author = {Smith, Brent and Linden, Greg},
doi = {10.1109/MIC.2017.72},
file = {:home/chris/Documents/Mendeley Desktop/Smith, Linden/Smith, Linden - 2017 - Two Decades of Recommender Systems at Amazon.com.pdf:pdf},
issn = {10897801},
journal = {IEEE Internet Computing},
keywords = {Internet/Web technologies,artificial intelligence,intelligent systems,item-based collaborative filtering,personalization,recommender systems,software engineering},
month = {may},
number = {3},
pages = {12--18},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Two Decades of Recommender Systems at Amazon.com}},
volume = {21},
year = {2017}
}

@article{Turney2010,
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
archivePrefix = {arXiv},
arxivId = {1003.1141v1},
author = {Turney, Peter D and Pantel, Patrick},
eprint = {1003.1141v1},
file = {:home/chris/Documents/Mendeley Desktop/Turney, Pantel/Turney, Pantel - 2010 - From Frequency to Meaning Vector Space Models of Semantics.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {141--188},
title = {{From Frequency to Meaning: Vector Space Models of Semantics}},
url = {http://www.natcorp.ox.ac.uk/.},
volume = {37},
year = {2010}
}

@misc{MaartenGr2021,
author = {Grootendorst, Maarten},
booktitle = {GitHub repository},
howpublished = {\url{https://github.com/MaartenGr/KeyBERT}},
publisher = {GitHub},
title = {{KeyBERT}},
year = {2021}
}

@inproceedings{Kohonen1997,
author = {Kohonen, T},
booktitle = {Proceedings of International Conference on Neural Networks (ICNN'97)},
doi = {10.1109/ICNN.1997.611622},
pages = {PL1--PL6 vol.1},
title = {{Exploration of very large databases by self-organizing maps}},
volume = {1},
year = {1997}
}

@article{Linden2003,
abstract = {Recommendation algorithms are best known for their use on e-commerce Web sites. It provides an effective form of targeted marketing by creating a personalized shopping experience for each customer. Amazon.com uses them to personalize the online store for each customer. Most of these algorithms start by finding a set of customers whose purchased and rated items overlap the user's purchased and rated items.},
author = {Linden, Greg and Smith, Brent and York, Jeremy},
doi = {10.1109/MIC.2003.1167344},
file = {:home/chris/Documents/Mendeley Desktop/Linden, Smith, York/Linden, Smith, York - 2003 - Amazon.com recommendations Item-to-item collaborative filtering.pdf:pdf},
issn = {10897801},
journal = {IEEE Internet Computing},
number = {1},
pages = {76--80},
title = {{Amazon.com recommendations: Item-to-item collaborative filtering}},
volume = {7},
year = {2003}
}

@inproceedings{Alshaikh2020,
abstract = {Conceptual spaces are geometric meaning representations in which similar entities are represented by similar vectors. They are widely used in cognitive science, but there has been relatively little work on learning such representations from data. In particular, while standard representation learning methods can be used to induce vector space embeddings from text corpora, these differ from conceptual spaces in two crucial ways. First, the dimensions of a conceptual space correspond to salient semantic features, known as quality dimensions, whereas the dimensions of learned embeddings typically lack any clear interpretation. This has been partially addressed in previous work, which has shown that it is possible to identify directions in learned vector spaces which capture semantic features. Second, conceptual spaces are normally organised into a set of domains, each of which is associated with a separate vector space. In contrast, learned embeddings represent all entities in a single vector space. Our hypothesis in this paper is that such single-space representations are sub-optimal for learning quality dimensions, due to the fact that semantic features are often only relevant to a subset of the entities. We show that this issue can be mitigated by identifying features in a hierarchical fashion. Intuitively, the top-level features split the vector space into domains, allowing us to subsequently identify domain-specific quality dimensions.},
annote = {-Conceptual Spaces have two advantages over vector-spaces: 1) they have interpretable dimensions, 2) they are have sets of domains, each with their own vector-space
-identify features in hierachical fashion. Top-Level features split vector space into domains, and inside that you have domain-specific quality dimensions.
Improvements over Schockhart 2015!},
author = {Alshaikh, Rana and Bouraoui, Zied and Schockaert, Steven},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2020/494},
file = {:home/chris/Documents/Mendeley Desktop/Alshaikh, Bouraoui, Schockaert/Alshaikh, Bouraoui, Schockaert - 2020 - Hierarchical linear disentanglement of data-driven conceptual spaces.pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
keywords = {Humans and AI: Cognitive Modeling,Machine Learning: Interpretability,Natural Language Processing: Natural Language Proc},
pages = {3573--3579},
title = {{Hierarchical linear disentanglement of data-driven conceptual spaces}},
year = {2020}
}

@inproceedings{Chen2018,
abstract = {The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then outperformed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English→French and English→German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1804.09849},
author = {Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Shazeer, Noam and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Schuster, Mike and Chen, Zhifeng and Wu, Yonghui and Hughes, Macduff},
booktitle = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
doi = {10.18653/v1/p18-1008},
eprint = {1804.09849},
file = {:home/chris/Documents/Mendeley Desktop/Chen et al/Chen et al. - 2018 - The best of both worlds Combining recent advances in neural machine translation.pdf:pdf},
isbn = {9781948087322},
pages = {76--86},
title = {{The best of both worlds: Combining recent advances in neural machine translation}},
url = {https://github.com/tensorflow/tensor2tensor},
volume = {1},
year = {2018}
}

@inproceedings{Schockaert2011,
author = {Schockaert, Steven and Prade, Henri},
doi = {10.1007/978-3-642-23580-1_16},
file = {:home/chris/Documents/Mendeley Desktop/Schockaert, Prade/Schockaert, Prade - 2011 - Interpolation and Extrapolation in Conceptual Spaces A Case Study in the Music Domain.pdf:pdf},
isbn = {978-3-642-23579-5},
pages = {217--231},
title = {{Interpolation and Extrapolation in Conceptual Spaces: A Case Study in the Music Domain}},
year = {2011}
}

@article{Carmel2009,
abstract = {This work investigates cluster labeling enhancement by utilizing Wikipedia, the free on-line encyclopedia. We describe a general framework for cluster labeling that extracts candidate labels from Wikipedia in addition to important terms that are extracted directly from the text. The "labeling quality" of each candidate is then evaluated by several independent judges and the top evaluated candidates are recommended for labeling. Our experimental results reveal that the Wikipedia labels agree with manual labels associated by humans to a cluster, much more than with significant terms that are extracted directly from the text. We show that in most cases even when human's associated label appears in the text, pure statistical methods have difficulty in identifying them as good descriptors. Furthermore, our experiments show that for more than 85% of the clusters in our test collection, the manual label (or an inflection, or a synonym of it) appears in the top five labels recommended by our system. Copyright 2009 ACM.},
author = {Carmel, David and Roitman, Haggai and Zwerdling, Naama},
doi = {10.1145/1571941.1571967},
file = {:home/chris/Documents/Mendeley Desktop/Carmel, Roitman, Zwerdling/Carmel, Roitman, Zwerdling - 2009 - Enhancing cluster labeling using wikipedia.pdf:pdf},
isbn = {9781605584836},
journal = {Proceedings - 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009},
keywords = {Cluster labeling,Wikipedia},
pages = {139--146},
title = {{Enhancing cluster labeling using wikipedia}},
year = {2009}
}

@book{Schroder2015,
address = {Bad Heilbrunn},
author = {Schr{\"{o}}der, Marco},
isbn = {978-3-7815-2015-8},
pages = {224},
publisher = {Klinkhardt},
title = {{Studienwahl unter den Folgen einer radikalen Differenzierung}},
year = {2015}
}

@inproceedings{Schrumpf2021DELPHI,
address = {Bonn},
author = {Schrumpf, Johannes and Weber, Felix and Thelen, Tobias},
booktitle = {DELFI 2021},
editor = {Kienle, Andrea and Harrer, Andreas and Haake, Joerg M and Lingnau, Andreas},
pages = {283--288},
publisher = {Gesellschaft f{\"{u}}r Informatik e.V.},
title = {{A Neural Natural Language Processing System for Educational Resource Knowledge Domain Classification}},
year = {2021}
}

@article{deerwester,
abstract = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. {\textcopyright} 1990 John Wiley \& Sons, Inc.},
author = {Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
doi = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
file = {:home/chris/Documents/Mendeley Desktop/Deerwester et al/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:pdf},
journal = {Journal of the American Society for Information Science},
number = {6},
pages = {391--407},
title = {{Indexing by latent semantic analysis}},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
volume = {41},
year = {1990}
}

@article{Gardenfors2004,
abstract = {The Semantic Web is not semantic. It is good for syllogistic reasoning, but there is much more to semantics than syllogisms. I argue that the current Semantic Web is too dependent on symbolic representations of information structures, which limits its representational capacity. As a remedy, I propose conceptual spaces as a tool for expressing more of the semantics. Conceptual spaces are built up from quality dimensions that have geometric or topological structures. With the aid of the dimensions, similarities between objects can easily be represented and it is argued that similarity is a central aspect of semantic content. By sorting the dimensions into domains, I define properties and concepts and show how prototype effects of concepts can be treated with the aid of conceptual spaces. I present an outline of how one can reconstruct most of the taxonomies and other meta-data that are explicitly coded in the current Semantic Web and argue that inference engines on the symbolic level will become largely superfluous. As an example of the semantic power of conceptual spaces, I show how concept combinations can be analysed in a much richer and more accurate way than in the classical logical approach.},
annote = {First description of CS: "Conceptual spaces are built up from quality dimensions that have geometric or topological structures. With the aid of the dimensions, similarities between objects can easily be represented and it is argued that similarity is a central aspect of semantic content. By sorting the dimensions into domains, I define properties and concepts and show how prototype effects of concepts can be treated with the aid of conceptual spaces."},
author = {G{\"{a}}rdenfors, P},
file = {:home/chris/Documents/Mendeley Desktop/G{\"{a}}rdenfors/G{\"{a}}rdenfors - 2004 - How to make the semantic web more semantic.pdf:pdf},
journal = {Formal Ontology in Information Systems. IOS Press},
pages = {17--36},
title = {{How to make the semantic web more semantic}},
url = {http://yaxu.org/tmp/Gardenfors04.pdf},
year = {2004}
}

@article{Cohn1997a,
abstract = {This paper surveys the work of the qualitative spatial reasoning group at the University of Leeds. The group has developed a number of logical calculi for representing and reasoning with qualitative spatial relations over regions. We motivate the use of regions as the primary spatial entity and show how a rich language can be built up from surprisingly few primitives. This language can distinguish between convex and a variety of concave shapes and there is also an extension which handles regions with uncertain boundaries. We also present a variety of reasoning techniques, both for static and dynamic situations. A number of possible application areas are briefly mentioned.},
author = {Cohn, Anthony G and Bennett, Brandon and Gooday, John and Gotts, Nicholas Mark},
doi = {10.1023/A:1009712514511},
file = {:home/chris/Documents/Mendeley Desktop/Cohn et al/Cohn et al. - 1997 - Qualitative Spatial Representation and Reasoning with the Region Connection Calculus(2).pdf:pdf},
issn = {1573-7624},
journal = {GeoInformatica},
number = {3},
pages = {275--316},
title = {{Qualitative Spatial Representation and Reasoning with the Region Connection Calculus}},
url = {https://doi.org/10.1023/A:1009712514511},
volume = {1},
year = {1997}
}

@inproceedings{2013ISOI,
title = {{ISO / IEC 25010 : 2011 Systems and software engineering — Systems and software Quality Requirements and Evaluation ( SQuaRE ) — System and software quality models}},
year = {2013}
}

@article{Sarwar2000,
author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
file = {:home/chris/Documents/Mendeley Desktop/Sarwar et al/Sarwar et al. - 2000 - Analysis of Recommendation Algorithms for E-Commerce(2).pdf:pdf},
title = {{Analysis of Recommendation Algorithms for E-Commerce}},
year = {2000}
}

@article{Atenas2014,
abstract = {Scholars are increasingly being asked to share teaching materials, publish in open access journals, network in social media, and reuse open educational resources (OER). The theoretical benefits of Open Educational Practices (OEP) have become understood in the academic community but thus far, the use of OER has not been rapidly adopted. We aim to understand the challenges academics face with in attempting to adopt OEP, and identify whether these are related to or stem from the functionalities afforded by current repositories of OER (ROER). By understanding what academics and experts consider good practices, we can develop guidelines for quality in the defile:///home/chris/Downloads/Open_Teaching_Landscapes.pdfvelopment of ROER. In this article we present the findings from a study surveying academics using OER and experts who develop and/or work with ROER. We conclude by suggesting a framework to enhance the development and quality of ROER. http://dx.doi.org/10.5944/openpraxis.6.1.81},
author = {Atenas, Javiera and Havemann, Leo and Priego, Ernesto},
doi = {10.5944/OPENPRAXIS.6.1.81},
file = {:home/chris/Documents/Mendeley Desktop/Atenas, Havemann, Priego/Atenas, Havemann, Priego - 2014 - Opening teaching landscapes The importance of quality assurance in the delivery of open educational re.pdf:pdf},
journal = {Open Praxis},
month = {feb},
number = {1},
publisher = {UNED - Universidad Nacional de Educacion a Distancia},
title = {{Opening teaching landscapes: The importance of quality assurance in the delivery of open educational resources}},
volume = {6},
year = {2014}
}

@article{Cohn1997,
abstract = {This paper surveys the work of the qualitative spatial reasoning group at the University of Leeds. The group has developed a number of logical calculi for representing and reasoning with qualitative spatial relations over regions. We motivate the use of regions as the primary spatial entity and show how a rich language can be built up from surprisingly few primitives. This language can distinguish between convex and a variety of concave shapes and there is also an extension which handles regions with uncertain boundaries. We also present a variety of reasoning techniques, both for static and dynamic situations. A number of possible application areas are briefly mentioned.},
annote = {Gelesen: Kapitel 1},
author = {Cohn, Anthony G. and Bennett, Brandon and Gooday, John and Gotts, Nicholas Mark},
doi = {10.1023/A:1009712514511},
file = {:home/chris/Documents/Mendeley Desktop/Cohn et al/Cohn et al. - 1997 - Qualitative Spatial Representation and Reasoning with the Region Connection Calculus.pdf:pdf},
issn = {13846175},
journal = {GeoInformatica},
keywords = {Qualitative spatial reasoning,Shape,Spatial logics,Topology,Vague boundaries},
number = {3},
pages = {275--316},
title = {{Qualitative Spatial Representation and Reasoning with the Region Connection Calculus}},
volume = {1},
year = {1997}
}

@article{Devlin2019,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:home/chris/Documents/Mendeley Desktop/Devlin et al/Devlin et al. - 2019 - BERT Pre-training of deep bidirectional transformers for language understanding.pdf:pdf},
isbn = {9781950737130},
journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
number = {Mlm},
pages = {4171--4186},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
volume = {1},
year = {2019}
}

@article{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the
Generative Adversarial Network that is able to learn disentangled
representations in a completely unsupervised manner. InfoGAN is a generative
adversarial network that also maximizes the mutual information between a small
subset of the latent variables and the observation. We derive a lower bound to
the mutual information objective that can be optimized efficiently, and show
that our training procedure can be interpreted as a variation of the Wake-Sleep
algorithm. Specifically, InfoGAN successfully disentangles writing styles from
digit shapes on the MNIST dataset, pose from lighting of 3D rendered images,
and background digits from the central digit on the SVHN dataset. It also
discovers visual concepts that include hair styles, presence/absence of
eyeglasses, and emotions on the CelebA face dataset. Experiments show that
InfoGAN learns interpretable representations that are competitive with
representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.48550/arxiv.1606.03657},
eprint = {1606.03657},
file = {:home/chris/Documents/Mendeley Desktop/Chen et al/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {2180--2188},
publisher = {Neural information processing systems foundation},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {https://arxiv.org/abs/1606.03657v1},
year = {2016}
}

@article{<unpublished>,
author = {<unpublished>},
doi = {18.18420/provided-by-editor-02},
file = {:home/chris/Documents/Mendeley Desktop/unpublished/unpublished - Unknown - A Data-Driven Study Assistant Architecture for Universities.pdf:pdf},
keywords = {ai in higher education,innovation of learning management,study assistants},
pages = {15--20},
title = {{A Data-Driven Study Assistant Architecture for Universities}}
}

@article{Schrumpf2021,
abstract = {In higher education, educational resources are the vessel with which information get transferred to the learner. Information on the content discussed in the scope of the educational resources, however, is implicit and must be inferred by the user by reading the resource title or through contextual information. In this paper we present a state-of-the-art neural natural language processing system, based on Google-BERT, that maps educational resource titles into one of 905 classes from the Dewey Decimal Classification (DDC) system. We present model architecture, training procedure dataset properties and our performance analysis methodology. We show that aside from classification performance, our model implicitly learns the class hierarchy inherent to the DDC.},
author = {Schrumpf, Johannes and Weber, Felix and Thelen, Tobias},
file = {:home/chris/Documents/Mendeley Desktop/Schrumpf, Weber, Thelen/Schrumpf, Weber, Thelen - 2021 - A Neural Natural Language Processing System for Educational Resource Knowledge Domain Classification.pdf:pdf},
journal = {DeLFI 2021 - Die 19. Fachtagung Bildungstechnologien},
keywords = {ai in higher education,machine learning,recommender systems},
pages = {283--288},
title = {{A Neural Natural Language Processing System for Educational Resource Knowledge Domain Classification}},
year = {2021}
}

@article{Mikolov:Regularities,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.},
author = {Mikolov, Tomas and Yih, Wen Tau and Zweig, Geoffrey},
file = {:home/chris/Documents/Mendeley Desktop/Mikolov, Yih, Zweig/Mikolov, Yih, Zweig - 2013 - Linguistic Regularities in Continuous Space Word Representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of the 2nd Workshop on Computational Linguistics for Literature, CLfL 2013 at the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2013},
pages = {746--751},
title = {{Linguistic Regularities in Continuous Space Word Representations}},
url = {http://research.microsoft.com/en-},
year = {2013}
}

@book{breiman1984classification,
abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
address = {Monterey, California: Wadsworth},
author = {Breiman, L. and Friedman, J.H. and Olshen, R.A. and Stone, C.J.},
doi = {https://doi.org/10.1201/9781315139470},
edition = {1st Editio},
isbn = {9781315139470},
pages = {368},
publisher = {Routledge},
title = {{Classification and Regression Trees}},
year = {1984}
}

@article{Molder2021a,
abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way. Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid.Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
author = {M{\"{o}}lder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and Tomkins-Tinch, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and K{\"{o}}ster, Johannes},
doi = {10.12688/F1000RESEARCH.29032.1},
file = {:home/chris/Documents/Mendeley Desktop/M{\"{o}}lder et al/M{\"{o}}lder et al. - 2021 - Sustainable data analysis with Snakemake(3).pdf:pdf},
issn = {2046-1402},
journal = {F1000Research},
keywords = {Lee S: Methodology, Software,Letcher B: Methodology, Software, Writing-Review &,Nahnsen S: Conceptualization,Rahmann S: Supervision, Writing-Review & Editing,Sochat V: Methodology, Software, Writing-Review &,Tomkins-Tinch CH: Methodology, Software, Writing-R,Twardziok SO: Methodology, Software,Wilm A: Methodology, Software, Writing-Review & Ed,adaptability,data analysis,reproducibility,scalability,sustainability,transparency,workflow management},
month = {jan},
pages = {33},
publisher = {F1000 Research Ltd},
title = {{Sustainable data analysis with Snakemake}},
url = {https://f1000research.com/articles/10-33},
volume = {10},
year = {2021}
}

@article{Ai2018,
abstract = {<p>Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms—especially the collaborative filtering (CF)- based approaches with shallow or deep models—usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users' historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines.</p>},
author = {Ai, Qingyao and Azizi, Vahid and Chen, Xu and Zhang, Yongfeng},
doi = {10.3390/a11090137},
file = {:home/chris/Documents/Mendeley Desktop/Ai et al/Ai et al. - 2018 - Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation.pdf:pdf},
issn = {1999-4893},
journal = {Algorithms},
keywords = {Collaborative filtering,Explainable recommendation,Knowledge-base embedding,Recommender systems},
month = {sep},
number = {9},
pages = {137},
publisher = {MDPI AG},
title = {{Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation}},
url = {http://www.mdpi.com/1999-4893/11/9/137},
volume = {11},
year = {2018}
}

@techreport{Derrac2015,
abstract = {Commonsense reasoning patterns such as interpolation and a fortiori inference have proven useful for dealing with gaps in structured knowledge bases. An important diculty in applying these reasoning patterns in practice is that they rely on fine-grained knowledge of how di↵erent concepts and entities are semantically related. In this paper, we show how the required semantic relations can be learned from a large collection of text documents. To this end, we first induce a conceptual space from the text documents, using multi-dimensional scaling. We then rely on the key insight that the required semantic relations correspond to qualitative spatial relations in this conceptual space. Among others, in an entirely unsupervised way, we identify salient directions in the conceptual space which correspond to interpretable relative properties such as 'more fruity than' (in a space of wines), resulting in a symbolic and interpretable representation of the conceptual space. To evaluate the quality of our semantic relations, we show how they can be exploited by a number of commonsense reasoning based classifiers. We experimentally show that these classifiers can outperform standard approaches, while being able to provide intuitive explanations of classification decisions. A number of crowdsourcing experiments provide further insights into the nature of the extracted semantic relations.},
author = {Derrac, Joaqu{\'{i}}n and Schockaert, Steven},
file = {:home/chris/Documents/Mendeley Desktop/Derrac, Schockaert/Derrac, Schockaert - 2015 - Inducing semantic relations from conceptual spaces a data-driven approach to plausible reasoning.pdf:pdf},
keywords = {Commonsense reasoning Email addresses: jderrac@csc,Conceptual spaces,Dimensionality reduction,Qualitative spatial relations,sschockaert@cscardiffacuk (Steven Schockaert)},
title = {{Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning}},
url = {http://dbpedia.org/About},
year = {2015}
}

@article{Dewey1876,
abstract = {[Selected sections]},
author = {Dewey, Melvil},
journal = {Search},
keywords = {classification},
pages = {44},
title = {{A Classification And Subject Index for Cataloguing And Arranging the Books And Pamphlets of a Library}},
url = {https://www.gutenberg.org/ebooks/12513},
year = {1876}
}

@misc{Ehlers2019,
address = {Karlsruhe},
author = {Ehlers, Ulf-Daniel and Kellermann, Sarah A.},
file = {:home/chris/Documents/Mendeley Desktop/Ehlers, Kellermann/Ehlers, Kellermann - 2019 - Future Skills - The Future of Learning and Higher education. Results of the International Future Skills D(2).pdf:pdf},
title = {{Future Skills - The Future of Learning and Higher education. Results of the International Future Skills Delphi Survey}},
url = {https://nextskills.org/wp-content/uploads/2020/04/2019-02-23-key-findings-future-skills-report1.pdf},
year = {2019},
howpublished = {\url{https://nextskills.org/wp-content/uploads/2020/04/2019-02-23-key-findings-future-skills-report1.pdf}}
}

@inproceedings{loper-bird-2002-nltk,
address = {Philadelphia, Pennsylvania, USA},
author = {Loper, Edward and Bird, Steven},
booktitle = {Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics},
doi = {10.3115/1118108.1118117},
pages = {63--70},
publisher = {Association for Computational Linguistics},
title = {{{NLTK} : The Natural Language Toolkit}},
url = {https://aclanthology.org/W02-0109},
year = {2002}
}

@article{Dayan1995,
abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
author = {Dayan, Peter and Hinton, Geoffrey E and Neal, Radford M and Zemel, Richard S},
doi = {10.1162/neco.1995.7.5.889},
file = {:home/chris/Documents/Mendeley Desktop/Dayan et al/Dayan et al. - 1995 - The Helmholtz Machine.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {sep},
number = {5},
pages = {889--904},
title = {{The Helmholtz Machine}},
url = {https://doi.org/10.1162/neco.1995.7.5.889},
volume = {7},
year = {1995}
}

@book{Gardenfors2000a,
author = {G{\"{a}}rdenfors, Peter},
doi = {10.7551/mitpress/2076.001.0001},
isbn = {0-262-07199-1},
pages = {318},
publisher = {Bradford Books},
title = {{Conceptual Spaces: The Geometry of Thought}},
year = {2000}
}

@inproceedings{Henrich,
abstract = {This paper introduces GernEdiT (short for: GermaNet Editing Tool), a new graphical user interface for the lexicographers and developers of GermaNet, the German version of the Princeton WordNet.},
address = {Valetta, Malta},
author = {Henrich, Verena and Hinrichs, Erhard},
booktitle = {Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010)},
file = {:home/chris/Documents/Mendeley Desktop/Henrich, Hinrichs/Henrich, Hinrichs - 2010 - GernEdiT-The GermaNet Editing Tool.pdf:pdf},
pages = {2228--2235},
title = {{GernEdiT-The GermaNet Editing Tool}},
url = {http://wordnet.princeton.edu/man/grind.1WN.html},
year = {2010}
}

@inproceedings{hamp-feldweg-1997-germanet,
author = {Hamp, Birgit and Feldweg, Helmut},
booktitle = {Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications},
title = {{{G}erma{N}et - a Lexical-Semantic Net for {G}erman}},
url = {https://aclanthology.org/W97-0802 https://aclanthology.org/W97-0802.pdf},
year = {1997}
}

@inproceedings{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: They lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of- words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc and Mikolov, Tomas},
booktitle = {31st International Conference on Machine Learning, ICML 2014},
eprint = {1405.4053},
file = {:home/chris/Documents/Mendeley Desktop/Le, Mikolov/Le, Mikolov - 2014 - Distributed representations of sentences and documents.pdf:pdf},
isbn = {9781634393973},
month = {may},
pages = {2931--2939},
title = {{Distributed representations of sentences and documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {4},
year = {2014}
}

@book{bird2009natural,
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
publisher = {" O'Reilly Media, Inc."},
title = {{Natural language processing with Python: analyzing text with the natural language toolkit}},
year = {2009}
}

@misc{nakatani2010langdetect,
author = {Shuyo, Nakatani},
title = {{Language Detection Library for Java}},
url = {http://code.google.com/p/language-detection/},
year = {2010},
howpublished = {\url{http://code.google.com/p/language-detection/}}
}

@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonyms that are in turn linked through semantic relations that determine word definitions. {\textcopyright} 1995, ACM. All rights reserved.},
author = {Miller, George A.},
doi = {10.1145/219717.219748},
file = {:home/chris/Documents/Mendeley Desktop/Miller/Miller - 1995 - WordNet A Lexical Database for English.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
month = {nov},
number = {11},
pages = {39--41},
publisher = {ACM
		PUB27
		New York, NY, USA},
title = {{WordNet: A Lexical Database for English}},
url = {https://dl.acm.org/doi/abs/10.1145/219717.219748},
volume = {38},
year = {1995}
}

@article{Bullinaria2007,
abstract = {The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures.},
author = {Bullinaria, John A. and Levy, Joseph P.},
doi = {10.3758/BF03193020},
file = {:home/chris/Documents/Mendeley Desktop/Bullinaria, Levy/Bullinaria, Levy - 2007 - Extracting semantic representations from word co-occurrence statistics A computational study.pdf:pdf},
issn = {1554-3528},
journal = {Behavior Research Methods 2007 39:3},
keywords = {Cognitive Psychology},
number = {3},
pages = {510--526},
pmid = {17958162},
publisher = {Springer},
title = {{Extracting semantic representations from word co-occurrence statistics: A computational study}},
url = {https://link.springer.com/article/10.3758/BF03193020},
volume = {39},
year = {2007}
}

@article{Lowe,
abstract = {This paper adds some theory to the growing literature of semantic space models. We motivate semantic space models from the perspective of distributional linguistics and show how an explicit mathematical formulation can provide a better understanding of existing models and suggest changes and improvements. In addition to pro- viding a theoretical framework for current models, we consider the implications of statistical aspects of language data that have not been addressed in the psychological modeling literature. Statistical approaches to language must deal principally with count data, and this data will typically have a highly skewed frequency distribution due to Zipf's law. We consider the consequences of these facts for the construction of semantic space models, and present methods for removing frequency biases from se- mantic space models.},
author = {Lowe, Will},
file = {:home/chris/Documents/Mendeley Desktop/Lowe/Lowe - 2001 - Towards a Theory of Semantic Space.pdf:pdf},
issn = {0305-0009},
journal = {Proceedings of the Twenty-Third Annual Conference of the Cognitive Science Society},
pages = {576--581},
pmid = {3980601},
title = {{Towards a Theory of Semantic Space}},
year = {2001}
}

@inproceedings{Guo,
abstract = {This paper presents an effective approach for parallel corpus mining using bilingual sentence embeddings. Our embedding models are trained to produce similar representations exclusively for bilingual sentence pairs that are translations of each other. This is achieved using a novel training method that introduces hard negatives consisting of sentences that are not translations but have some degree of semantic similarity. The quality of the resulting embeddings are evaluated on parallel corpus reconstruction and by assessing machine translation systems trained on gold vs. mined sentence pairs. We find that the sentence embeddings can be used to reconstruct the United Nations Parallel Corpus (Ziemski et al., 2016) at the sentence-level with a precision of 48.9% for en-fr and 54.9% for en-es. When adapted to document-level matching, we achieve a parallel document matching accuracy that is comparable to the significantly more computationally intensive approach of Uszkoreit et al. (2010). Using reconstructed parallel data, we are able to train NMT models that perform nearly as well as models trained on the original data (within 1-2 BLEU).},
archivePrefix = {arXiv},
arxivId = {1807.11906},
author = {Guo, Mandy and Shen, Qinlan and Yang, Yinfei and Ge, Heming and Cer, Daniel and Abrego, Gustavo Hernandez and Stevens, Keith and Constant, Noah and Sung, Yun Hsuan and Strope, Brian and Kurzweil, Ray},
booktitle = {WMT 2018 - 3rd Conference on Machine Translation, Proceedings of the Conference},
doi = {10.18653/v1/w18-6317},
eprint = {1807.11906},
file = {:home/chris/Documents/Mendeley Desktop/Guo et al/Guo et al. - Unknown - Effective Parallel Corpus Mining using Bilingual Sentence Embeddings.pdf:pdf},
isbn = {9781948087810},
pages = {165--176},
title = {{Effective Parallel Corpus Mining using Bilingual Sentence Embeddings}},
volume = {1},
year = {2018}
}
