\chapter{Background}

% SIDDATA (mit Educational Resources dabei, muss kein sub-chapter sein)
% Conceptual Spaces (What is this?)
% Required techniques and algorithms

\section{SIDDATA and Educational Resources}

\includeMD{pandoc_generated_latex/2_0_siddata}

\section{Conceptual Spaces}

\includeMD{pandoc_generated_latex/2_2_conceptualspaces}

\subsection{Data-Driven Generation of Conceptual Spaces}

\includeMD{pandoc_generated_latex/2_3_datadrivengeneration}

\textbf{Conceptual Space in our Case = Euclidian space with interpretable dimensions}

\subsection{Explainable Reasoning with Conceptual Spaces}
% was "computational reasoning"
\label{sec:reasoning}
\includeMD{pandoc_generated_latex/2_4_typesofreasoning}

\subsection{Other Related Work}
\label{sec:otherwork}

\includeMD{pandoc_generated_latex/2_5_relatedwork}


\section{Required Algorithms and Techniques}

Note that those that are not defined here are likely to be found in the Glossary \nameref{glo:defs}.

\subsubsection*{Semantic Knowledge Bases}

Lexical databases of semantic relations between words, the most famous of which being WordNet\footnote{\url{https://wordnet.princeton.edu/}}, link words in a graph that encodes explicit semantic relations like synonyms and hyponyms (subtypes/ \emph{is-a}-relationships). While neural %TODO: nicht neural, aber halt data-driven? naja das was word2vec undso sind... by-context-created/trained...?! similarity-based? -> DISTRIBUTIONAL MODELS (ones trained from the co-occurrence patterns of temrs)
embeddings may encode similar information implicitly, when relying on dictionary-based word encodings they are an important tool when using classical linguistic techniques. For the developed algorithm, the information how many hyponyms of a candidate word for a semantic direction %TODO: did I explain the algorithm well enough before this to throw this much information at the reader?!
occur in its corresponding text-corpus is highly relevant. To do that, WordNet \cite{Miller1995} and it's german equivalent, GermaNet \cite{hamp-feldweg-1997-germanet,Henrich}\footnote{\url{(https://uni-tuebingen.de/fakultaeten/philosophische-fakultaet/fachbereiche/neuphilologie/seminar-fuer-sprachwissenschaft/arbeitsbereiche/allg-sprachwissenschaft-computerlinguistik/ressourcen/lexica/germanet-1/)}}, are required in the respective step.


\subsubsection*{Word-weighting techniques}
\label{sec:word_count_techniques}

So, making 100\% sure:

\begin{itemize}
    \item term-frequency tf(term, doc): How often term occurs in doc
    \item doc-frequency df(term):  the number of documents in the corpus that contain the word
    \item summed term-frequncy sdf(term): How often term occurs in ANY DOC = tf(term, doc) forall doc
\end{itemize}

When comparing the \gls{bow}-representations of texts, it is reasonable to give more weight to \emph{surprising} words and less weight to expected ones. \q{The hypothesis is that surprising events, if shared by two vectors, are more discriminative of the similarity between the vectors than less surprising events.} \cite[156]{Turney2010} 
Another crucial reason is, that the entities's are of drastically varying length, so longer entities would naturally dominate shorter ones when only comparing the raw counts - considering relative frequencies instead of absolute ones alleviates such problems.

Because of these reasons, in the algorithm it will often be talked about \glspl{quant}. The algorithms explained below transforms the raw frequency-counts of a document and an \gls{ngram} into some \emph{score}, dependent on the number of occurences of this term in this document as well as the counts of other \glspl{ngram} and other documents. This score is henceforth called a \gls{quant}.
%TODO: term? phrase? n-gram?


\label{sec:embeddings}

\includeMD{pandoc_generated_latex/2_6_requiredalgos}

\includeMD{pandoc_generated_latex/2_7_seperatrixdistance}
