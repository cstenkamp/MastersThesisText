\chapter{Discussion and Conclusion}
% (was sind die broaden takeaways von meinem Kram)
% * Nochmal nen theoretisches Embedding, Kontextualisieren f√ºr Bildungsressourcen
% * ...and conclusion

Now that we have the results, let's interpret them and subsequently take a step back to see if we answered our research questions and achieved our thesis goals in general: Is the methodology applicable to the domain of educational resources?

\section{Interpretation and Discussion of results}
%Research Question: Ich will die Methodik von dem Paper auf educational resources Applien. Das unbedingt in discussion & conclusion aufgreifen.

Let's go over and interpret the results in order.

1) Results for Placetypes
2) Results for educational resources, answering "is the algorithm able to cope with the siddata-dataset" and "does the general methodology work for the domain"
3) Hyperparameters
4) Architecture 

\subsection{Results for Placetypes}

Let us first discuss the results for placetypes. 

When doing classification with decision-trees it is a design-decision to either decision-tree per class that just classifies if a sample is that class or not, or instead just one tree that must predict the exact class membership for all classes.

In \autoref{tab:f1_geonames_foursquare_all}, we reported the results both for the case of 1-vs-rest and all-at-once. Let us discuss them:

Performances for depth-limited trees are not consistently worse than unbounded trees. Surprising on first glance, but decision trees are known to be prone to overfitting, which can only really happen for unbounded trees.

The best results are achieved for 1vsRest and balancing or not 1vsRest and no balancing. Generally, especially for depth-limited trees it holds that If AllAtOnce, balancing is bad for performanc, and If 1vsRest, balancing is good for performance. That makes sense because as stated above, the depth-limited ones can only predict very few classes, explaining why these are bad at AllAtOnce. For the same reason however they benefit from unbalanced datasets, in which case they just assign the two most common class labels - this happens without balanced sample-weighting: the trees simply assign the respectively most common Geonames/Foursquare class. Generally however, especially for depth-limited trees, 1vsRest improves performance - reason for this explained below.


\subsubsection*{Explanations for good results}


As shown in \autoref{tab:f1_mainalgos_me_short}, the achieved results outperform those of the literature for the placetypes-dataset, often with a significant margin. Considering that this implementation replicates \cite{Derrac2015} without major algorithmic improvements and does not contain some of the improvements of \cite{Ager2018, Alshaikh2020}, this is initially surprising, so here we will discuss some possible explanations for that.

\paragraph{Errors} 
Naturally, the first thing to check in this situation is to check for errors in the implementation. Following that route it is however important to keep in mind that errors in the actual algorithm are an unlikely candidate for an erroneously high performance. As elaborated before, the performance of the decision-tree is only a surrogate metric to evaluate the resulting semantic directions. Among others this implies that the classification target for the task is disregarded in all algorithm steps except the final evaluation with the decision trees. As long as that is given,  the only realistic source of error that leads to higher-than-expected accuracies reliably is thus in this step. This does not mean that there are certainly no errors in the implementation of the rest, but as long as the classification target is not used, all these errors would only coincidentally lead to better classification results. In contrast to that, there are many sources of errors in the decision tree classification that will likely lead to unrealistically high performances such as mixing up the training- and testing set. In any case, both the algorithm itself and the decision-tree classification was triple-checked for errors and many sanity-checks were performed that all lead to the same conclusion, so from now on we will assume that the results are correct and discuss possible reasons for that\footnote{The code is open-source and available at \url{github.com/cstenkamp/derive_conceptualspaces}, and \me is thankful for any issues.}


\paragraph{One-Vs-Rest-Classification}

\cite{Ager2018, Alshaikh2020} are both unclear if they did the former or the latter. Generally, All-At-Once would be the harder task and comparing accuracies of 1vsRest to AllAtOnce an unfair comparison. However, there are a few things that can be assumed:

Both of them report to have used the sklearn-implementation of decision-trees, just like this work. This specific implementation reportedly uses the CART algorithm \cite{breiman1984classification}, which only allows binary trees, where every node has exactly two children\footnote{\url{https://scikit-learn.org/stable/modules/tree.html\#tree-algorithms-id3-c4-5-c5-0-and-cart}}. Consequently, a decision tree of depth one can only classify $2^1 = 2$ classes, whereas a tree of depth two can classify up to $2^2=4$ classes. 
%TODO: actually for depth 2 it's 2^2 + 2^1 + 2^0 = 7 and thus for depth 1 = 3 
Due to that, the best achievable accuracy of a perfect depth-1-tree is $\frac{\text{||samples in two most common classes||}}{\text{||samples in all classes||}}$, which is $\frac{176+74}{403} = 0.62$ in the case of Geonames and $\frac{88+82}{391} = 0.43$ for Foursquare \todoparagraph{(on average! still random assignment to train and test sets!)} the latter is lower than what \cite{Alshaikh2020} report. This would be even a lot more pronounced when classifying the movie genre, which has 100 classes. 

Also semantically it is reasonable to assume to do 1vsRest: they state extensively that they are looking for a direction for \textit{scarieness} in movies, where the genre corresponding to that \textit{(Horror)} is only one of the genres. This kind of mapping Genre-FeatureDirectionPredictingGenre can only be found with separate trees per genre - and thus generally per class. Both of these reasons lead us to the assumption that they likely also did a separate tree for each of the features.

\paragraph{Dataset size}
Only a small subset of samples even \textit{have} a class assignment (403 of 1383 in 7 classes in the case of Geonames (see \autoref{fig:scatter_mds_placetypes}), 391 in 9 classes for Foursquare), and the classes are heavily imbalanced (Geonames between 176 and 14 samples per class, Foursquare between 88 and 6). I used the classes as uploaded to \url{https://www.cs.cf.ac.uk/semanticspaces/} by \cite{Derrac2015}, of course there is the chance that they did not make all of their data publicly available, whereas \cite{Ager2018, Alshaikh2020} had access. Furthermore, that is generally a tiny dataset so the statistical power of any result here is really low - maybe it is just coincidendence

\paragraph{Not having some improvements of \cite{Ager2018}}
The contributions from \cite{Ager2018} were primarily the Fine-Tuning and the condition where averaged word-embeddings are used instead of \gls{mds} (denoted \textbf{AWV}). As can be seen in \autoref{tab:f1_placetypes_long}, these contributions don't seem to affect the classification performance much, being in the same region as those for their MDS-condition which is implemented here as well: There are no really significant improvements that are not given here, giving no reasong to assume that their performance should be superior.  

\paragraph{Not having some improvements of \cite{Alshaikh2020}}

The \textbf{Ortho} condition of \cite{Alshaikh2020} actually does significantly outperform the base algorithm for many configurations. For Foursquare, their performance comes very close to mine, whereas their Geonames-performances are a lot worse than mine.

With the utmost respect, if the code uploaded by \cite{Alshaikh2020} is really the basis for their implementation, I have reasons to doubt what they say they do and what they do really matches. A quick inspection of their uploaded source code\footnote{\url{https://github.com/rana-alshaikh/Hierarchical_Linear_Disentanglement/blob/master/Hierarchical_Linear_D4.py\#L485-L486}} revelaed for example they take the kappa-score from on the raw predictions, not on the rank like \cite{Derrac2015} described and like we do (see \autoref{tab:kappa_measures}) and they also don't weight their kappas. Apart from observations like this, it is hard to say more about their code, because the two files uploaded by them are not the whole algorithm and they load quite a few files that are not in the repository, and also none of the evaluation on basis of decision tree performance is in the repository.

\paragraph{Using the best configuration}

One difference is as far as I can tell, that we looked for the best configuration \textit{for this particular task}, which is a different one for each combination of dataset $\times$ \gls{dt}-depth $\times$ classification-target. I interpret \mainalgos such that they did hyperparameter-tuning before, using another possibly subjective metric \todoparagraph{test this claim}, and then decided on one (or rather four, see \autoref{tab:f1_placetypes_long}) configuration that was not optimized for the dataset $\times$ \gls{dt}-depth $\times$ classification-target. It should be noted that in this work, the algorithm is also not optimized for that task, but only the best of the 80 different parameter-combinations that were executed is used respectively. At the same time however, some \todoparagraph{which} way to find a hyperparameter-configuration has to be used, and it is unlikely that \mainalgos chose the worst configuration. \autoref{tab:f1_geonames_foursquare_all} displays robust results of a parameter-configuration that proved good on average. As these results, however, also outperform those of \mainalgos significantly, choosing the best configuration seems to play only a minor role for the performance.

\paragraph{Weighted Average of the individual classifiers}

The considered scores for the 1vsRest-condition of this work are calculated from the scores of the individual per-class classifiers both with uniform weighting per class (bottom two rows of \autoref{tab:f1_geonames_foursquare_all}) and also with class weights inversely proportional to class size (middle two rows). Clearly, the condition that weights the indiviudal scores leads to better results. Weighting the score is a reasonable assumption, given that the individual class frequencies are very imbalanced (see \autoref{fig:scatter_mds_placetypes}). Unfortunately, \mainalgos do not explicitly share if they calculated weighted class scores as well. If we assume for now that \mainalgos did report unweighted scores and thus disregard the condition where class-scores are weighted (see bold scores in \autoref{tab:f1_geonames_foursquare_all} or last column of \autoref{tab:f1_placetypes_long})), our results are still comparable with those of \mainalgos and especially in the case of GeoNames-labels a lot closer to those reported in the literature. So while I hereby argue that weighting the scores makes sense, even if that is not the case our results are still acceptable. Again I want to stress that there are only really few and imbalanced labels for this dataset in general, making the statistical power of these results very small.

\paragraph{Improvements that we do have}

We do not have many differences in hyperparameters or algorithm-components than \mainalgos do, but there some. For example using tf-idf as \gls{quant} instead of PPMI: Close inspection of \autoref{tab:best_params} shows that often, the tf-idf results are superior to the PPMI-results, and sometimes the combination of using tf-idf as quantification and tf-idf as dtm-quantification is good. This may lead to two conclusions: Either tf-idf is just better than PPMI under certain conditions, or just that the fact that more different results generated here just increased the statistical chance that good results were among the generated ones.

\subsubsection*{Conclusion} 

Even though this work did not do much beyond \mainalgos, maybe there were some small things that were done here that gave an edge, such as trying out different or just more hyperparameter-combinations. Maybe the scores here were calculated differently than in \mainalgos, but even if that were the case the results generated here are still comparable. Maybe I had errors, maybe \mainalgos did, but in any case as the dataset is so small exact results do not seem incredibly informative anyway. Most importantly the comparison was performed to check if this implementation is working correctly, and the evidence for that appears very strong.

\subsection{Results for educational resources}

One of our two research questions was to figure out if the methodology works for our domain. So now that we have established that the implementation seems to work and the dataset works on other datasets, let us finally look how the algorithm copes with the Siddata-dataset and answer this question.

\subsubsection{Fear of dataset differences}
\label{sec:discuss_datasetdiffs}

When describing the datasets in \autoref{sec:datasets}, we noticed that they are quite different. An important difference is that in contrast to the originally used datasets, in our dataset the relationship between how relevant a concept is for an entity and how often its words occur in the respective \gls{bow} is not given. Furthermore, the Siddata-dataset contains far less words per entity: the median number of unique words per description is three orders of magnitude smaller compared to the placetypes-dataset (see \autoref{tab:summed_unique_words}). Even though the number of entities in the dataset is higher, it contains substantially less unique words, especially in relation to the dataset-size (see \autoref{tab:summed_unique_words}). The difference is very prominent in relation to the dataset size (indicated by the last two columns). This leads to the fact that for most of the \glspl{ngram} that can serve as candidate-terms, the number class with entities that contain the phrase is far smaller than the negative class. 
% As established, a very important difference is that more relevant words do not occur more often. We assumed  because of that, the kappas that compare rankings are not so good

\textcite{Derrac2015} extracted roughly 20\,000 candidates for the movies- and placetypes-datasets. If the same number of candidates were to be extracted in our case, more than half of them would occur in less than 25 descriptions, such that the positive class for the corresponding classification problem contains only $\frac{24}{26346}=0.09\%$ of the samples. It seems unlikely that even class weighting can make up for that, yielding a bad classification. 

\todoparagraph{Well actually, see} \autoref{tab:corpussizes} \todoparagraph{used only more-than-80 words, leading to only 11601 entities in our dataset}

\todoparagraph{Come back to} \autoref{tab:generated_stats} and the Sum-columns indicating robustness or not!

In \autoref{sec:results_datasetdiffs}, and parts of the distribution of interim results seem to support this hypothesis: In a sample run, 10\,060 candidates were extracted (\autoref{tab:generated_stats}) As displayed in \autoref{fig:candidate_histogram}, the number of candidates is exponentially decreasing. 
% less candidates produced

Despite this, a sample run with 200 dimensions still had 5016 phrases with $\kappa \geq 0.1$ and 1008 with $kappa \geq 0.5$, which is enough for the algorithm (for 200D only 400 ones with $kappa \geq 0.5$ would be necessary). However there are far less ones in $\kappa_{0.1}$, meaning the clusters are considerably smaller. On the other hand, consider that for the placetypes-dataset in the 200D-case 21819 out of 21833 candidates had such a kappa score, which actually does not seem helpful at all.
% The #Texts containing a candidate are exponentially decreasing, which means that for many of the candidates that ARE produced the classification to measure the faithfulness has a really hard time

When discussing the dataset, we already theorized that only keeping those entites for which a classifier can successfully predict its faculty may help to increase dataset quality. That turned out to be not necessary but would still be a good future research opportunity.

If we had bad results we could have only used the 1500 with the longest descriptions, bringing us closer to the distribution of the placetypes-dataset, but still not changing the properties. In sum, despite the dataset differences the algorithm does seem to extract enough \todoparagraph{and is robust bla bla}. But even though the dataset seems to be quite different and some intermediate results also show significant differences which makes downstream tasks harder, the final performance is suprisingly robust, indicating that the methodology is robust and does not only work for datasets with the aforementioned properties. \todoparagraph{If it hadnt we had some Ideas like using wordnet for hyponyms etc but that does not seem necessary.}

In terms of the obviousness of the faculty from the samples, the dataset actually seems to look better than placetypes. The scatterplot (\autoref{fig:scatter_mds_siddata}) clusters good enough to be optimistic, and both BERT and even a random 3D-embedding can reach okayish classifications.

As established, a very important difference is that more relevant words do not occur more often. we assumed before that because of that, the kappas that compare rankings are not so good. WELL IS THAT THE CASE

\subsubsection{Classification results}

\cite{Ager2018}: \todoparagraph{"Interestingly, depth-1 trees achieved the best overall performance, with depth-3 trees and es- pecially unbounded trees overfitting"}

Keep in mind that we're only looking at the Faculty! We know that this by far does not test everything the algorithm does or how it performs for other things-to-compare, so the best analysis would just be human studies.

We compared to BERT (85.19\% accuracy), and to 3D-embedding (64.3\% weighted accuracy), and we robustly achieved 81.4\% accuracy with depth one trees! Really surprised me. The one that uses only three dimensions is already better than BERT, and the unbounded one has 94.3\% classification accuracy. Interestingly, classifying all faculties at once is a lot worse, so maybe a fairer comparison to BERT would have been if there we also would have followed the 1vsRest way, but that is future work.
%Really good results for faculty in general

When lookign at the individual faculties, we see that \textit{Humanwissenschaften} is for depth-1-trees by far the worse, but also has a big standard deviation. \autoref{fig:faculty_boxplots} shows this even better. This indicates that the classification performance for this faculty strongly depends on what ends up in the train set, so maybe the Faculty is just too different in their descriptions. This is a bit surprising, I would have rather expected Erziehungs-/Kulturwissenschaften is more diverse with Lehramt in it.  Interestingly, preliminary analysis of Johannes and SidBERT (unpublished) showed that Mathe/Informatik (top-1 25.7\%, top-5 42.1\%) and Sozialwissenschaften (top-1 32.7\%, top-5 65.5\%) are most problematic (in the sense that their neirest neighbor embedding is not from the same faculty), followed then by Humanwissenschaften. Also the class frequency for that class is not the culprit, its one of the larger ones.

\paragraph{Qualitative analysis}

When looking at the examples per faculty, we observe mixed results. ON the one hand, erziehungswissenschaft is perfect, on the other hand physik and bio/chemie suck. Acutally, physik and bio/chemie always suck, for most of the runs we did. When looking at the class frequencies for these we see that well yes, these two are among the smaller classes, but so are Mathemaik/Informatik and Wirtschaftswissenschaften, both of which are generally classified really well.

Interestingly even though for Humanwissenschaften the performance is generally not too good, the dimensions we extracted from it are plausible.

Would you want to take the words from the top 3 direcitons as names from the dimension? Some of them definitely: erziehungswissenschaft, programmiersprache, "deutsch literaturwissenschaft", "psychologie", "arbeitsmarkt"... but some of them meh.


\paragraph{Post-Hoc-Qualitative Analysis}
\label{sec:duplicate_maps}

\todoparagraph{Darauf referenzier ich mich in der datasets-section, das muss rein welp}

We talked a lot about the dataset differences in terms of \textbf{the property}, but lets now also look if we can say something about the dataset \textbf{Quality}.

I have the feeling a lot of bad performance can be explained by the dataset suckities, such as courses of different years being modelled as completely different things in studip

noticed things are:

Das wie gerade auch in der duplicate-per-combi-of-ndims-and-ncats sichtbar wird dass letztlich halt "kurs 123" und "!!F√ÑLLT AUS!! kurs 123" auf den selben fallen, was zwar quantiativ schei√üe ist aber ACTUALLY GOOD! Also dass 2 EIGENTLICH gleiche kurse auf das selbe mappen, that's a feature not a bug

wenn man sich anschaut WAS denn falsch klassifiziert wurde sieht man halt dass die Kurse die offensichtlich nicht Mathe sind eben falsch klassifiziert wurden, oder dann eben "logic" in mathe landet, well it's not wrong, it's the dataset 

There is really a lot of crap in the data, like the  "Tutoren sind: Susi Sorglos Willi Wacker" ones. If the performance is not enough, one may also check if taking only the 1000 with the longest decription would help

or only those ones where BERT can sucessfully classify the faculty. Again, faculty is not everything, BUT if the faculty CAN be extracted, cases that only list names or places are out (...except FROM the name of place follows the faculty but lets ignore that)


\todoparagraph{Are short descriptions worse?}

\paragraph{concluding remarks}

In general we must say that we have really good accuracies and this really seems to indicate that it works.  We could to the shallow-decisiontrees-thingy for other attributes of the dataset than I currently have (see future work)

\subsection{Hyperparameters results}

I considere the results of 165 different param-combis in the final run, but so many previous runs were there as well were I looked at previous stuff (see the image that I originally had in workflow)

\includeMD{pandoc_generated_latex/5_hyperparamresults}

\subsection{Robustness}

Robust meaning here that multiple runs yield similar results. We said before that in word2vec the actual directions of the vectors are completely incomprehensibly dependent on the random initial conditions, and we also said that that is an absolute no-go in conceptual spaces.

Already here go into detail how unrobust the implementation of derrac and the others is. 

Show and interpret results for how robust my implementation is and if close words are close

..then again elaborate that explicit word choices are not too relevant and also not tested here and that I already experimented with different techniques (see appendix for keybert vs the others) but that there is no formal way to test them.

I hypothesize (see \autoref{sec:extract_cands}) that the reason I am having more robust results than \cite{Derrac2015} is among others because I have less candidates.

Results of multiple runs: 

from all the 165 different param-kombis I did, the following had enough (ndims*2) kappas-over-0.5:
\{3: 40, 50: 38, 200: 15\}





\subsection{Did we achieve the thesis goal?}

\includeMD{pandoc_generated_latex/5_2_siddata}

\section{General Algorithm}

Now that we seen the algorithm in action and painstaking implemented it we are experts on the algorithm of \textcite{Derrac2015}, so let us critically reflect on the algorithm as such.

\subsection{General algorithm idea}

When first confronted with the algorithm of \textcite{Derrac2015}, it appears oddly specific. However after having read the G√§rdenfors' book \textcite{Gardenfors2000a}, the idea for their algorithm was acutally pretty pretty self-evident: In his book \cite{Gardenfors2000}, G√§rdenfors suggests that conceptual spaces can be generated from high-dimensional sensory input by using \gls{mds} to project the original data into a Euclidian space to do geometric reasoning in that space. The book has an entire chapter on computational aspects, in which the author discusses vector space models, dimensionality reduction techniques an \gls{ann} architectures for different levels of human conceptualization. According to that, MDS is especially good at dealing with pairwise distances judgements from a subject's perception, to create a more \textit{economic} representation for \textit{phenonemal} \glspl{cs} \cite[221]{Gardenfors2000a}. Considering that \textcite{Derrac2015} did not create the space from sensory data but from text corpora, their algorithm combines this with the steps for a classical \gls{nlp} pipeline as described by \cite{Turney2010} (\autoref{sec:vsm_construction}) where the distance of two texts is measured by the words they share. With that, the core contribution of \cite{Derrac2015} mainly lies in the idea that the \textit{faithfulness} of a potential direction for the resulting semantic space can be assessed by the performance of the corresponding decision problem.

\paragraph{Measuring the Faithfulness of directions}

It seems reasonable that this assumption holds for the datasets originally considered by \textcite{Derrac2015}. As extensively discussed, when concatenating reviews of movies or tags describing pictures of places it is natural that words that describe a salient feature of the respective entity occur more often. The Siddata-dataset consists of short description without this property, where most words have a relative document-frequncy of less than a percent. However, the technique of comparing the ranking induced by the classification with the score of the words still yielded enough results, and the directions seem to consist of interpretable property. The robustness of the algorithm in that regard is surprising. 

% Under the assumption of the \gls{distribhyp}, the assumption that this is possible seems reasonable. \todoparagraph{Naher drauf eingehen warum? Ich kann nicht einfach distributional hypothesis sagen und fertig}

\subsubsection{Requiring MDS}
\label{sec:discuss_mds}

\textcite{Derrac2015} explicitly state that MDS is best dimensionality reduction technique for their algorithm, as it is one of the only ones that results in a metric space. In their paper, they describe SVD (the mathematical algorithm behind \gls{lsa}) as popular technique to for dimensionality reduction, but further state that \q{SVD produces a representation in which entities correspond to vectors, which should be compared in terms of cosine similarity rather than Euclidean distance. [...] However, we can expect that spatial relations such as betweenness and parallelism [...] are not meaningful in the representations derived from SVD} \cite[14]{Derrac2015}. These relationships are required for semantic classifiers which mimic as analogical reasoning and betweeness-based reasoning, which they demonstrate to work for all of their domains. However, this spaces does not have semantic directions. The \textit{final} feature-based representation of the entities is reached by ranking each entity for each of the feature directions and creating new vectors from these ranks. As they state themselves \cite[22]{Derrac2015}, the feature vectors are not orthogonal, not linearly independent and only of ordinal scale level, withhout meaningful distances. \textcite{Derrac2015} create semantic classifiers both for the \textit{intermediate} space with meaningful distances (geometric betweeness- or parallelism-based classifiers) and also for feature-based representation (a-fortiori-classifiers, see \autoref{sec:reasoning}). However, \textcite{Ager2018,Alshaikh2020} are only interested in the latter space and its resulting feature axes \todoparagraph{..and so are we. die anderen sind meh}. If that is the case however, it becomes irrelevant if the intermediate space is metric or not, which enables for other algorithms to be used in that step. As stated in \autoref{sec:dim_red}, LSA may be the better choice as it explicitly detects latent topics in descriptions instead of relying on words that are explicitly mentioned. \todoparagraph{However when using it you are constantly dealing with vecotrs. However that is good.} 

\cite{Ager2018} and \cite{Alshaikh2020} both don't care for this interim space and only do stuff on the re-embedded one, however keep MDS! As far as I understand, they have no reason to do so. It seems like they read to use MDS in G√§rdenfors' book and then forgot that their final re-embedding step makes that irreelevant. 

So if for the explainable classifiers the relevant space is the one with semantic directions, geometric properties of the intermediate space are irrelevant. Instead one should try to generate a space that has both a useful metric \textit{and} interpretable directions! Depending on what the authors consider the end result of their algortihm, only one of these necessary requirements for conceptual spaces is fulfilled. So if their end-result is the intermediate space, that's nothing more than a normal \gls{vsm} with some interesting but long-term useless geometric properties. How about instead calculating a new orthonormal basis on the coordinate system? Enforcing orthogonality of the semantic directions as good as possible, and then using linear algebra for a change of basis for the entites, such that not only their ranks but their exact position for each semantic direction (axis) is relevant.  Maybe something like Principal Component analysis to decorrelate the directions. Yes that leads to vectors without a name, that's where LSA helps!



\paragraph{Using classical techniques}

The algorithm relied on a classical linguistic pipeline where a frequency matrix is generated from the \gls{bow}-representation of each entity. As discussed earlier, this comes with problems such as destroying the information about word order and conceptual similarity of words. Interestingly, for both follow-ups \cite{Ager2018,Alshaikh2020} using modern neural embeddings instead did not yield substantial improvements (see \autoref{tab:f1_placetypes_long}) (even though however \cite{Alshaikh2020} state in their paper that relying on modern embeddings such as \gls{bert} \cite{Devlin2019} may lead to better results). However, what the authors did was to just replace the \gls{vsm} generated classically in the first steps of the algorithm with a neural embedding, such as averaged GloVe embeddings \cite{pennington2014glove} of the words in the text. On that they ran the exact original algorithm of creating a frequency matrix from the \gls{bow} and using a linear classifier to get the direction. \todoparagraph{Wenn man schon embeddings nutzt dann soll man auch die eigenschaft dass die sinnvolle richtungen haben nutzen. Und wenn man dann shcon mit vektoren arbeitet dann kann man auch lsa undso nutzen. Und dann hat man auch nicht mehr das problem mit too-small-positive-class}

\paragraph{Other things}
\todo


\paragraph{Robustness}
\todo


\subsection{Is that a Conceptual Space?}

In \autoref{sec:cs}, we looked at what a conceptual space is, before introducing \gencite{Derrac2015} algorithm to automatically induce these. However, their algorithm only approximates \glspl{cs} does model some parts of the definition. A first issue with the algortihm is, that it needs a clearly defined domain from the start. It takes a corpus of texts and embeds each of these into a single high-dimensional vector space. If not all texts in the corpus are from a single domain, due to the similarity-based vector space generation, outliers will greatly affect the embedding of all entities (as \cite{Ager2018} discusses at length). This sounds irrelevant in practice, but the problem is that it is impossible to clearly define what a domain is. There is the set of place types, but this domain consists of various subdomains, and some concepts apply only to a specific subset of entities. The described algorithm has no way to figure out such subdomains. This issue is partially addressed by the works of \textcite{Alshaikh2019, Alshaikh2020, Alshaikh2021} which eloaborate on the idea of subdomains. As they state, \textit{\q{When representing a particular entity in a conceptual space, we need to specify which domains it belongs to, and for each of these domains we need to provide a corresponding vector.}} \cite{Alshaikh2020} \todoparagraph{Wenn nicht in Algorithm geschrieben, dann hier die idee mit dem dass political nur relevant ist for a subset blabla} 

Also we should not make the mistake of confusing what \mainalgos and this work consider a domain (the set of movies, places or courses), and the definition of domain as used by \textcite{Gardenfors2000a}. According to the latter, a domain is a low-dimensional set of correlated properties, such as \textit{the color domain} consisting of \textit{hue, saturation} and \textit{value}. This also points out another difference of the original \gls{cs} definition and the definition used here: Conceptual spaces describe an entity through several uncorrelated low-dimensional vector spaces, not a single one with several dozen dimensions. As \cite{Ager2018} puts it more humbly, \textit{\q{The idea of learning semantic spaces with accurate feature directions can be seen as a first step towards methods for learning conceptual space representations from data [...]}}. Again it is referred to thw works of \textcite{Alshaikh2019, Alshaikh2020, Alshaikh2021} which alleviate this by iteratively finding disentangled low-dimensional feature spaces.

As discussed earlier, the algorithm of \textcite{Derrac2015} first embeds the entities into a euclidian space where the concepts of betweeness and parallelism make sense, and subsequently create a feature-based representation that bases on an entity's rank \wrt several human-interpretable features. In other words, the algorithm produces \textit{either} a space with a euclidean metric, \textit{or} one with interpretable directions, but no space that has \textit{both properties}. As both are important necessary conditions for a conceptual their algorithm at no point generates something that resembles a \gls{cs}. It is unclear to us why they only consider the ranking of the entites regarding the feature axis instead of their distance which may retain relations of distances, for example by applying an arithmatic change of basis for the coordinate system (linear transformation). The way their algorithm works, the final space only has ordinal scale level and linearly dependent (correlated) dimensions. An interesting research avenue is to figure out what properites the space they have, and if small changes to the final algorithm step (such as PCA to decorrelate dimensions or not only taking the rank) could help in retaining the euclidean or at least another usable metric.


\subsubsection*{Points instead of Regions}

Another important different from the resulting space to \glspl{cs} is that we are dealing with points instead of regions. The authors acknowledge that themselves





\subsection{Regarding their research practices}

\paragraph{Robustness}
\todo

\paragraph{Ambiguity}
\todo




% \includeMD{pandoc_generated_latex/5_3_evalderrac}




\section{Architecture}

\includeMD{pandoc_generated_latex/5_4_architecture}

\section{Future Work}
\label{sec:futurework}

\subsection*{Algorithm Addendums}
\todo

\paragraph{Stuff to better deal with the dataset}
\todo

\subsection*{Work ontop}
Everything From ager and Alshaikh
Use \cite{Erk2009}'s technique to make regions again
It is unclear to us why they only consider the ranking of the entites regarding the feature axis instead of their distance, why not try to do some linear algebra
\todo



\subsection*{Completely Different algo}
\todo



% \includeMD{pandoc_generated_latex/5_0_futurework}


\section{Conclusion}

\includeMD{pandoc_generated_latex/5_6_conclusion}