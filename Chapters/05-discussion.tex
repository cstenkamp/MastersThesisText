\chapter{Discussion and Conclusion}
% (was sind die broaden takeaways von meinem Kram)
% * Nochmal nen theoretisches Embedding, Kontextualisieren f√ºr Bildungsressourcen
% * ...and conclusion

The results generated thus far should finally answer our research questions and check if the general thesis goals were achieved. In this section, we will first interpret the generated results iteratively to be able to assess the performance of the algorithm on the dataset. On the basis of that, an answer to the general question if the methodology is applicable for the domain will be formulated. This is followed by a discussion of the algorithm in itself from the perspective of somebody that worked with it extensively. Finally the architechture is quickly discussed and the thesis concluded.

\section{Interpretation and Discussion of results}
%Research Question: Ich will die Methodik von dem Paper auf educational resources Applien. Das unbedingt in discussion & conclusion aufgreifen.

First we evaluate our implementation by comparing its performance for the placetypes-dataset and discussing possible reasons for any discrepancies. Following that, we will discuss the results for the Siddata-dataset when compared to those of the literature to see if the algorithm can cope with the domain transfer. On the basis of which we will try to find an answer to the question if the general methodology is able to cope with the domain in question. Afterwards we will discuss the results of the hyperparameter-search and its implications on our dataset.

\subsection{Results for Placetypes}

When doing classification with decision-trees it is a design-decision to either train one decision-tree per class that just classifies if a sample is that of that class or not (\textit{1vsRest}), or alternatively generate a single tree that must predict the exact class membership for all classes at once (\textit{AllAtOnce}). In \autoref{tab:f1_geonames_foursquare_all}, we reported the results for both of these conditions. 

The results indicate that performances for depth-limited trees are not consistently worse than unbounded trees. This is not suprising, considering that decision trees are known to be prone to overfitting, which can only really happen for unbounded trees. Similar results were reported in the work of \textcite{Ager2018}.

The best results were achieved for the condiditions \textit{balanced 1vsRest} and \textit{unbalanced AllAtOnce}. In general the results indicate that especially for depth-limited trees it holds that for single \textit{AllAtOnce} classifiers, balancing is bad for performance an vice versa. Considering that depth-limited ones can only predict very few classes explains why the \textit{AllAtOnce} condition performs badly overall. For the same reason, however, these classifier benefit from unbalanced datasets - without balanced sample-weighting, these trees can just detect the most common class labels and assign them, which was shown to be the case here as well. Generally however, especially for depth-limited trees, \textit{1vsRest} improves performance.


\subsubsection*{Explanations for good results}


As shown in \autoref{tab:f1_mainalgos_me_short}, the achieved results outperform those of the literature for the placetypes-dataset in all cases, often with a significant margin. Considering that this implementation replicates \cite{Derrac2015} without major algorithmic improvements and does not contain some of the improvements of \cite{Ager2018, Alshaikh2020}, this is initially surprising, so here we will discuss some possible explanations for that.

\paragraph{Errors} 
Naturally, the first thing to to in this situation is to check for errors in the implementation. In following that route, however, it is important to keep in mind that errors in the actual algorithm are an unlikely candidate for an erroneously high performance. As elaborated before, the performance of the decision-tree is only a surrogate metric to evaluate the resulting semantic directions. Among others this implies that the classification target for the task is disregarded in all algorithm steps except the final evaluation with the decision trees. As long as that is given,  the only realistic source of error that leads to higher-than-expected accuracies reliably is thus in this step. This does not mean that there are certainly no errors in the implementation of the rest, but as long as the classification target is not used, all these errors would only coincidentally lead to better classification results. In contrast to that, there are many sources of errors in the decision tree classification that will likely lead to unrealistically high performances such as mixing up the training- and testing set. In any case, both the algorithm itself and the decision-tree classification was triple-checked for errors and many sanity-checks were performed that all lead to the same conclusion, so from now on we will assume that the results are correct and discuss possible reasons for that\footnote{The code is open-source and available at \url{github.com/cstenkamp/derive_conceptualspaces}, and \me is thankful for any issues.}


\paragraph{One-Vs-Rest-Classification}

\cite{Ager2018, Alshaikh2020} are both unclear if they did the former or the latter. Generally, All-At-Once would be the harder task and comparing accuracies of 1vsRest to AllAtOnce an unfair comparison. However, there are a few things that can be assumed:

Both of them report to have used the sklearn-implementation of decision-trees, just like this work. This specific implementation reportedly uses the CART algorithm \cite{breiman1984classification}, which only allows binary trees, where every node has exactly two children\footnote{\url{https://scikit-learn.org/stable/modules/tree.html\#tree-algorithms-id3-c4-5-c5-0-and-cart}}. Consequently, a decision tree of depth one can only classify $2^1 = 2$ classes, whereas a tree of depth two can classify up to $2^2=4$ classes. 
%TODO: actually for depth 2 it's 2^2 + 2^1 + 2^0 = 7 and thus for depth 1 = 3 
Due to that, the best achievable accuracy of a perfect depth-1-tree is $\frac{\text{||samples in two most common classes||}}{\text{||samples in all classes||}}$, which is $\frac{176+74}{403} = 0.62$ in the case of Geonames and $\frac{88+82}{391} = 0.43$ for Foursquare.\footnote{Note that these values only hold on average, as the samples are arbitrarily assigned to the train- and test-set.} The latter value is lower than what \cite{Alshaikh2020} report, indicating that is not how the authors generated results. This would be even a lot more pronounced when classifying the movie genre, which has 100 classes. 

Also semantically it is reasonable to assume to do 1vsRest: they state extensively that they are looking for a direction for \textit{scarieness} in movies, where the genre corresponding to that \textit{(Horror)} is only one of the genres. This kind of mapping Genre-FeatureDirectionPredictingGenre can only be found with separate trees per genre - and thus generally per class. Both of these reasons lead us to the assumption that they likely also did a separate tree for each of the features.

\paragraph{Dataset size}
Only a small subset of samples even \textit{have} a class assignment (403 of 1383 in 7 classes in the case of Geonames (see \autoref{fig:scatter_mds_placetypes}), 391 in 9 classes for Foursquare), and the classes are heavily imbalanced (Geonames between 176 and 14 samples per class, Foursquare between 88 and 6). I used the classes as uploaded to \url{https://www.cs.cf.ac.uk/semanticspaces/} by \cite{Derrac2015}, of course there is the chance that they did not make all of their data publicly available, whereas \cite{Ager2018, Alshaikh2020} had access. Furthermore, that is generally a tiny dataset so the statistical power of any result here is really low - maybe it is just coincidendence

\paragraph{Not having some improvements of \cite{Ager2018}}
The contributions from \cite{Ager2018} were primarily the Fine-Tuning and the condition where averaged word-embeddings are used instead of \gls{mds} (denoted \textbf{AWV}). As can be seen in \autoref{tab:f1_placetypes_long}, these contributions don't seem to affect the classification performance much, being in the same region as those for their MDS-condition which is implemented here as well: There are no really significant improvements that are not given here, giving no reasong to assume that their performance should be superior.  

\paragraph{Not having some improvements of \cite{Alshaikh2020}}

The \textbf{Ortho} condition of \cite{Alshaikh2020} actually does significantly outperform the base algorithm for many configurations. For Foursquare, their performance comes very close to mine, whereas their Geonames-performances are a lot worse than mine. Apart from that, if the code uploaded by \cite{Alshaikh2020} is really the basis for their implementation, ther are strong reasons to doubt what they claim to do and what they actually do really matches. A quick inspection of their uploaded source code\footnote{\url{https://github.com/rana-alshaikh/Hierarchical_Linear_Disentanglement/blob/master/Hierarchical_Linear_D4.py\#L485-L486}} revelaed for example they take the kappa-score from on the raw predictions, not on the rank like \cite{Derrac2015} described and like we do (see \autoref{tab:kappa_measures}) and also do not apper to weight the kappa-scores. Apart from observations like this, it is hard to say more about their code, because the two files uploaded by them are not the whole algorithm and also depend on loading many files that are not in the repository, and also none of the evaluation on basis of decision tree performance is in the repository.

\paragraph{Using the best configuration}

Another difference appears to be that we looked for the best configuration \textit{for this particular task}, which is a different one for each combination of dataset $\times$ \gls{dt}-depth $\times$ classification-target. It appears from their description that \mainalgos did hyperparameter-tuning before, using another possibly subjective metric, and then decided on one (or rather four, see \autoref{tab:f1_placetypes_long}) configuration that was not optimized for the dataset $\times$ \gls{dt}-depth $\times$ classification-target. It should be noted that in this work, the algorithm is also not optimized for that task, but only the best of the 80 different parameter-combinations that were executed is used respectively. At the same time however, some way to find a hyperparameter-configuration has to be used, and it is unlikely that \mainalgos chose the worst configuration. \autoref{tab:f1_geonames_foursquare_all} displays robust results of a parameter-configuration that proved good on average. As these results, however, also outperform those of \mainalgos significantly, choosing the best configuration seems to play only a minor role for the performance.

\paragraph{Weighted Average of the individual classifiers}

The considered scores for the 1vsRest-condition of this work are calculated from the scores of the individual per-class classifiers both with uniform weighting per class (bottom two rows of \autoref{tab:f1_geonames_foursquare_all}) and also with class weights inversely proportional to class size (middle two rows). Clearly, the condition that weights the indiviudal scores leads to better results. Weighting the score is a reasonable assumption, given that the individual class frequencies are very imbalanced (see \autoref{fig:scatter_mds_placetypes}). Unfortunately, \mainalgos do not explicitly share if they calculated weighted class scores as well. If we assume for now that \mainalgos did report unweighted scores and thus disregard the condition where class-scores are weighted (see bold scores in \autoref{tab:f1_geonames_foursquare_all} or last column of \autoref{tab:f1_placetypes_long})), our results are still comparable with those of \mainalgos and especially in the case of GeoNames-labels a lot closer to those reported in the literature. So while I hereby argue that weighting the scores makes sense, even if that is not the case our results are still acceptable. Again I want to stress that there are only really few and imbalanced labels for this dataset in general, making the statistical power of these results very small.

\paragraph{Improvements that we do have}

We do not have many differences in hyperparameters or algorithm-components than \mainalgos do, but there some. For example using tf-idf as \gls{quant} instead of PPMI: Close inspection of \autoref{tab:best_params} shows that often, the tf-idf results are superior to the PPMI-results, and sometimes the combination of using tf-idf as quantification and tf-idf as dtm-quantification is good. This may lead to two conclusions: Either tf-idf is just better than PPMI under certain conditions, or just that the fact that more different results generated here just increased the statistical chance that good results were among the generated ones.

\subsubsection*{Conclusion} 

Even though this work did not do much beyond \mainalgos, maybe there were some small things that were done here that gave an edge, such as trying out different or just more hyperparameter-combinations. Maybe the scores here were calculated differently than in \mainalgos, but even if that were the case the results generated here are still comparable. Maybe I had errors, maybe \mainalgos did, but in any case as the dataset is so small exact results do not seem incredibly informative anyway. Most importantly the comparison was performed to check if this implementation is working correctly, and the evidence for that appears very strong.

\subsection{Results for educational resources}

One of our two research questions was to figure out if the methodology works for our domain. So now that we have established that the implementation seems to work on other datasets, we finally look how the algorithm copes with the Siddata-dataset.

\subsubsection{Quantifiable dataset differences}
\label{sec:discuss_datasetdiffs}

When describing the datasets in \autoref{sec:datasets}, we noticed that they are quite different. An important difference from our to the originally used datasets is, that in our dataset the relationship between how relevant a concept is for an entity and how often its words occur in the respective \gls{bow} is not given. Furthermore, the Siddata-dataset contains far less words per entity: the median number of unique words per description is three orders of magnitude smaller compared to the placetypes-dataset (see \autoref{tab:summed_unique_words}). Even though the number of entities in the dataset is higher, in sum it contains substantially less unique words. The difference is very prominent in relation to the dataset size (indicated by the last two columns). Because of this, for most of the \glspl{ngram} that serve as candidate-terms in the subsequent classification the positive class (\textit{entities that contain the phrase}) is far smaller than the negative class. 
% As established, a very important difference is that more relevant words do not occur more often. We assumed  because of that, the kappas that compare rankings are not so good

\textcite{Derrac2015} extracted roughly 20\,000 candidates for the movies- and placetypes-datasets. If the same number of candidates were to be extracted in our case, more than half of them would occur in less than 25 descriptions, such that the positive class for the corresponding classification problem contains only $\frac{24}{26346} \approx 0.09\%$ of the samples. It seems unlikely that even class weighting can make up for that, yielding a bad classification. Because of this, it is justified to consider less entities. To improve the ratio of classes, it was accordingly decided to only consider entities of at least 80 words (see \autoref{tab:corpussizes}), which yielded the final number of 11\,601 considered entities. 

\todoparagraph{Come back to} \autoref{tab:generated_stats} and the Sum-columns indicating robustness or not!

When discussing the dataset differences in \autoref{sec:results_datasetdiffs}, we assumed that the dataset differences will likely lead to less candidates being extracted. This is confirmed by the results: \autoref{fig:candidate_histogram} shows that the number of candidates that apply to each entity is exponentially decreasing. The score represents a low \textit{faithfulness} in the representational capacity for many of the produced candidates. This indicates that there is a much variability in the dataset that is not explained by any of the extracted words. A consequence of this is that mapping this space onto a limited number of extracted directions will lead to a loss of information which does not model the full latent information.

% The #Texts containing a candidate are exponentially decreasing, which means that for many of the candidates that ARE produced the classification to measure the faithfulness has a really hard time

Despite the small number of extracted candidates however, a sample run with 200 dimensions still yielded 5016 phrases with $\kappa \geq 0.1$ ($T^{0.1}$) and 1008 with $\kappa \geq 0.5$ ($T^{0.5}$), which is enough for the algorithm, considering that for a 200-dimensional embedding only 400 values with $\kappa \geq 0.5$ would be necessary. However, there are far less ones in $T^{0.1}$ compared to \textcite{Derrac2015}, meaning the resulting clusters are considerably smaller. 

This, however, is not necessarily a sign of bad performance of the algorithm: The number of cluster-elements that \cite{Derrac2015} for the 200-dimensional \gls{cs} for the place-types dataset (see \autoref{tab:generated_stats}) is 21\,819. Considering that they considered  number of candidate is 21\,833, the threshold does not meaningfully reduce the number of considered words. Accordingly, in this dataset all extracted words (which are all words with $\gls{df} \geq 50$) are considered in the final embedding. This leads to high amounts of noise, \ie a bad modelling of the actual latent topics.

Increasing the threshold a candidate to be considered a \textit{faithful} representation also does not help: Consider the \textbf{Sum} column of \autoref{tab:generated_stats}. It can be seen as a measure of \textit{Robustness} of the algorithm. If different parameter-combinations or just different initial random number generator results have a high impact on the generated results, the algorithm is not robust. In the case of our algorithm this shows in different extracted candidate terms. If all runs for all dimensionalities considered by \textcite{Derrac2015} would result in the same set of directions (best case), the sum would be 400, in the worst case 740. For the placetypes-dataset, 697 different semantic directions are found, indicating much noise in the dataset that obfuscates the latent information.

These observations lead us to the conclusion, that extracting less candidates may better capture the semantic content of the dataset. The disadvantage is that the resulting embedding captures less variance of the original dataset, however, those directions that are extracted show increased robustness. This is also indicated by the lower number of uniquely extracted candidates summed over all run-configurations in \autoref{tab:generated_stats}. 

When discussing the dataset, we already theorized that only keeping those entites for which a classifier can successfully predict its faculty may help to increase dataset quality. That turned out to be not necessary but would still be a good future research opportunity. Another possibility that could have been considered in the case of low performances is to use only the 1500 with the longest descriptions, bringing its distribution closer to the placetypes-dataset (but still not changing the properties). 

In sum, our previous hypothesis that the different dataset statistics leads to different conditions for the algorithm seems confirmed by the intermediate results. On the other hand, the final classification performances (\autoref{tab:robustresults_perfb}) are proof some important information of the dataset is captured regardless. In fact, not only are enough candiates extracted by the algorithm, but the results even indicate less sensibility for different hyperparameters for our dataset compared to the results of \cite{Derrac2015} for the placetypes-dataset. Regarding the algorithm, our results indicate that the methodology is robust and does not only work for datasets with the aforementioned properties.

\todoparagraph{TODO CONTINUE HERE YOU CAN DO IT CHRIS}

\subsubsection{Classification results}

In terms of the obviousness of the faculty from the samples, the dataset actually seems to look better than placetypes. The scatterplot (\autoref{fig:scatter_mds_siddata}) clusters good enough to be optimistic, and both BERT and even a random 3D-embedding can reach okayish classifications.

As established, a very important difference is that more relevant words do not occur more often. we assumed before that because of that, the kappas that compare rankings are not so good. WELL IS THAT THE CASE


\cite{Ager2018}: \todoparagraph{"Interestingly, depth-1 trees achieved the best overall performance, with depth-3 trees and es- pecially unbounded trees overfitting"}

Keep in mind that we're only looking at the Faculty! We know that this by far does not test everything the algorithm does or how it performs for other things-to-compare, so the best analysis would just be human studies.

We compared to BERT (85.19\% accuracy), and to 3D-embedding (64.3\% weighted accuracy), and we robustly achieved 81.4\% accuracy with depth one trees! Really surprised me. The one that uses only three dimensions is already better than BERT, and the unbounded one has 94.3\% classification accuracy. Interestingly, classifying all faculties at once is a lot worse, so maybe a fairer comparison to BERT would have been if there we also would have followed the 1vsRest way, but that is future work.
%Really good results for faculty in general

When lookign at the individual faculties, we see that \textit{Humanwissenschaften} is for depth-1-trees by far the worse, but also has a big standard deviation. \autoref{fig:faculty_boxplots} shows this even better. This indicates that the classification performance for this faculty strongly depends on what ends up in the train set, so maybe the Faculty is just too different in their descriptions. This is a bit surprising, I would have rather expected Erziehungs-/Kulturwissenschaften is more diverse with Lehramt in it.  Interestingly, preliminary analysis of Johannes and SidBERT (unpublished) showed that Mathe/Informatik (top-1 25.7\%, top-5 42.1\%) and Sozialwissenschaften (top-1 32.7\%, top-5 65.5\%) are most problematic (in the sense that their neirest neighbor embedding is not from the same faculty), followed then by Humanwissenschaften. Also the class frequency for that class is not the culprit, its one of the larger ones.

\paragraph{Qualitative analysis}

When looking at the examples per faculty, we observe mixed results. ON the one hand, erziehungswissenschaft is perfect, on the other hand physik and bio/chemie suck. Acutally, physik and bio/chemie always suck, for most of the runs we did. When looking at the class frequencies for these we see that well yes, these two are among the smaller classes, but so are Mathemaik/Informatik and Wirtschaftswissenschaften, both of which are generally classified really well.

Interestingly even though for Humanwissenschaften the performance is generally not too good, the dimensions we extracted from it are plausible.

Would you want to take the words from the top 3 direcitons as names from the dimension? Some of them definitely: erziehungswissenschaft, programmiersprache, "deutsch literaturwissenschaft", "psychologie", "arbeitsmarkt"... but some of them meh.


\paragraph{Post-Hoc-Qualitative Analysis}
\label{sec:duplicate_maps}

\todoparagraph{Darauf referenzier ich mich in der datasets-section, das muss rein welp}

We talked a lot about the dataset differences in terms of \textbf{the property}, but lets now also look if we can say something about the dataset \textbf{Quality}.

I have the feeling a lot of bad performance can be explained by the dataset suckities, such as courses of different years being modelled as completely different things in studip

noticed things are:

Das wie gerade auch in der duplicate-per-combi-of-ndims-and-ncats sichtbar wird dass letztlich halt "kurs 123" und "!!F√ÑLLT AUS!! kurs 123" auf den selben fallen, was zwar quantiativ schei√üe ist aber ACTUALLY GOOD! Also dass 2 EIGENTLICH gleiche kurse auf das selbe mappen, that's a feature not a bug

wenn man sich anschaut WAS denn falsch klassifiziert wurde sieht man halt dass die Kurse die offensichtlich nicht Mathe sind eben falsch klassifiziert wurden, oder dann eben "logic" in mathe landet, well it's not wrong, it's the dataset 

There is really a lot of crap in the data, like the  "Tutoren sind: Susi Sorglos Willi Wacker" ones. If the performance is not enough, one may also check if taking only the 1000 with the longest decription would help

or only those ones where BERT can sucessfully classify the faculty. Again, faculty is not everything, BUT if the faculty CAN be extracted, cases that only list names or places are out (...except FROM the name of place follows the faculty but lets ignore that)


\todoparagraph{Are short descriptions worse?}

\paragraph{concluding remarks}

In general we must say that we have really good accuracies and this really seems to indicate that it works.  We could to the shallow-decisiontrees-thingy for other attributes of the dataset than I currently have (see future work)

\subsection{Hyperparameters results}

I considere the results of 165 different param-combis in the final run, but so many previous runs were there as well were I looked at previous stuff (see the image that I originally had in workflow)

\includeMD{pandoc_generated_latex/5_hyperparamresults}

\subsection{Robustness}

Robust meaning here that multiple runs yield similar results. We said before that in word2vec the actual directions of the vectors are completely incomprehensibly dependent on the random initial conditions, and we also said that that is an absolute no-go in conceptual spaces.

Already here go into detail how unrobust the implementation of derrac and the others is. 

Show and interpret results for how robust my implementation is and if close words are close

..then again elaborate that explicit word choices are not too relevant and also not tested here and that I already experimented with different techniques (see appendix for keybert vs the others) but that there is no formal way to test them.

I hypothesize (see \autoref{sec:extract_cands}) that the reason I am having more robust results than \cite{Derrac2015} is among others because I have less candidates.

Results of multiple runs: 

from all the 165 different param-kombis I did, the following had enough (ndims*2) kappas-over-0.5:
\{3: 40, 50: 38, 200: 15\}





\subsection{Did we achieve the thesis goal?}

\includeMD{pandoc_generated_latex/5_2_siddata}

\section{General Algorithm}

% Now that we seen the algorithm in action and painstaking implemented it we are experts on the algorithm of \textcite{Derrac2015}, so let us critically reflect on the algorithm as such.

After having presented and meticulously implemented the algorithm of \textcite{Derrac2015}, we are familiar enough with the relevant topics, allowing us to now critically reflect on the algorithm in general.

\subsection{Algorithm idea}

When first looking the algorithm of \textcite{Derrac2015}, it appears astonishingly specific. However after having read G√§rdenfors' book \textcite{Gardenfors2000a}, the idea for their algorithm is self-evident: In it, G√§rdenfors suggests that conceptual spaces can be generated from high-dimensional sensory input by using \gls{mds} to project the original data into a Euclidian space to do geometric reasoning in that space. The book has an entire chapter on computational aspects, in which the author discusses vector space models, dimensionality reduction techniques an \gls{ann} architectures for different levels of human conceptualization. According to that, MDS is especially good at dealing with pairwise distances judgements from a subject's perception, to create a more \textit{economic} representation for \textit{phenonemal} \glspl{cs} \cite[221]{Gardenfors2000a}. \textcite{Derrac2015} did not create the space from sensory data but from text corpora, where the distance of two texts can be measured by the words they share. Their algorithm reasonably combines the idea to generate \glspl{cs} with the steps for a classical \gls{nlp} pipeline as described by \cite{Turney2010} (\autoref{sec:vsm_construction}) Given that, the core contribution of \cite{Derrac2015} mainly lies in the idea that the \textit{faithfulness} of a potential direction for the resulting semantic space can be assessed by the performance of the corresponding decision problem.

\paragraph{Measuring the Faithfulness of directions}

It seems reasonable that this assumption holds for the datasets originally considered by \textcite{Derrac2015}. As extensively discussed, when concatenating reviews of movies or tags describing pictures of places it is natural that words that describe a salient feature of the respective entity occur more often. The Siddata-dataset consists of short descriptions without this property, where most words have a relative document-freqeuncy of less than a percent. However, the technique of comparing the ranking induced by the classification with the score of the words still yielded enough results, and the directions seem to consist of interpretable properties. This surprisingly confirmed the robustness of the algorithm in that regard.

% Under the assumption of the \gls{distribhyp}, the assumption that this is possible seems reasonable. \todoparagraph{Naher drauf eingehen warum? Ich kann nicht einfach distributional hypothesis sagen und fertig}

\subsubsection{Requiring MDS}
\label{sec:discuss_mds}

\textcite{Derrac2015} explicitly state that MDS is the best dimensionality reduction technique for their algorithm, as it is one of the few ones that result in a metric space. In their paper, they describe SVD (the mathematical algorithm behind \gls{lsa}) as a popular technique for dimensionality reduction, but further state that \q{SVD produces a representation in which entities correspond to vectors, which should be compared in terms of cosine similarity rather than Euclidean distance. [...] However, we can expect that spatial relations such as betweenness and parallelism [...] are not meaningful in the representations derived from SVD} \cite[14]{Derrac2015}. These relationships are required for semantic classifiers which mimic analogical and betweeness-based reasoning, which they demonstrate to work for all of their domains. However, this space does not have semantic directions. The \textit{final} feature-based representation of the entities is reached by ranking each entity for each of the feature directions and creating new vectors from these ranks. As acknowledged by \textcite[22]{Derrac2015}, the feature vectors are not orthogonal, not linearly independent and only of ordinal scale level, withhout meaningful distances. \textcite{Derrac2015} create semantic classifiers both for the \textit{intermediate} space with meaningful distances (geometric betweeness- or parallelism-based classifiers) as well as for feature-based representation (a-fortiori-classifiers, see \autoref{sec:reasoning}). However, \textcite{Ager2018,Alshaikh2020} are only interested in the latter space and its resulting feature axes. If that is the case however, it becomes irrelevant if the intermediate space is metric or not, which enables for other algorithms to be used in that step. As stated in \autoref{sec:dim_red}, LSA may be the better choice as it explicitly detects latent topics in descriptions instead of relying on words that are explicitly mentioned. This may also lead to other desirable properties such as comparability of documents and phrases.

If for the explainable classifiers the relevant space is the one with semantic directions, geometric properties of the intermediate space are irrelevant. Instead one should try to generate a space that has both a useful metric \textit{and} interpretable directions. Depending on what the authors consider to be the end result of their algorithm, only one of these necessary requirements for conceptual spaces is fulfilled. So if their end-result is the intermediate space, that is nothing more than a normal \gls{vsm} with some interesting but long-term useless geometric properties. Instead, another possibility may be to calculating a new orthonormal basis on the coordinate system. The next step would then be to enforce orthogonality of the semantic directions as good as possible, and using linear algebra for a change of basis for the entites, such that not only their ranks but their exact position for each semantic direction (axis) is relevant. Subsequently, one could use techniques like Principal Component Analysis to decorrelate the directions. In this way, one would obtain vector without a name, which however could again be found with techniques that rely on \glspl{cos} such as LSA.

\textcite{Ager2018,Alshaikh2020} both do not this interim space and only use re-embedded one, but retain the use of MDS regardless. To our best understanding, they have no reason to do so, giving impression that they read to use MDS in G√§rdenfors' book and then forgot that their final re-embedding step makes that irrelevant. 

\paragraph{Using classical techniques}

In fairness, \cite{Ager2018,Alshaikh2020} both experiment with modern neural embeddings for the entities such as averaged \gls{word2vec} or \gls{doc2vec}. Both authors report that this decreased performance (see \autoref{tab:f1_placetypes_long}) compared to their MDS-condition.\footnote{Even though \textcite{Alshaikh2020} state in their paper that relying on better embeddings such as \gls{bert} \cite{Devlin2019} may lead to better results.} However, while they use neural embeddings, they do not adjust any of the later steps to regard for that: they just replaced the \gls{vsm} generated classically in the first steps of the algorithm with a neural embedding, such as averaged GloVe embeddings \cite{pennington2014glove} of the words in the text. On that they ran the exact original algorithm of creating a frequency matrix from the \gls{bow} and using a linear classifier to get the direction. 

% \todoparagraph{Wenn man schon embeddings nutzt dann soll man auch die eigenschaft dass die sinnvolle richtungen haben nutzen. Und wenn man dann shcon mit vektoren arbeitet dann kann man auch lsa undso nutzen. Und dann hat man auch nicht mehr das problem mit too-small-positive-class}

They do not consider looking for latent topics or make use of the algebraic properties that are given for these embeddings (see \autoref{eq:w2vregularity}). A possible avenue could be to look for shared vector components of entity-embeddings and candidate-word-embeddings. Instead, they still rely on the usage of linear classifiers that split according to frequency matrices. We will look more detailed into that soon, but this makes much more sense for \textit{points} in Euclidean spaces than it does for \textit{vector} embeddings where the space is built up from a cosine-distance-similarity-based objectives (see \autoref{sec:mds}). Instead of that, it seems the better idea to take advantage of dealing with vectors, such as the fact that they are inherently directional already. As discussed in \autoref{sec:lsi}, it seems that the concepts of \gls{lsi} to compare the similarity of documents and candidate feature directions seems more appropriate. This has added benefit that not only words literally occuring in the respective texts can be considered, which among others counters the previously stated problem that the used classifiers deal with heavy class imbalance as well accounting for polysemy and synonymy.

\removeMe{

\todo 
\paragraph{Other things}
Their last step to cluster the good-kappa-ones is very basic and has much room for improvements, see my suggestions.
Their merge-cnadidate-step (alle nehmen und die zum closestem herclustern und dann die richtung des $T^0.5$ √ºbernehmen) also has much room for improvement, see my suggestion for another algo - I did implement already some stuff but many better ones are imaginable (see future work)
better ways of getting rid of irrelevant clusters (see my suggestion and also problems with stopwords)

They do one SVM per term and then cluster similar ones. Ther terms sometimes occur only in like 50/15000 entities, so the validity of the kappa is should be doubted. \cite{VISR12} and many others first try to find latent stuff, which would improve that by a lot because its a lot less sparse. ("contains-one-of-the-terms" is a lot more than "contains-this-term" - that knowledge is also used by the postprocessing of Ager even though shitty.). According to \cite{Derrac2015} there are no methods that keep a metric space, however as discussed for our aim we can drop that.

\paragraph{Robustness}

\todoparagraph{We discussed Robustnes before!}
An algorithm that is so unrobust \wrt its precise parameter-choices seems a bad choice for conceptual spaces in general. A big issue why we disregarded VSMs is because of their arbitrary dimensions!

}

\subsection{Does the algorithm actually produce a Conceptual Space?}

\autoref{sec:cs} explained what a conceptual space is, before introducing \gencite{Derrac2015} algorithm to automatically induce them. However, their algorithm only approximates \glspl{cs} and does model some parts of the definition. A first difficulty with the algortihm is, that it needs a clearly defined domain from the start. It takes a corpus of texts and embeds each of these into a single high-dimensional vector space. If not all texts in the corpus are from a single domain, due to the similarity-based vector space generation, outliers will greatly affect the embedding of all entities (as \cite{Ager2018} discusses at length). This sounds irrelevant in practice, but the problem is that it is impossible to clearly define what a domain is. There is the set of place types, but this domain consists of various subdomains, and some concepts apply only to a specific subset of entities. The described algorithm cannot figure out such subdomains. This issue is partially addressed by the works of \textcite{Alshaikh2019, Alshaikh2020, Alshaikh2021} which eloaborate on the idea of subdomains. As they state, \textit{\q{When representing a particular entity in a conceptual space, we need to specify which domains it belongs to, and for each of these domains we need to provide a corresponding vector.}} \cite{Alshaikh2020} \todoparagraph{Wenn nicht in Algorithm geschrieben, dann hier die idee mit dem dass political nur relevant ist for a subset blabla} 

Also it is important to be aware of the difference between what \mainalgos and this work consider a domain (the set of movies, places or courses), and the definition of domain as used by \textcite{Gardenfors2000a}. According to the latter, a domain is a low-dimensional set of correlated properties, such as \textit{the color domain} consisting of \textit{hue, saturation} and \textit{value}. This also points out another difference of the original \gls{cs} definition and the definition used here: Conceptual spaces describe an entity through several uncorrelated low-dimensional vector spaces, not a single one with several dozen dimensions. As \cite{Ager2018} puts it more humbly, \textit{\q{The idea of learning semantic spaces with accurate feature directions can be seen as a first step towards methods for learning conceptual space representations from data [...]}}. Again it is referred to the works of \textcite{Alshaikh2019, Alshaikh2020, Alshaikh2021} which alleviate this by iteratively finding disentangled low-dimensional feature spaces.

As discussed earlier, the algorithm of \textcite{Derrac2015} first embeds the entities into a euclidian space where the concepts of betweeness and parallelism make sense, and subsequently create a feature-based representation that bases on an entity's rank \wrt several human-interpretable features. The final embedding is only of ordinal scale level and thus unable to model  degrees of similarities. In other words, the algorithm produces \textit{either} a space with a euclidean metric, \textit{or} one with interpretable directions, but no space that has \textit{both properties}. As both are necessary conditions for a conceptual space, \gencite{Derrac2015} algorithm at no point generates something that resembles a \gls{cs}. It remains unclear to us why they only consider the ranking of the entites regarding the feature axis instead of their distance which may retain relations of distances, for example by applying an arithmatic change of basis for the coordinate system (linear transformation). The way their algorithm works, the final space only has ordinal scale level and linearly dependent (correlated) dimensions. An interesting research avenue is to figure out what properites the space they have, and if small changes to the final algorithm step (such as PCA to decorrelate dimensions or not only taking the rank) could help in retaining the euclidean or at least another usable metric.

\subsubsection*{Points instead of Regions}

Another important difference between the resulting space and \glspl{cs} is that we are dealing with points instead of regions. Advantages of doing that include that it allows to distinguish \textit{protypical} examples from borderline cases \cite{Gardenfors2000a} and straight-forward application of ontological relations through the \gls{rcc} (see \autoref{sec:ontology_rcc}). \textcite{Derrac2015}, however, drop this assumption and work with vectors instead of regions, claiming that this is a \q{coarse-grained approximations of conceptual spaces, where points correspond to fine-grained categories instead of specific instances, while convex regions are used to model higher-level categories} \cite[8]{Derrac2015}. Despite that, they never address it again, so in the end they stick with points. This breaks with one of the key concepts from \glspl{cs} and also renders it impossible to simulate any of the ontological relations with the resulting spaces, which G√§rdenfors considered their most relevant practical application \cite{Gardenfors2004}. 

However, when elaborating on the idea that conceptual spaces can be induced using Kohonen-Nets (\autoref{sec:algo_variants}), G√§rdenfors himself claims that mapping regions of the original space to point-embeddings in the \gls{cs} is an \textit{advantage}, because it resembles \textit{generalization}. Another aspect to consider is whether the \glspl{entity} as considered by \mainalgos actually resemble what G√§rdenfors originally considered an entity. The entities in the used datasets of educational resources, placetypes or movies actually are specific instances instead of general \textit{concepts}. Instances in a conceptual space are correctly modelled as points - one could say that regions denote \textit{types}, with the individual points corresponding to their \textit{tokens}. Considering that we have only one instance per entity, we are dealing with types. Concepts (Regions in a CS) could accordingly be induced as the set of multiple tokens. A practical implementation could, for example, model the concept of \textit{introductory computer science courses} as a convex region spanned by all of the instances it contains. \textcite{Erk2009} propose an algorithm that creates such a region from a set of instances. It should be noted, however, that learning boundaries for such regions requires much more data \cite{Derrac2015} and the aforementioned reasoning on regions is computationally very complex \cite{Hernandez-Conde2017}.


\paragraph{Vectors or Points}
\label{sec:discuss_points}

The authors explicitly claim that they are dealing with points in a Euclidean space, which should be compared in terms of Euclidean distance \cite[14]{Derrac2015} instead of \gls{cos}. Despite this, in the merge-canidates step of their algorithm (\autoref{sec:algo:cluster}), they compare the candidate feature directions using the \gls{cos} of their orthogonals. As they require similarity of directions and not of positions, this approach seems plausible. However, these vectors do not have their origin in the coordinate base but in the position where they cut across the SVM's decision surface: A SVM is described by the vector of its orthogonal intercept, a scalar describing where the decision hyperplane crosses it. This means that one is dealing with \textit{affine frames} (which are described by basis and origin) instead of vector spaces. However, vectors in affine spaces cannot be compared solely by the angle between them\footnote{For better comprehension it is referred a StackOverflow question of \me at \url{https://stackoverflow.com/a/69407977/5122790}}. The way their algorithm is described, their merging of feature direction disregards the origin. Consider the following example: The SVMs for two candidates have exactly the same orthogonal vector, but different intercepts. There might be samples between the two decision surfaces, which are classified towards the positive class by the one classifier, and towards the negative by the other. If they classify samples differently, they must express different concepts. When only accounting for the direction of the orthogonal, such information gets lost. 

\subsection{Outlook}

There are also techniques that extend the algorithm of \textcite{Derrac2015}: \textcite{Alshaikh2019} take a vector space embedding and decompose it to several low-dimensional spaces, such that it corresponds more closely to the definition of a \gls{cs} which are split into multiple domain-specific spaces of low dimension. For that, they take the spaces from \cite{Derrac2015} to then cluster their features by domain and iteratively remove these groups to create multiple subspaces, while ensuring that \gls{word2vec} embeddings close to those of the removed ones are disregarded for future features.

\textcite{Alshaikh2021} want to get rid of MDS with its quadratic space complexity and also write a completely new, unsupervised ANN algorithm based on GloVe embeddings \cite{pennington2014glove}. In it, they learn domain-specific embeddings from the BoW and like \cite{Derrac2015} use classification, splitting entities that contain one of the verbatim candidates vs. those that do not. They train an \gls{ann} on this while also punishing close embeddings, similar to their previous work \cite{Alshaikh2019}.


\subsection{Regarding their research practices}

In \autoref{sec:howtoreplicate}, we stated the importance that all claims made in research should be reproducible and testable. While replicating the work of \cite{Derrac2015}some Questionable Research Practices came apparent. For example, 



\paragraph{Robustness}
\todo

\paragraph{Ambiguity}
\todo




% \includeMD{pandoc_generated_latex/5_3_evalderrac}




\section{Architecture}

As one of the thesis goals asked for a good and scalable architecture, let us also evaluate if the implementation fulfills the set criteria (\autoref{sec:success_conds}) and if the aspects we considered for a qualitative software and sustainable data analysis (\autoref{sec:reproducibility}) are fulfilled.
% We remember, we also wanted to build a good architecture and set goals for that, such as adaptability to new datasets etc. We said a good architecture would show in adaptability, scability, ..., so we wanna show that these are achieved. 

First of all, we wanted to show that that this implementation works and is able to replicate the results of one dataset of \mainalgos. This aim was reached, indicating \textbf{Functional Suitability} of our implementation and also the \textbf{Reproducibility} of the original algorithm.\footnote{If our implementation itself is reproducible can hardly be shown by \me in this thesis.}

Another set goal was that the code-base successfully runs on the \gls{ikw} compute grid. This goal was also reached, and we are very happy with the resulting implementation and its \textbf{Automation} and \textbf{Scalability}. Testing new hyperparameters is as easy as changing a YAML-file, transferring the file to the grid and executing a command such as \codeother{MA_ENV_FILE=siddata.env submit by_config --configfile config/CONFIGFILE.yml}. The maximum number of cores per node and of nodes available to a user (64) are maximally utilized, which also indicates optimal dependency resolution. Until the very last moments of this thesis, new components were added, running the algorithm on sample datasets worked without any complications in a matter of minutes (\textbf{Modularity, Maintainability, Adaptability}).

This all being said, allowing for all this was much more work than anticipated. \me has used with \textit{Snakemake} on clusters before, however the amount of customization to allow for workflow management on the \gls{ikw} grid was excessive. Part of this was due to heavy iterative restructuring of the code-base to comply with the logic as demanded by this workflow management system. Part of this is due to peculiarities of its configuration\footnote{For example that \textit{accounting files} that keep track of jobs are inaccessible to users which means that the scheduler needs to simulate their behaviour}, and to a huge part to the walltime-limit of 90 minutes. Because of this, most of the algorithm components need to be written in a way such that they both massiveley parallelize, but also gracefully end and store interim results, and the scheduler must comply with this. Comparing this with the uploaded implementation of \cite{Alshaikh2020}\footnote{\url{https://github.com/rana-alshaikh/Hierarchical_Linear_Disentanglement}}, which consists of one Jupyter Notebook and one Python file, gives indication as to the time invested in this thesis. We sincereley hope that this thesis helps in \textbf{Transparency} and that at least the cluster execution will be re-used by other students of the \gls{ikw}.

All the analyses that were conducted for this thesis are given in the accompaning source code. All plots and tables from all previous sections that were not reprinted were created in publicly available Jupyter-Notebooks, together with many more analyses that were conducted but would go far beyond the scope of this thesis. \todoparagraph{All plots and tables are explicitly linked} and easily re-creatable and runnable.\todoparagraph{In fact, even the} \LaTeX {source codes for most of the tables were automatically exported from python} A lot of work went into the architecture, and as soon as architechture and worklow management worked as intended, further development on the algorithm was suprisingly quick. This indicates the general aim of creating an architecture that helps to answer related future research questions is fulfilled. For the sake of brevity, this analysis and the code-base itself, which is available at \url{https://github.com/cstenkamp/derive_conceptualspaces} shall suffice as explanation regarding final two set goals. 

\removeMe{
    \todoparagraph{IN 3_3_architecture is a section} "My workflow to generate results", darauf nochmal eingehen und sagen dass das wirklich wunderbar funktioniert, dass es am ende nur ein wenig chaotisch ist weil man darauf achten muss dass man consistent ist in den conditions f√ºr best-config (random seed f√ºr die decision trees undso)
}


% \includeMD{pandoc_generated_latex/5_4_architecture}

\section{Future Work}
\label{sec:futurework}

\subsection*{Algorithm Addendums}
\todo

\paragraph{Stuff to better deal with the dataset}
\todo

\subsection*{Work ontop}
Everything From ager and Alshaikh
Use \cite{Erk2009}'s technique to make regions again
It is unclear to us why they only consider the ranking of the entites regarding the feature axis instead of their distance, why not try to do some linear algebra
\todo



\subsection*{Completely Different algo}
\todo



% \includeMD{pandoc_generated_latex/5_0_futurework}


\section{Conclusion}

\includeMD{pandoc_generated_latex/5_6_conclusion}