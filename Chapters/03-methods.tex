\chapter{Methods}

Direkt am Anfang schreiben dass ich halt auf den main algorithmus eingehe und das laut meiner research diese 3 paper am besten den main algo beschreiben (bzw sinnvoll erweitern) - was nicht heißt dass das die einzigen in dem kontext sind, Alshaikh2019 bspw nutzen ja den main algorithmus, aber ja nur als komponente, und haben andere Ziele was sie dann damit machen

Im folgenden gibt es neben Datasets 2 main sections: algoritm and architecture. Dass Algorithm und Architecture 2 subsection von methods sind ist halt "Der allgemeine Algorithmus und die spezifische Anwendung" Warum Architecture section? es kostet extrem viel zeit die schwammigen formulierungen in den papern genau zu verstehen, man probiert super oft falsche parameter-kombinationen aus etc etc, es ist halt ein riesiger ewig langer lernprozess den man von vorne machen müsste wenn man es nachimplementieren möchte, ich hätte mir gewünscht die authors hätten darüber mehr worte verloren, and also the scalable reproducible open-science part. And also - it took me a shitton of time, way more than working on the algorithm (but NOW it can run so easily on the grid and all param-combis simultaneously, ..), so this is what you'll get.


\section{Datasets}

All considered algorithms \mainalgos use a dataset of 15.000 movies and their reviews on IMDB, as well as a placetypes-dataset to evaluate their methods. The former consists of the concatenation of all available reviews for movies from IMDB\footnote{\url{https://www.imdb.com/}}, whereas the latter a collection of tags from photos uploaded to Flickr\footnote{\url{https://www.flickr.com}} that co-occur with a certain placetype. Other considered datasets are wine reviews, posts to certain newsgroups and another IMDB-review-dataset (see \tref{tab:all_datasets}). 

All of these datasets have in common that they are made up from a collection of independent texts or tags, created by different people. This means, that the more obvious or distinct a property of the respective entity is, the more often words describing that property will be used as tag or as part of the review. For example, a movie that is \emph{scary} to a lot of people will lead to many reviews mentioning that, which means that the word scary (or other words commonly co-occuring with it) will have a high count in the concatenation of that review. The algorithm from \cite{Derrac2015} heavily leans on this property by using the (relative) frequency of certain words as signal for the importance of the concept they may refer to. 

The main considered dataset of this thesis unfortunately does not share this property, as the texts that belong to an entity are not collected from different independent texts, but solely from the description of that entity - while it may be the case that the more \emph{mathematical} a course is, the more often the word \emph{math} occurs in it description, but the correlation is likely not as prominent as in the aforementioned datasets. Interestingly, \cite{Alshaikh2020} also used three datasets that only use a sort of description for an entity: its Wikipedia\footnote{\url{https://en.wikipedia.org/}}-article.


\subsection{SIDDATA-courses}

% TODO: bei dataset section darauf verweisen dass große teile des siddata-datasets mit gtranslate übersetzt wurden und auf den entsprechenden anhang verweiseen

% * Steht ja schon woanders dass mein Datensatz anders ist als concatenated-movie-reviews und ich deswegen nicht einfach "je öfter 'scary' desco scarier" machen kann. Da gibt's several ways mit umzugehen
	% * Das sich-die-richtigen-wörter-per-candidate-svm-bootstrappen
	% * Mit LSI rausfinden welche Terme genausogut in dem Text hätten vorkommen können (hab ich auch irgendwo schon)
	% * Explizit einfach zu gucken "Welche Terme kommen oft in den gleichen dokumenten vor" (und das inverse (steht iwo im code)), und dann ne candidate SVM für grouped terms anstelle von einzelterms machen (auch schon iwo als code)
	% * Mit Wordnet hypernyms/hyponyns und synonyms zu finden damit ebenfalls zu arbeiten (kann man wit wordnet angeben welches abstraktionsniveau ich haben will?)
	%     * Abstraktionsniveau gibt's nicht in wordnet, das heißt das richtige layer zu finden ist schwer. Was man auf jeden Fall machen kann ist die Terme zu den bases ihrer synsets umzuwandeln (dadurch wird aus "math" und "mathematics" das gleiche), aber in anderen Fällen ist es halt so dass ich die Candidate-Terms schon vorher brauche und nur sagen kann "diese entity enhält X wörter die halt hyponyms von dem Term sind"

their algorithm is tailored to concatenated-reviews or concatenated-bags-of-tags. Take their success-metric for the SVMs splitting the embedding. The more often the word "scary" comes in the concatenated reviews, the more scary the movie is. Sounds legit. The more often the people that took pictures at a particular place mentioned the "nature" of that, the more relevant "nature" is to that place. Also legit. But in the descriptions for courses that involve a lot of mathematics, it is not necessarily the case that the term "mathematics" occurs often. So due to the different nature of my dataset I have to go beyond their algorithm at some points - in this case it is probably the case that different kinds of mathematical terms actually do occur more often, so I'd need calculate these kinds of kappas not based oon a single term but ALREADY on a cluster of terms (... and I can bootstrap my way there, because after I do this I get more words to add to my cluster, rinse and repeat!)


% * Dass man theoretisch sich den task einfacher machen kann indem man nur die correctly-classified Kurse des fb-classifiers verwendet
% * MEINEN DATENSATZ mal mit den anderen vergleichen!! 
% 	* die Plots die schon drin sind beschreiben und warum der Datensatz whack ist.
% * Den ganzen "wo ist mein dataset anders als deren" Kram (teilweise schon im text, teilweise very old)
% * Woher der Datensatz kommt, dass es ja version 2 des Kurs-Datensatzes von Johannes ist (kommt von: /home/chris/Documents/UNI_neu/Masterarbeit/OTHER/study_behavior_analysis/src/data/course_data/db_dump_new/course_dump_new.csv)
% * Meine Pre-Preprocessing Schritte die da ja auch noch gut rumfiltern und rummergen beschreiben
% * Candidate-Word-Threshold: movies has samples-to-threshold value of 100, placetypes has 35, 20newsgrups has 614, so for 8000 courses any threshold from 2 to 25 seems reasonable => \cite{Derrac2015} say they intentionally kept the number of candidateterms approximate equal (at around 22.000), so to do the same I'd need a threshold of [TODO: optimal value]
% * [AGKR18] use a dataset that has fucking scipy preprocessing
% * ausrechnen "um so gut zu sein wie die, müsste der datensatz größe xyz haben"
% * Die standard-whackities des datensatzes, dass halt viele nur sind "Tutoren sind: Susi Sorglos Willi Wacker", oder "Findet statt in Raum XYZ", oder dass alle Sprachkurse die gleichen Beschreibung haben (beispiel. `....len([i for i in descriptions._descriptions if "kompetenzen entwickelt befahigen akademischen berufstypischen" in i.processed_as_string()]) == 25  ... weil es genau 25 exakt gleiche Beschreibungen gibt, für die Fremdsprachkurse. Deswegen ist up to jede 5-wort-kombination davon ein extracted keyword`)
% * Der Kappa-Score der rankigns vergleicht ist für mich ne kack metric weil ich ebennicht reviews nehme und more-occurences better-candidate heißen -> gucken wie ich stattdessen gute dimensionen und cluster finde (klingt doch so als sei accuracy/f1/... doch wichtig)

\input{include/dataset_table}


% Empirie, auch specifics über den Datensatz

%To write:
% * where does the data come from
% * what size is the data, what is the distribution, ...
% * Preliminary analysis (if I delete all that are shorter than X, it are |Y|..)
% * Does it cluster and look nice?
% * Verteilung der Sprachen
% * Preprocessing in kurzem Fließtext beschreiben - "After throwing out all descriptions shorter than xyz chars, 2323 courses where left. 223 of these were ..."
% * That the type of dataset differs from DESC15 and followups - mainly used movie-dataset consists of concatenated reviews (which means relevant words occur more often!) 
%     (TODO: look/think was die anderen auszeichnet - bei dem placetypedataset ists ja gar kein fließtext sondern direkt ein bag-of-tags)
% Dass mein Datensatz kleiin ist! Bei keinem sonderlichen min-word-per-desc threshold hab ich halt 7588 samples, bei 50 schon nur noch 4123, das ist wirklich little
% Dass auch die Descriptions echt kurz sind! Ich hab rund 8k samples, um das selbe samples-to-threshold verhältnis zu haben wie DESC15 wäre rechnerisch ein wert von 2 bis 25 sinnvoll (wobei man beachten muss das 2 schon richtig kacke ist weil dann die SVM 2 vs 8000 klassifizieren muss and that will never work -> 25 ist minimum), ABER wenn ich dann 25 nehme hab ich nur 2.4k candidates statt the 22k DESC15 aimed at, which also sucks!! --> CONCLUSION: Datensatz scheint zu klein.

The main goal of this thesis was to create a conceptual space of courses, automatically generated by course descriptions.


For that, a dataset of courses and their descriptions was obtained as export from the Stud.IP system as used at the universities of Osnabrück, Hannover and Bremen.
%TODO wait, woher kam der datensatz überhaupt? Tobias hat mir den geschickt, aber kam er zustande im Rahmen von Siddata?

The dataset comes from Johannes' Repo at \url{https://git.siddata.de/jschrumpf/study_behavior_analysis} (requires authentification over UOS!)

\begin{figure}[H]
	\centering
	\includegraphics[width=\figwidth]{graphics/figures/courses_language_distribution.png}
	\slcaption{
		\label{fig:courses_language_distribution}
		Distribution of languages of course descriptions.
		%TODO figure if this is the correct amount of preprocessing/throwout to have done
		Of the 21337 courses left after preprocessing, 18,679 were in german language according to the \textit{langdetect} python-package (for details, see \aref{ap:translating}).
		}
\end{figure}


The faculty is easily obtainable from the dataset, as the first one or two digits of the course ID correspond to it. The distribution of the faculties is depicted in figure \ref{fig:faculty_plot}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\figwidth]{graphics/figures/faculty_plot.png}
	\slcaption{
		\label{fig:faculty_plot}
		Distribution of faculties in the courses
		}
\end{figure}

The purpose of the Neural Network classifier is to check if it is anyhow possible to extract meaningful information from the descriptions: If it is possible to train a classifier on the data that can reasonably predict a qualitative feature, there is enough structure in the data such that the algorithm I'm about to produce can work.
Also, we have a lower bound for useful data: we can just throw away data that cannot be classified!
%TODO: train a second classifier on something else and throw away data that gets classified by neither and inspect it

(-> 91\% test accuracy)

% =============== Besonderheiten vom Siddata-datensatz

....len([i for i in descriptions._descriptions if "kompetenzen entwickelt befahigen akademischen berufstypischen" in i.processed_as_string()]) == 25  ... weil es genau 25 exakt gleiche Beschreibungen gibt, für die Fremdsprachkurse. Deswegen ist up to jede 5-wort-kombination davon ein extracted keyword
(und das obwohl sie verschiedene Namen haben! merging them doesn't make sense but they are almost equal)

% =============== Schreiben zum Thema Datensatz-Vergleich:

...ist es richtig dass nur 6000 verschiedene Terms >= 25 mal vorkommen?! 6000?!
=> auch in groß ist mein datensatz ja noch deutlich kleiner als placetypes, die haben immerhin 22k candidates
--> n-docs: 7596
--> 1-grams >= 25 times: 5054, 1-5-grams >= 25 times: 6717
--> unique 1-grams: 106235

bei placetypes sind es 
* unique 1-grams: 746180, davon 41320 >= 25 mal und 21833 >= 50 mal (their threshold)

--> das verhältnis Anzahl Texte zu Länge Texte ist bei mir halt komplett off 

% =============== 


\subsection{Place-Types}

\begin{itemize}
	\item Took it to be able to compare my results to the ones of \mainalgos
	\item Did NOT do the movies-dataset (also used by all \mainalgos, see \tref{tab:all_datasets}), because version available online does not contain n-grams so it will not be comparable
	\item Also took it to be able to sanity-check if my implementation was correct, which was extremely helpful
	\item Didn't do the openCYC taxonomy bc they say that they don't use one level of the taxonomy consistently but also never explain where they go to which level
\end{itemize}

%TODO: write IN THE ALGORITHM & ARCHITECTURE SECTIONS that I of course tried the placetypes-dataset as sanity-check to find errors - for that dataset, stuff like the good-candidates is known so as long as I don't reach their performances for that dataset I know my code is the problem, but as soon as I reach their performance I can savely say that the actual algorithm is correct and if it's still bad on the siddata dataset it's just not applicable to this kind of data

So, infos from \cite{Derrac2015}:
\begin{itemize}
	\item GeoNames has 667 place-types in 9 categories (403 used)
	\item Foursquare has 435 place-types in 9 top-level categories (391 used))
	\item content: tags of Flickr photos. Photos assumed to be of a type if one of the tags is the name of that type (so they queried for photos with that tag), and then all other tags of that picture make up the BoW.
	\item 22816139 photos, types with less than 1000 photos removed.
\end{itemize}


Also tried the Plactypes-Dataset used by all main-paper-authors. When doing so I noticed that there are definitely duplicates (which are consistently recognized as closest-terms in embedding):
  abandoned rail road and abandoned railroad
  boat yard and boatyard
  coral reef and reef
  court house and courthouse
  grass land and grassland
  sheep fold and sheepfold
  skate park and skatepark
  steak house and steakhouse
  water fall and waterfall
  wind mill and windmill

Next to that, the embedding however also sees very similar ones as very similar, which is a nice sanity-check, eg.

  abandoned farm and abandoned home
  airfield and airport
  airport and airport terminal
  ancient site and archaeological site
  arch and arch bridge
  art gallery and art museum
  coffee house and coffee shop
  aircraft cabin and airplane cabin
  apartment and apartment building
  bank and bank building
  field hockey field and hockey field

\subsection{Other Datasets}

Also tried a dataset of 100.000 coursera course reviews from \url{https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset}. Why? Because it's also eduactional resources, but as it's reviews it seems closer to the movies dataset
See \url{https://www.kaggle.com/roshansharma/coursera-course-reviews} for exploratory analysis of the dataset (there he also has another dataset he writes about, but you cannot merge them unfortunately, so besides course name the only possible task is the rating)
%TODO: I could try to merge it with this one https://www.kaggle.com/siddharthm1698/coursera-course-dataset or another one (see https://www.kaggle.com/mihirs16/coursera-course-data which links names to links, https://www.kaggle.com/search?q=coursera+in%3Adatasets for other places)

Also, there's the Large Movie Review Dataset\footnote{\url{http://ai.stanford.edu/~amaas/data/sentiment/}, \url{https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html}}, also used by \cite{Ager2018}.




\section{Algorithm}

\input{Chapters/sections/methods_algorithms}


\section{Architecture}
\label{sec:architecture}

As elaborated in \autoref{sec:reproducibility}, one of the main motivations for this thesis was to create a publicly available \textit{open-source} version of the algorithm that is easily \textit{understood} and \textit{reproduced}, \textit{adaptable} for other datasets and methods, as well as fast and \textit{scalable}, meaning it can be run maximally efficient on single machines but also on compute clusters, such as the \acrshort{ikw} Grid.
%TODO: Hier schon eindeutig sagen dass es auf ner single machine infeasibly lange läuft und deswegen der ganze Bums fürs Grid nötig war!!

% Main goal: BETTER ARCHITECTURE. Most important things for that: scalability, modularity, transparency, reproducibility, understandability, objectiveness, systematicacy, sustainability, adaptability
% describing this because I want to encourage extending the code etc and for that not only the algorithm but also the architecture should be described 
% and I think that was successful: This codebase contains everything and finally fulfills code-standards! 

This section will outline the architecture that was developed in order to achieve the aforementioned results. The resulting pipeline is the result of a lot of trial-end-error, but fulfills all of the aformentioned criteria, dealing with vastly differing sizes and kinds of datasets, minimizing runtime wherever feasible and allowing for a multitude of parameters at every step of the process. %TODO: don't like this paragraph, lieber später nohcmal auf die design principles eingehen und sagen dass sie alle fulfilled sind.

The rest of this section will go into further detail regarding the architecture of the resulting code-base. \todoparagraph{it will start with xyz and then asdf and then yaddayadda}

\subsection{Implementation}

The associated program is written by the author of this work and licensed under the \emph{GNU General Public License} (GNU GPLv3). The source code is written in the Python Programming Language and available digitally on GitHub\footnote{Source code: \url{https://github.com/cstenkamp/derive_conceptualspaces/}\\Source of this Document: \url{https://github.com/cstenkamp/MastersThesisText/}\\Compiled Document: \url{https://nightly.link/cstenkamp/MastersThesisText/workflows/create_pdf_artifact/master/Thesis.zip}}. In order to ensure that no work after the deadline is considered, it is referred to the signed commits \todoparagraph{COMMIT} and \todoparagraph{COMMIT}. 

The code is a proper python-package that can be installed into any Python 3.10 environment using for example python's default package manager pip:\\ \mytokens{pip install git+https://github.com/cstenkamp/derive_conceptualspaces.git@main}~ .\\ It can then be run using \mytokens{python -m derive_conceptualspace <COMMAND>} \footnote{The command \mytokensfnote{python -m derive_conceptualspace --help} gives a peak into what sub-commands can be used}. For more information on how to invoke the code base with these commands it is referred to \autoref{ap:usecase_click}

To guarantee reusability of this code-base, there is also a \emph{Dockerfile}\footnote{{\url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/Dockerfile}}} that allows to easily create a \emph{Docker-Container\footnote{\url{https://www.docker.com/resources/what}}} from it\footnote{A Container can be thought of as a lightweight virtual operating system, in which the codebase is bundled together with all required dependencies, libraries and configurations, enabling users install this software on any system without having to download or install anything besides this container, irrespective of operating system or software versions on the host \acrshort{os}. For more info about the container, it is referred to \url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/doc/docker_intro.md}.}.

\subsection{Modularity}

The developed algorithm consists of clearly divisible components (as demonstrated in \autoref{fig:dependency_graph}), where the runtime for each of the steps is roughly in the same order of magnitude. All of the aforementioned (\autoref{sec:algorithm_steps}) steps are itself algorithms with many \gls{param} each. Furthermore, the framework described here does not even require particular algorithms for the individual components, but rather a classes of algorithms like \emph{dimensionality reduction techniques}. This means that in practice, there is a combinatorical explosion of settings and \glspl{param} that must be experimented with in order to find the best-performing one. Because of the clear modularity of the algorithm however, many of these become only relevant in a later step of the pipeline. Due to this, it is reasonable to make the architecture as modular as possible, storing interim results before every step, such that two parameter-combinations that differ only in \eg the fourth step of the pipeline can share the intermediate results up to that point, keeping the required computation to a minimum. 

The design principle of maximal modularity is the cornerstone of the developed pipeline. All of the interim results store the configurations that were required for the respective algorithm (and forward the ones of the input-files they transformed), as well as the created output and plots. When there are different possible algorithms for a step, it is ensured that its result are of the same format, as required by the next step. Many of the individual steps generate additional plots that can be used as sanity-checks to quickly inspect if the results so far are reasonable.

\subsubsection{Workflow Management}

A pipeline where multiple intermediate files for different parameter-combinations are created introduces the problem of \emph{dependency resolution}: Ultimately, there is supposed to be one final file for every combination. This file however relies on intermediate files, which in turn rely on intermediate files. To resolve these dependencies, there are many existing \textbf{Workflow Management Systems}. For this thesis, \textbf{Snakemake}\footnote{\url{https://snakemake.readthedocs.io/en/stable/}} \cite{Molder2021a} seemed the right choice.

Snakemake defines a small comprehensible domain-specific language ontop of python. With this, a workflow is described in terms of individual \textbf{rules}, each of which defining how an \textbf{output} is generated from several \textbf{inputs} using code or shell-commands. Through \textbf{wildcards}, these rules can be generalized and hyperparameters introduced \cite{Molder2021a}. The job of Snakemake is to infer a \gls{dag} from these, finding for every rule in the dependency tree for the demanded file an output that generates the required inputs, and to create jobs for all required instanciations of the wildcards if the required files are not already present. Importantly, Snakemake then also handles the inevitable scheduling problem: Due to (explicitly specified) restrictions of \acrshort{cpu} and \acrshort{ram} and the nature of the unresolved dependencies, not all jobs of the workflow can be executed simultaneously. Its scheduler favors maximal utilization of \acrshort{cpu} and parallelisation for minimal execution time \cite{Molder2021a}. Especially relevant was also that it allows to schedule these jobs on high performance clusters and computation grids, and supports among others the scheduling system \gls{sge} which is used to orchestrate jobs at the \gls{ikw} grid. Configurations for the grid, like the maximal runtime or the amount of \gls{ram} and \glspl{cpu} to request, can be specified per-rule as well as in special configuration files.
%TODO: gibt noch 1-2 buzzwords from paper, ich kann schonmal aufs Grid hinaus und dann halt der wann-ist-snakemake-sinnvoll-und-wann-nicht.

Snakemake was chosen because it is a lightweight system ontop of python, adding only a few lines of code to specify what inputs and outputs are created ontop of the \gls{cli} that is necessary to run and debug individual steps anyway. It is a useful tool if the workflow can be divided into rougly equally long steps which can run independently and heavily parallelized (possibly on multiple machines) with an optimal usage of resources. Its file-centric dependency resolution system allows to fill in missing steps seemlessly when working on specific configurations for later step, but on the other hand requires unintuitive customization if instead configuration-files with explicit parameter-choices declare the demanded output for dynamically generated filenames. Also it unfortunately doesn't allow debugging and has a comparably small community\footnote{As of \DTMdisplaydate{2022}{03}{16}{-1}, there are only 1256 question tagged ``snakemake'' on StackOverflow (\url{https://stackoverflow.com/questions/tagged/snakemake})}. \autoref{ap:usecase_snakemake} shows the different ways the full pipeline can be invoked using Snakemake.

%wenn viele parameter die an gwissen punkten relevant werden und später nicht mehr, wenn viele param-kombis, it's main thing is the automatic dependency resolvement (which means I can just tell it "hey I need this file" (automatically creating missing stuff), but with config-files you're abusing it. good for optimal CPU/RAM usage. Good if independent parallelzed steps, not if one main step. Have to abuse it for configs, no good way to debug, small comunities, nondynamic I need nondynamic filenames that are set from the start of the execution 


\subsection{Modes of Execution / Use-Cases}

It is possible to run the full pipeline for individual files as well as for a set of \gls{param}-configurations specified via configuration files, but also possible to run individual steps to inspect or debug the respective steps. To inspect and compare results it is possible to load all available parameter-configurations, as well as the complete history for a certain combination, listing the generated outputs and metrics. Further, individual configurations can be loaded in \emph{Jupyter Notebooks} to generate and export plots and tables from them (like the ones used in this text). The three main ways of exectution are:
%TODO: deutlicher drauf eingehen dass man wegen dem ganzen bums mit intermediate files undso speziell drauf achten muss dass 
% * keine plots/prints verloren gehen
% * man mitschreibt wann welche configs genutzt werden
% * immer eindeutig drauf geachtet wird dass dependencies für genau die konfigurationen as demanded verwendet werden!! 

\begin{description}[style=unboxed]
	\item[Running individual Steps per \gls{cli}] is the mode of choice when working on custom steps, as it allows to attach debuggers and executes in the main thread. If a later step is executed, it is also possible to automatically generated its required dependencies using the workflow-definition. Passing configurations is possible using configuration-files, command-line-arguments or enviroment-files/-variables. For usage-examples, see \autoref{ap:usecase_click}.
	\item[Loading existing Configurations for inspection] especially in Notebooks, allowing to easily load a complete configuration including all its dependencies to inspect and plot (intermediate) previously created results and outputs, also allowing to iterate over several configurations to compare their results\footnote{The tables used in thesis are also automatically exported as \LaTeX- code from the functions available there, as specified in their respetive references.}. For usage-examples, see \autoref{ap:usecase_notebook}.
	\item[Running/Scheduling a Workflow] This mode is used to execute several \gls{param}-combinations at once, specified via configuration-files. Thanks to heavy integration for cluster scheduling systems, this allows for heavily parallelisation of jobs. Executing such a workflow on computation clusters is special case of this and elaborated further in the following section. For usage-examples, see \autoref{ap:usecase_snakemake}.
\end{description}
% 3 ways: Snakemake for shitton of param-combinations, invididual steps via the CLI for looking, debugging, creating, and 
% context-loading for jupyter to inspect and plot results - allowing load-context, where you can call eg. `print(ctx.display_output("embedding"))` of every component, read in several configs, iterate over them, re-create plots, allow for show-data-info showing where plots are first used, ...


\subsubsection*{Running on the \gls{sge}}

Due to a combinatorical explosion in the \gls{param}-space as well as the computational complexity of the algorithm, running the pipeline a sufficient amount of parameter-combinations would take several weeks on a single machine\footnote{\todoparagraph{Give a few examples!}}. As the \gls{ikw} at the \gls{uos} owns a dedicated computation grid\footnote{\url{https://doc.ikw.uni-osnabrueck.de/content/grid-computing}} with considerable modern hardware\footnote{Currently comprising, among many others, of 26 machines with an i7-11700 \gls{cpu} and 64 GB \gls{ram}} which uses the \gls{sge} as workload manager, which is supported by snakemake, it was the obvious candidate. Snakemake encodes special configurations for clusters using \emph{profiles}\footnote{\url{https://snakemake.readthedocs.io/en/stable/executing/cluster.html}}, and there exists a profile for the Sun Grid engine\footnote{\url{https://github.com/Snakemake-Profiles/sge}}. Unfortunately, this default configuration does not take into account many of the pecularities of the \gls{ikw} grid and it needed to be heavily customized in order to work. Foremost, all available machines to \me have a runtime-limit of 90 minutes, which means all of the algorithm-steps that take longer than that must be able to be interrupted and gracefully shut down before getting killed and pick up the work on a new machine afterwards (including the job responsible for the workflow scheduling itself). Additionally, the arguments to request resources (such as \emph{memory} or \emph{parallel environments}) often differ from the documentation, and the \emph{accounting file} which keeps track if jobs succeeded is not available to users, so a custom one must be written. Resolving these and other issues required changing the available profile heavily, so the result was open-sourced\footnote{The resuling Snakemake-Profile is available and documented at \url{https://github.com/cstenkamp/Snakemake-IKW-SGE-Profile}. Note that it is heavily customized to the specific engine and thus includes explicit machine names or runtimes. This repository also contains convenience-terminal-commands to inspect failed pipeline-steps or to show the current progress of the current run. A sample output of the latter is presented in \autoref{lst:joblog}. Furthermore it contains \mytokensfnote{.sge}-files and shell-scripts to schedule or run a requested workflow (see \autoref{ap:usecase_snakemake})}. 


Scheduling on such engines interestingly unveils a whole new set of ``hyperparameters'' that have to be optimized to use the available hardware as efficiently as possible: there are limits of how many slots are available per user, there is a fixed walltime (and interrupting and restarting leads to overhead), and the effiency of multiprocessing is not linear in the number of threads per process. Thus, depending on the size of the dataset, resources must be divided among the steps with care. The required resources of the rules are accordingly dynamically allocated in the rule-descriptions of the workflow manager.

While the code required to scalably run on the \gls{ikw}-grid required much more work than expected, the result fulfills all demands perfectly, %TODO: WHAT demands
and the 64 allocated \emph{parallel environments} (slots) are maximally utilized, while most of the complexity of the scheduling system is abstracted away\footnote{To the best of \my knowledge, no attempts going beyond simple \mytokensfnote{.sge}-files as job-descriptions were attempted on the IKW-grid before, and much of the available documentation turned out to be false information (as consultations with the grid's administrator have shown).}. The workflow is installed and run with a single (well documented) command and can be customized using explicit configuration-files. A sample output of the custom-made watcher is listed in \autoref{lst:joblog}.  


\begin{widepage}
	\lstconsolestyle
	\lstinputlisting[
		caption={[Sample terminal output of the custom watcher, when running a full configuration on the grid.]Sample terminal output of the custom watcher, when running a full configuration on the \gls{ikw}-grid. The script lists the currently running jobs continously, including their progress and runtime and informs of finished jobs and failed jobs. There is another script that summarizes the progress as per snakemake's dependency-graph.}, 
		label={lst:joblog},
		float,
		floatplacement=h!,
		xleftmargin=-0.5cm, 
		xrightmargin=-0.5cm,
		]{listings/joblog\_grid.txt}
	\lstdefaultstyle
\end{widepage}

% \includeMD{pandoc_generated_latex/chapter_methods_section_architecture}

\subsection{Conclusion}

It was originally unexpected, but implementing an appropriate architecture for the present codebase has been a major focus of work for this thesis, and the result fulfills all of the desired design criteria: 

% reproducibility alone is not enough to sustain the hours of work that scientists invest in crafting data analyses. Here, we outlined how the interplay of automation, scalabil- ity, portability, readability, traceability, and documentation can help to reach beyond reproducibility, making data analyses adaptable and transparent.

\begin{description}[style=unboxed]
	\item[Modularity] has been the main focus in the design, so exchanging components or running individual steps is easy and intuitive.
	\item[Scalability] is reached thanks to massive parallelisation wherever possible as well as a professional workflow management system that is perfectly adjusted to the available cluster engine but also highly customizable for other engines.
	\item[Reproducibility and Adaptability] are guaranteed by rigorous encapsulation of components, completely automating the full data-analysis-pipeline, open-sourcing the code as proper package and containerization of the entire codebase for guaranteed and worry-free setup on any machine or compute cluster. The exact \gls{param}-combinations of \mainalgos are included (see \autoref{ap:yamls_for_origalgos}), allowing to re-create even the original papers using this code-base. Running the code on new datasets is extensively documented\footnote{\todoparagraph{TODO: link to that}} and a matter of minutes. Extending or exchanging steps of the pipeline is seamless due to a consistent and understandable data schema, and pre-existing analysis-notebooks can easily create informative plots and figures.
	\item[Transparency and Understandability] are ensured due to rigorous documentation\footnote{\todoparagraph{Link github-documentation!}} (among others in this thesis) at any level of detail, from rough descriptions to concrete code-examples. Code, documentation and used data are publicly and easily available. Many analyses are inlcuded with the source-codes, for example allowing to visualize all steps of the process that can work with arbitrary numbers of dimension interactively in 3D. Code, data and configurations are clearly divided. All steps of the pipeline are very explicit about the used configurations and dependencies (making them traceable) and generate output at configurable levels of verbosity. All intermediate output can be re-accessed using helper commands (see \footnote{\todoparagraph{ref the appendix with the show-info-command and a notebook with} \mytokensfnote{create_svm("mathematik", embedding, dcm, pp_descriptions, highlight=["Informatik A: Algorithmen", "Informatik B: Grundlagen der Software-Entwicklung"])}}), including clear traces of the first usage of parameters (as \eg in a plot as depicted in \autoref{fig:dependency_graph})
	% it is crucial that the analysis code is as readable as possible such that it can be easily modified (looking at you, 40 unnamed cmd-args!)
	% code is readable and well-documented 
	% mit 2 Zeilen code kannst du dir in nem Jupyternotebook nen 3D-Plot anzeigen mit ner SVM die "Mathematik" von nicht-mathe trennt, mit gehighlighted ob "Informatik A" und "Informatik B" beeinander sind
\end{description}