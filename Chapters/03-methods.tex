\chapter{Methods}

%TODO: IST KLAR GENUG: "ich replizier diese 3 algorithmen, aus diesen 3 papern"

This chapter explains the methods that were used to achieve the two goals of replicating the algorithm of \textcite{Derrac2015} with the improvements of \cite{Ager2018,Alshaikh2020} with a sustainable architecture and applying it to the domain of educational resources.  

To do that, we will firstly look at the datasets used by \mainalgos, as well as our own one and compare key statistics. The subsequent section then explains the used algorithm broadly before looking at each of its steps in detail. Based on that, the specific architecture that was created as well as the workflow used to generate meaningful results will be introduced to demonstrate its process. The final section explains what kind of methods and metrics were used to generate and interpret the results that will follow in the next chapter.

Given that there were two research questions, one asking if the replicating algorithm can be applied to another domain and the other asking for a reliable application, it makes sense to both look at datasets and general algorithm on the one side, but also at the worflow and architecture. Writing about the latter while also open-sourcing the code-base is especially useful to ensure ease and speed of future replication, such that all claims can be independently tested with the exact same implementation without having to rely on ambiguous and incomplete verbal explanations.\footnote{To achieve reproducibility. Just like this thesis could have been a lot less effort if \mainalgos did that instead of ambiguous incomplete descriptions.}


%JOHANNES' KOMMENTARE TO WHAT I SENT HIM
% * Starker Unterschied zwischen Theoretisch (Dataset,..) und implementation unten in Terms of Verständlichkeit.
% => Implementation extrem verständlich, erster teil (dataset+algo) "teilweise unverstädnlich"
% * Struktur mit den Subsections so sinnvoll. Aber Streamlinen:
% * Verhältnis von Datensatz und Algo wird nicht klar und von den einzelnnen Datensätzen (am anfang sag ich ja es gibt das eine mit den 15.000 und beim zweiten sag ich goar nix dazu)


\section{Datasets}

\label{sec:datasets}

%TODO: wird klar genug "Die 3 haupt-autoren nutzen halt diesen algo auf diesen datasets, ich will domänentransfer machen, daher macht es sinn zu gucken ob die grundlegend unterschiedlich sind"
% -> damit auch main-claim (Kommentar Johannes, es war sehr unklar welche datasets mich inwieweit interessieren und warum ich die beschreib die ich beschreib)
% * Verhältnis von Datensatz und Algorithmus wird nicht klar
% * Warum ich die einzelnen Datensätze erwähne wird nicht klar


Let us first elaborate on the datasets used both in the works that are being replicated as well as the dataset used in the scope of this thesis. Doing both is important because (as explained in \autoref{sec:howtoreplicate}), to really know if an algorithm is a good choice for a task, it must be ensured that it does not only work on a special kind of dataset due to a special property that it happens to have. So, we will first compare all datasets to see if there are important differences between them, as for example in its structure, size or just general \emph{logic}.

\textcite{Derrac2015} used their algorithm to create conceptual spaces for three domains: \textit{movies}, \textit{placetypes} and \textit{wines}. Accordingly, they created corpora for these three domains. The two considered follow-up works both re-used the datasets for movies and placetypes and also created additional datasets respectively. \autoref{tab:all_datasets} shows a complete comparison of all datasets used in any of \mainalgos, including their origin, properties as well as associated pre-processing steps.

% NOW at first the ones from \mainalgos I am using

\subsection*{Comparing dataset properties}

This work applies the replicated algorithm to a new dataset, so it is important to check if the new dataset differs from the originally used ones. To ensure comparability in algorithm details, this implementation is also run on the \emph{placetypes}-dataset, and some of its statistics as well as that of the other two datasets made public by \cite{Derrac2015}.\footnote{\url{https://www.cs.cf.ac.uk/semanticspaces/}} Let us first compare some key statistics of them, before looking into two of them individually in the next section.

A very distinctive difference of the datasets is their size: the movies-dataset consists of 15.000 entities, placetypes of 1383, and wines of only 330. Just as important as the number of entities is however also the length of their associated text-corpora. Figures \ref{fig:sid_wordsperdesc} and \ref{fig:placetypes_dist_unique} show a histogram of the distribution of text lengths for the Siddata-dataset and placetypes-dataset respectively, and \autoref{tab:corpussizes} summarise corpus size and distribution of text lengths per entity for all handled datasets. Note that the Siddata-dataset is listed twice, once including all available entities and once after filtering out those whose description was shorter than 80 words.

\begin{table}[H]
	\centering
	\begin{tabular}{r|l|lll|lll}
		&  \textbf{Entities}   & \multicolumn{6}{c}{\textbf{Words per Entity}}                                 \\
		&       & \multicolumn{3}{c}{\textbf{Unique}} & \multicolumn{3}{c}{\textbf{Non-Unique}} \\
	 &  & \textbf{5\textsuperscript{th}} & \textbf{Med} & \textbf{95\textsuperscript{th}} & \textbf{5\textsuperscript{th}} & \textbf{Med} & \textbf{95\textsuperscript{th}} \\ \midrule
	\textbf{movie reviews}             & 13\,978 & 565    & 1358 & 5\,510  & 962   & 3179    & 38\,378     \\
	\textbf{place types}               & 1383    & 159    & 2215 & 18\,117 & 2378  & 55\,422 & 886\,233    \\
	\textbf{Siddata (all)}    		   & 26\,346 & 19     & 60     & 169   & 22    & 71      & 239         \\
	\textbf{Siddata} ($\geq 80$ words) & 11\,601 & 63     & 99     & 211   & 83    & 124     & 323         
	\end{tabular}
	\slcaption{Sizes of the text-corpora of the Siddata-dataset and those of \cite{Derrac2015}. The columns \textit{Words per Entity} list thresholds for the 5\textsuperscript{th}, 50\textsuperscript{th}, and 95\textsuperscript{th} percentile respectively.}
	\label{tab:corpussizes}
\end{table}

\begin{table}[h]
	\centering
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{rcccccccccc}
	\multicolumn{1}{l}{} &
	  \textbf{1} & \textbf{2} & \textbf{5} &  \textbf{25} &
	  \textbf{50} & \textbf{100} & \textbf{500} & \textbf{1000} &
	  \textbf{$|C|$/10} & \textbf{$|C|$/4} \\ \midrule
	\textbf{Siddata} & 163\,285 & 80\,802  & 35\,697  & 9702     & 5535     & 3059     & 570     & 210     & 23     & 2    \\
	\textbf{placetypes}  & 746\,180 & 428\,810 & 182\,906 & 41\,320  & 21\,833  & 11\,166  & 1452    & 183     & 8047   & 2669 \\
	\textbf{movies}  & 589\,727 & 279\,429 & 128\,850 & 55\,429  & 39\,976  & 28\,768  & 11\,431 & 6931  & 1786   & 332 
	\end{tabular}
	}
	\caption[Words that exceed various df thresholds for three datasets]{Words that exceed various \gls{df} thresholds for the Siddata, placetypes and movies datasets. In \cite{Derrac2015}, the candidate-threshold for placetypes was 50, and the threshold for movies 100. The last columns are relative to the dataset-size ($|C|$ = number of entities).}
	\label{tab:summed_unique_words}
	% SUMMED-TERM-FREQUENCIES:
	% \textbf{Siddata} & 163285 & 87840  & 40998  & 11619  & 6649   & 3722   & 765   & 317   & 60     & 8     \\
	% \textbf{placetypes}  & 746180 & 746180 & 746180 & 313858 & 202713 & 126899 & 41108 & 24735 & 101867 & 53588 \\
	% \textbf{movies}  & 589727 & 392885 & 200470 & 83354  & 60952  & 44809  & 20314 & 13558 & 5317   & 2484 
\end{table}

It is obvious that even though the number of Siddata-\glspl{entity} is comparable to the bigger of \cite{Derrac2015}'s datasets, the number of unique words in the individual texts associated with the entities are two orders of magnitude shorter than both. As the algorithm does not rely on deep learning, this does not mean that the dataset is unsuited for it, but let us explore how the distance in their lengths can be explained and what consequences this has.

The works considered in this replication \mainalgos all use the movies- and the  placetypes-dataset to evaluate their methods. The former consists of the concatenation of all available reviews for 15.000 movies from IMDB,\footnote{\url{https://www.imdb.com/}} grouped by the movie the reviews are for. The placetypes-dataset is created from a collection of tags that belong to photos uploaded to the photo sharing platform Flickr\footnote{\url{https://www.flickr.com}} that co-occur with other tags that denote one of several placetypes. Other datasets considered in the works of \mainalgos include wine reviews, posts to certain newsgroups, and another dataset created from IMDB-reviews (see \tref{tab:all_datasets}). 
% "their algorithm is tailored to concatenated-reviews or concatenated-bags-of-tags"

\input{include/dataset_table}

All of these datasets have in common that they are made up from a collection of independent texts or tags, created by different people. This means, that the more obvious or distinct a property of the respective entity is, the more often words describing that property will be used as tag or as part of the review. For example, a movie that is \emph{scary} to a lot of people will lead to many reviews mentioning that, which means that the word scary (or other words commonly co-occuring with it) will have a high count in the concatenation of all these reviews. The algorithm from \cite{Derrac2015} heavily leans on this property by using the (relative) frequency of certain words as signal for the importance of the concept they may refer to. 

The Siddata-dataset unfortunately does not share this property, as the texts that belong to an entity are not collected from different independent texts, but solely from the description of that entity. It may of course be roughly the case that the more \emph{mathematical} a course is, the more often the word \emph{math} occurs in its description, but the correlation is likely not as prominent as in the aforementioned datasets. It should however be noted that \cite{Alshaikh2020} also used the algorithm on three datasets where the description for an entity was collected from its Wikipedia\footnote{\url{https://en.wikipedia.org/}}-article, which also does not contain the described duplication of words.


\paragraph{Other considered datasets}

One of the goals was to build an adaptable architecture, so we also want to check if it can quickly and easily be applied to other datasets. For that, a dataset of 1002 short stories from project guttenberg\footnote{\url{https://www.kaggle.com/datasets/shubchat/1002-short-stories-from-project-guttenberg?select=db_books.csv}} and one of 100\,000 coursera course reviews\footnote{Source: \url{https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset}, Exploratory Analysis: url{https://www.kaggle.com/roshansharma/coursera-course-reviews}} were created. The former is similar to our dataset because it is also not made up as concatenation of multiple invididual texts. The latter is made up from reviews like the aformentioned ones, but from the educational domain. These were created primarily to test the architecture, and no results will reported or analysed for these datasets, as that would go far beyond its scope and intended length.
%TODO: I could try to merge it with this one https://www.kaggle.com/siddharthm1698/coursera-course-dataset or another one (see https://www.kaggle.com/mihirs16/coursera-course-data which links names to links, https://www.kaggle.com/search?q=coursera+in%3Adatasets for other places)


\subsection{Siddata-courses}
\label{sec:dataset_siddata}

% Generelle Analyse Datensatz mit Plots (Längenverteilung der Descriptions, ...)

% \todoparagraph{Wege damit umzugehen dass es nicht so ist dass relevantere worter haufiger vorkommen sind elaboriert in section xyz!!}


The main goal of this thesis was to create a conceptual space of courses, automatically generated by course descriptions. For that, a dataset of courses and their descriptions was obtained as export from the Stud.IP system as used at the universities of Osnabrück, Hannover and Bremen.

\subsubsection*{Resource Type and Origin}

As explained in \autoref{sec:siddata}, the dataset itself is collected collected from all courses that are present in the Stud.IP systems of the universities of Osnabrück, Hannover and Bremen and is crawled by the Siddata-\gls{dsa}. Additionally, it contains a small amount of \glspl{mooc} and other \glspl{oer} and resources collected by web crawers. The distribution of types (if an entity is collected from the \gls{lms}, an \gls{oer} or another web source) is depicted in \autoref{fig:sid_statistics} among the distribution of other metadata. Almost all educational resources in the dataset are \textit{Courses}, which refers to regular events at the respective university. As the figure shows, there are also PDF-documents in the dataset, but again the amount is neglibile. Roughly 75\% of all resources in this dump are courses from \gls{uos}, another 23\% courses from the universities Bremen and Hannover, the rest split among other formats. 

The data used here is collected from three separate dumps, created at different protype stages of the \gls{dsa}. The dumps had a high overlap in resources, however had different metadata attached to them, requiring careful merging for the creation of the individual raw datasets. So far, neither the datasets nor any of its underlying dumps have been published.\footnote{But the raw data is available for members of the SIDDATA-project at \url{https://git.siddata.de/jschrumpf/study_behavior_analysis} (requires authentification), and the dataset-creation notebooks for this dataset can be found at \urlx{https://github.com/cstenkamp/MAAnalysisNotebooks/tree/main/create_datasets/siddata}}


\begin{figure}[h]
	\includegraphics[width=1.15\textwidth,center]{graphics/dataset_new/statistics_bars.pdf}
	\slcaption{Distribution of metadata in the raw Siddata-dataset. Languages are reported as detected (see \ref{ap:translating}), other metadata as it was available in the dumps.}
	\label{fig:sid_statistics}
\end{figure}


\begin{table}[h]
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{lllllrrrr}
	\toprule
	 &  &  &  &  & \textbf{$\geq$20} & \textbf{$\geq$50} & \textbf{$\geq$200} & \textbf{$\geq$500} \\
	\textbf{Source} & \textbf{Type} & \textbf{Format} & \textbf{Uni} & \textbf{Lang.} &  &  &  &  \\
	\midrule
	\multirow[t]{3}{*}{\textbf{2021 Dump}} & \multirow[t]{3}{*}{\textbf{Stud.IP}} & \multirow[t]{3}{*}{\textbf{Course}} & \multirow[t]{3}{*}{\textbf{\acrshort{uos}}} & \textbf{de} & 18203 & 14551 & 2671 & 100 \\
	 &  &  &  & \textbf{en} & 1950 & 1773 & 616 & 53 \\
	 &  &  &  & \textbf{other} & 681 & 612 & 100 & 1 \\
	\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5}
	\multirow[t]{6}{*}{\textbf{2022 Dump}} & \multirow[t]{6}{*}{\textbf{Stud.IP}} & \multirow[t]{6}{*}{\textbf{Course}} & \multirow[t]{3}{*}{\textbf{\acrshort{uos}}} & \textbf{de} & 1009 & 807 & 188 & 5 \\
	 &  &  &  & \textbf{en} & 132 & 123 & 70 & 13 \\
	 &  &  &  & \textbf{other} & 20 & 14 & 1 & - \\
	\cline{4-5}
	 &  &  & \multirow[t]{3}{*}{\textbf{other}} & \textbf{de} & 5246 & 4353 & 1246 & 53 \\
	 &  &  &  & \textbf{en} & 771 & 622 & 260 & 25 \\
	 &  &  &  & \textbf{other} & 63 & 46 & 9 & 2 \\
	\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5}
	\multirow[t]{15}{*}{\specialcell[t]{\textbf{Educational}\\ \textbf{Resources}}} & \multirow[t]{4}{*}{\textbf{OER}} & \multirow[t]{2}{*}{\textbf{PDF}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{de} & 51 & 22 & 2 & - \\
	 &  &  &  & \textbf{en} & 1 & - & - & - \\
	\cline{3-5} \cline{4-5}
	 &  & \multirow[t]{2}{*}{\textbf{unknown}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{de} & 125 & 50 & 6 & - \\
	 &  &  &  & \textbf{en} & 7 & 4 & - & - \\
	\cline{2-5} \cline{3-5} \cline{4-5}
	 & \multirow[t]{6}{*}{\textbf{Stud.IP}} & \multirow[t]{6}{*}{\textbf{Course}} & \multirow[t]{3}{*}{\textbf{\acrshort{uos}}} & \textbf{de} & 993 & 684 & 121 & 5 \\
	 &  &  &  & \textbf{en} & 236 & 163 & 53 & 2 \\
	 &  &  &  & \textbf{other} & 168 & 130 & 91 & 25 \\
	\cline{4-5}
	 &  &  & \multirow[t]{3}{*}{\textbf{other}} & \textbf{de} & 737 & 514 & 145 & 23 \\
	 &  &  &  & \textbf{en} & 303 & 258 & 89 & 23 \\
	 &  &  &  & \textbf{other} & 33 & 23 & 3 & - \\
	\cline{2-5} \cline{3-5} \cline{4-5}
	 & \multirow[t]{3}{*}{\specialcell[t]{\textbf{Udemy-}\\ \textbf{\acrshort{mooc}}}} & \multirow[t]{2}{*}{\textbf{Course}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{en} & 22 & - & - & - \\
	 &  &  &  & \textbf{other} & 3 & - & - & - \\
	\cline{3-5} \cline{4-5}
	 &  & \textbf{\acrshort{mooc}} & \textbf{other} & \textbf{en} & 8 & - & - & - \\
	\cline{2-5} \cline{3-5} \cline{4-5}
	 & \multirow[t]{2}{*}{\textbf{web}} & \multirow[t]{2}{*}{\textbf{unknown}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{de} & 48 & 3 & - & - \\
	 &  &  &  & \textbf{en} & 1 & - & - & - \\
	\cline{5-9}
	 &  &  &  & \textbf{sum} & 30811 & 24752 & 5671 & 330 \\
	\bottomrule
	\end{tabular}
	\caption[Metadata of the Siddata-dataset]{Metadata of the Siddata-dataset. Languages are reported as detected (see \ref{ap:translating}), other metadata as it was available in the dumps. Column titles encode the number of entities whose description has at least 20, 50, 200 or 500 words. \label{tab:siddata_metadata}}
	}
\end{table}

\subsubsection*{Resource Content}

Considering that most of the data are Stud.IP courses, in the following only this format will be described.\footnote{As the final dataset will only consider resoures with a text-length of at least 80 words, the proportionality of courses is even more pronounced (see \autoref{tab:siddata_metadata})} The most relevant properties for the cause in this thesis are a courses' title, possibly its event number, and its description. The description consists of whatever the creator of the course wrote to describe it to aspiring participants. These descriptions differ a lot in their informativeness, and importantly they are no concatenation of reviews or similar from multiple persons but a continous text description. The distribution of the number of words per description is depicted in \autoref{fig:sid_wordsperdesc}, showing that 98\% of descriptions consist of maximally 339 words.


\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{graphics/dataset_new/words_per_desc.pdf}
	\slcaption{Distribution of description-lengths of the Siddata-dataset. The rightmost bar represents the longest 2\% of descriptions.}
	\label{fig:sid_wordsperdesc}
\end{figure}

\paragraph{Duplicates}

There are duplicates in the dataset: Often, the same course is offered over multiple years, which is mapped on to separate resources in the raw dataset. Unfortunately, there is no bijective mapping of course titles or event numbers, meaning that the same event number sometimes refers to the same course over the span of multiple years.\footnote{see \autoref{tab:sample_duplicates}} The name of courses also may change throughout the years it is offered. A very prominent characteristic is for example that courses cancelled due to the COVID-19-Pandemic prepended an information such as \textsc{!!Fällt aus!!} to its title, or that a few sentences explaining that the new iteration will be performed digitally. Some techniques have been used to merge possible duplicates based on their title, number and description, however not all duplicates could be eliminated. \footnote{Many such duplicates were mapped towards similar embeddings by the algorithm, see \autoref{sec:duplicate_maps}.} If the same title was connected to different descriptions in the dataset, the sentences of all duplicating samples were tokenised, concatenated and duplicates removed, such that the resulting description contained every sentence of the original description exactly once. This means that in cases where sentences were only slightly changed, both variants will be present. This is possible because the algorithm only relies on the Bag-of-n-grams representation for these texts (\autoref{sec:techniques:bow}), which is weighted and ignores word order.


\paragraph{Possible Classification Targets}

To evaluate the usefulness of our algorithm to recommend resources from this dataset, we will test how good the extracted semantic directions predict human classes extracted from the data. For that we need classification targets that can be extracted from the data. As stated above, those resources that are courses from the university of Osnabrück also have their course number given. The first digit of that encodes the faculty a course belongs to, making the faculty an easily obtainable classification target for 75\% of the entities.\footnote{Which is a lot more than there are for the placetypes-dataset, see \autoref{fig:scatter_mds_placetypes}} The distribution of the faculties is depicted in \ref{fig:courses_per_faculty}. If the dataset quality should turn out to be very low, it would also be possible to use a classifier that predicts the faculty from its description, and only use those ones that were correctly classified by it, which theoretically would ensure that only courses whose description is meaningful would be obtained. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{graphics/dataset_new/courses_per_faculty.pdf}
	\caption{Distribution of faculties for those courses at University of Osnabrück.}
	\label{fig:courses_per_faculty}
\end{figure}

\paragraph{Other Metadata}

Apart from \textit{title}, \textit{description} and \textit{course number}, the individual resources may have some additional properties. Among these are \textit{format}, \textit{type} and \textit{source} as meta-information encoding their origin and type. As most of the resources come from an older dump, these are not given for all. A small fraction of rerources additionally lists \textit{subjects}, which is a list of keywords describing the course. This is very relevant information for the algorithm, however as only a tiny fraction of entities lists it, the information is just appended to the respective description, just as the \textit{subtitle} of a course. Next to some other information only given for resources that are not of the format \textit{Course}, the only abundantly available metadata is the DDC-Code that was assigned to it by SidBERT (see \autoref{sec:sidbert}), which may be used as additional classification-target.\footnote{For that, however, one has to keep in mind that these were also only produced by a \gls{ml}-algorithm which has no perfect accuracy.} Besides this, there are unfortunately no further obvious candidates to generate prediction-targets from. This problem was discussed with \my supevisors, however the process was complicated due to privacy concerns, anonymization and inappropriate data formats.\footnote{One plan was to get the semester of a courses' enrolled students, but this data is stored timeless, and the mapping of the participation year is obfuscated through k-anonymization.}


\paragraph{Implications} There are quite a few duplicates in the dataset, the descriptions are shorter and different than the original ones, and many descriptions seem to be short and uninformative. The low dataset quality is a reason to later look at our results with a grain of salt: If for examply quantitatively examining if different courses are mapped onto the same embedding, it must also be qualitatively looked at these samples, as it may be the case that they are in fact multiple entries of the same course and thus correctly detected duplicates.

\paragraph{Low Quality Samples}
Unfortunately, the dataset contains many samples that have only very short and uninformative descriptions. The analysis-script in the repository\footnote{\url{https://github.com/cstenkamp/MAAnalysisNotebooks/blob/main/create_datasets/siddata/data_exploration_Siddata2021.ipynb}} does some exploration regarding length and quality of the data. Low-quality descriptions include: \textit{"BA/MA Hauptmodul"} - \textit{"Bestandteile:Vorlesung + Übung"} - \textit{"Dozent  Dr. Michael Wicke"} - \textit{"Siehe Gruppe A"} - \textit{"s. Modulbeschreibung"} - \textit{"Literatur:wird noch bekannt gegeben"}


% \includeMD{pandoc_generated_latex/3_0_1_datasets_siddata}

% \todoparagraph{Ref auf die Jupyter-plots, wie der sunburst-plot!}


\subsection{Placetypes}
\label{sec:dataset_placetypes}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{graphics/figures/placetypes_dist_unique.pdf}
	\caption{Distribution of unique words per entity for the placetypes dataset.}
	\label{fig:placetypes_dist_unique}
\end{figure}




Because the Siddata-dataset differs in many regards from the datasets used in \mainalgos, %AS stated above (in siddta-section, in the first part of datasecs-section, in workflow-section, ...)
it makes sense to compare the results of the given pipeline on that dataset to the results of \mainalgos. To be able to compare the results achieved here with theirs, one of the datasets used by all of their papers was considered as well. Comparing on an original dataset also has the benefit of allowing to sanity-check if the implementation is correct: If the performance achieved on this dataset is comparable to the performances of \mainalgos whereas the peformance for the Siddata-dataset is considerably worse, there is strong indication that the quality or quantity of the dataset is insufficient for acheiving high-quality representations. Conversely, if our algorithm applied to the dataset used in \mainalgos produces results that fall considerably short of those reported in literature, it is likely that there is a fault in our implementation.

There are two datasets used by all three authors: placetypes and movie-reviews (see \autoref{tab:all_datasets}). Both of the datasets are available in preprocessed form\footnote{\url{https://www.cs.cf.ac.uk/semanticspaces/}} \cite{Derrac2015}. It was originally planned to use both of the datasets as basis for comparison, however unfortunately it is impossible to recover the form of it as originally used by \textcite{Derrac2015}: The original texts of the reviews are not available, but only their respective bag-of-1-grams (\gls{bow}) - even though the authors explicitly state that they worked with variable-length-n-grams. Even though the provided list of candidates contains \glspl{ngram}, it is impossible to recover which of the \glspl{entity} contained it. While the algorithm can still be run only for the 1-grams, the results are not comparable with the original ones anymore.\footnote{As all of the \mainalgos share an author, it is assumed that \cite{Alshaikh2020,Ager2018} had access to the full version of the dataset and did not share the stated problem.}

In this work, we will only compare the results \wrt the GeoNames\footnote{\url{http://www.GeoNames.org/export/codes.html}} and Foursquare taxonomies, because the level used for the openCYK taxonomy was not sufficiently described by \textcite{Derrac2015}. For the GeoNames taxonomy, 403 of the placetypes are split into 9 categories. These labels are annotated to a \gls{tsne} plot genreated from \gencite{Derrac2015} uploaded data in \autoref{fig:scatter_mds_placetypes} in the Appendix. In the case of Foursquare-labels, 391 placetypes are split among 9 (different) categories as well.

The dataset was created from tags of images shared flickr\footnote{\url{https://www.flickr.com/}}. A picture was considered to be of a certain type, if one of its tags is the name of that type according to one of the aforementioned taxonomies, and the \gls{bow}-representation considered for this algorithm was created from all other tags of these pictures. In contrast to our dataset, this one was specifically created by \textcite{Derrac2015} to test their claims of the algorithm implemented here. Preliminary inspection of the dataset has however revealed that it however still contains many duplicates, such as the entries \textit{boat yard} and \textit{boatyard}, or \textit{skate park} and \textit{skatepark}, among others.


\section{Algorithm}


\input{Chapters/sections/methods_algorithms}

\section{Architecture}
\label{sec:architecture}
% \removeMe{
% TODO: nen dot-Dag mit den configs wie für desc15 in die Arbeit 
% (try out --dag, --rulegraph and --filegraph)
% }

As elaborated in \autoref{sec:reproducibility}, one of the main motivations for this thesis was to create a publicly available \textit{open-source} version of the algorithm that is easily \textit{understood} and \textit{reproduced}, \textit{adaptable} for other datasets and methods, as well as fast and \textit{scalable}, meaning it can be run maximally efficient on single machines but also on compute clusters, such as the \acrshort{ikw} Grid.
%TODO: Hier schon eindeutig sagen dass es auf ner single machine infeasibly lange läuft und deswegen der ganze Bums fürs Grid nötig war!!

% Main goal: BETTER ARCHITECTURE. Most important things for that: scalability, modularity, transparency, reproducibility, understandability, objectiveness, systematicacy, sustainability, adaptability
% describing this because I want to encourage extending the code etc and for that not only the algorithm but also the architecture should be described 
% and I think that was successful: This codebase contains everything and finally fulfills code-standards! 

This section will outline the architecture that was developed in order to achieve the aforementioned results. The resulting pipeline is the result of a lot of trial-end-error, but fulfills all of the aformentioned criteria, dealing with vastly differing sizes and kinds of datasets, minimizing runtime wherever feasible and allowing for a multitude of parameters at every step of the process. %TODO: don't like this paragraph, lieber später nohcmal auf die design principles eingehen und sagen dass sie alle fulfilled sind.

The rest of this section will go into further detail regarding the architecture of the resulting code-base. % \todoparagraph{it will start with xyz and then asdf and then yaddayadda}

\subsection{Implementation}

The associated program is written by the author of this work and licensed under the \emph{GNU General Public License} (GNU GPLv3). The source code is written in the Python Programming Language and available digitally on GitHub.\footnote{Source code: \url{https://github.com/cstenkamp/derive_conceptualspaces/}\\Source of this Document: \url{https://github.com/cstenkamp/MastersThesisText/}\\Compiled Document: \url{https://nightly.link/cstenkamp/MastersThesisText/workflows/create_pdf_artifact/master/Thesis.zip}} %In order to ensure that no work after the deadline is considered, it is referred to the signed commits \todoparagraph{COMMIT} and \todoparagraph{COMMIT}. 

The code is a proper python-package that can be installed into any Python 3.10 environment using for example python's default package manager pip:\\ \mytokens{pip install git+https://github.com/cstenkamp/derive_conceptualspaces.git@main}~ .\\ It can then be run using \mytokens{python -m derive_conceptualspace <COMMAND>}.\footnote{The command \mytokensfnote{python -m derive_conceptualspace --help} gives a peak into what sub-commands can be used} For more information on how to invoke the code base with these commands it is referred to \autoref{ap:usecase_click}

To guarantee reusability of this code-base, there is also a \emph{Dockerfile}\footnote{{\url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/Dockerfile}}} that allows to easily create a \emph{Docker-Container\footnote{\url{https://www.docker.com/resources/what}}} from it.\footnote{A Container can be thought of as a lightweight virtual operating system, in which the codebase is bundled together with all required dependencies, libraries and configurations, enabling users install this software on any system without having to download or install anything besides this container, irrespective of operating system or software versions on the host \acrshort{os}. For more info about the container, it is referred to \url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/doc/docker_intro.md}.}

\subsection{Modularity}

The developed algorithm consists of clearly divisible components (as demonstrated in \autoref{fig:dependency_graph}), where the runtime for each of the steps is roughly in the same order of magnitude. All of the aforementioned (\autoref{sec:algorithm_steps}) steps are itself algorithms with many \gls{param} each. Furthermore, the framework described here does not even require particular algorithms for the individual components, but rather a classes of algorithms like \emph{dimensionality reduction techniques}. This means that in practice, there is a combinatorical explosion of settings and \glspl{param} that must be experimented with in order to find the best-performing one. Because of the clear modularity of the algorithm however, many of these become only relevant in a later step of the pipeline. Due to this, it is reasonable to make the architecture as modular as possible, storing interim results before every step, such that two parameter-combinations that differ only in \eg the fourth step of the pipeline can share the intermediate results up to that point, keeping the required computation to a minimum. 

The design principle of maximal modularity is the cornerstone of the developed pipeline. All of the interim results store the configurations that were required for the respective algorithm (and forward the ones of the input-files they transformed), as well as the created output and plots. When there are different possible algorithms for a step, it is ensured that its result are of the same format, as required by the next step. Many of the individual steps generate additional plots that can be used as sanity-checks to quickly inspect if the results so far are reasonable.

\subsubsection{Workflow Management}

A pipeline where multiple intermediate files for different parameter-combinations are created introduces the problem of \emph{dependency resolution}: Ultimately, there is supposed to be one final file for every combination. This file however relies on intermediate files, which in turn rely on intermediate files. To resolve these dependencies, there are many existing \textbf{Workflow Management Systems}. For this thesis, \textbf{Snakemake}\footnote{\url{https://snakemake.readthedocs.io/en/stable/}} \cite{Molder2021a} seemed the right choice.

Snakemake defines a small comprehensible domain-specific language ontop of python. With this, a workflow is described in terms of individual \textbf{rules}, each of which defining how an \textbf{output} is generated from several \textbf{inputs} using code or shell-commands. Through \textbf{wildcards}, these rules can be generalized and hyperparameters introduced \cite{Molder2021a}. The job of Snakemake is to infer a \gls{dag} from these, finding for every rule in the dependency tree for the demanded file an output that generates the required inputs, and to create jobs for all required instanciations of the wildcards if the required files are not already present. Importantly, Snakemake then also handles the inevitable scheduling problem: Due to (explicitly specified) restrictions of \acrshort{cpu} and \acrshort{ram} and the nature of the unresolved dependencies, not all jobs of the workflow can be executed simultaneously. Its scheduler favors maximal utilization of \acrshort{cpu} and parallelisation for minimal execution time \cite{Molder2021a}. Especially relevant was also that it allows to schedule these jobs on high performance clusters and computation grids, and supports among others the scheduling system \gls{sge} which is used to orchestrate jobs at the \gls{ikw} grid. Configurations for the grid, like the maximal runtime or the amount of \gls{ram} and \glspl{cpu} to request, can be specified per-rule as well as in special configuration files.
%TODO: gibt noch 1-2 buzzwords from paper, ich kann schonmal aufs Grid hinaus und dann halt der wann-ist-snakemake-sinnvoll-und-wann-nicht.

Snakemake was chosen because it is a lightweight system ontop of python, adding only a few lines of code to specify what inputs and outputs are created ontop of the \gls{cli} that is necessary to run and debug individual steps anyway. It is a useful tool if the workflow can be divided into rougly equally long steps which can run independently and heavily parallelized (possibly on multiple machines) with an optimal usage of resources. Its file-centric dependency resolution system allows to fill in missing steps seemlessly when working on specific configurations for later step, but on the other hand requires unintuitive customization if instead configuration-files with explicit parameter-choices declare the demanded output for dynamically generated filenames. Also it unfortunately doesn't allow debugging and has a comparably small community.\footnote{As of \DTMdisplaydate{2022}{03}{16}{-1}, there are only 1256 question tagged ``snakemake'' on StackOverflow (\url{https://stackoverflow.com/questions/tagged/snakemake})} \autoref{ap:usecase_snakemake} shows the different ways the full pipeline can be invoked using Snakemake.

%wenn viele parameter die an gwissen punkten relevant werden und später nicht mehr, wenn viele param-kombis, it's main thing is the automatic dependency resolvement (which means I can just tell it "hey I need this file" (automatically creating missing stuff), but with config-files you're abusing it. good for optimal CPU/RAM usage. Good if independent parallelzed steps, not if one main step. Have to abuse it for configs, no good way to debug, small comunities, nondynamic I need nondynamic filenames that are set from the start of the execution 


\subsection{Modes of Execution / Use-Cases}

It is possible to run the full pipeline for individual files as well as for a set of \gls{param}-configurations specified via configuration files, but also possible to run individual steps to inspect or debug the respective steps. To inspect and compare results it is possible to load all available parameter-configurations, as well as the complete history for a certain combination, listing the generated outputs and metrics. Further, individual configurations can be loaded in \emph{Jupyter Notebooks} to generate and export plots and tables from them (like the ones used in this text). The three main ways of exectution are:
%TODO: deutlicher drauf eingehen dass man wegen dem ganzen bums mit intermediate files undso speziell drauf achten muss dass 
% * keine plots/prints verloren gehen
% * man mitschreibt wann welche configs genutzt werden
% * immer eindeutig drauf geachtet wird dass dependencies für genau die konfigurationen as demanded verwendet werden!! 

\begin{description}[style=unboxed]
	\item[Running individual Steps per \gls{cli}] is the mode of choice when working on custom steps, as it allows to attach debuggers and executes in the main thread. If a later step is executed, it is also possible to automatically generated its required dependencies using the workflow-definition. Passing configurations is possible using configuration-files, command-line-arguments or enviroment-files/-variables. For usage-examples, see \autoref{ap:usecase_click}.
	\item[Loading existing Configurations for inspection] especially in Notebooks, allowing to easily load a complete configuration including all its dependencies to inspect and plot (intermediate) previously created results and outputs, also allowing to iterate over several configurations to compare their results.\footnote{The tables used in thesis are also automatically exported as \LaTeX- code from the functions available there, as specified in their respetive references.} For usage-examples, see \autoref{ap:usecase_notebook}.
	\item[Running/Scheduling a Workflow] This mode is used to execute several \gls{param}-combinations at once, specified via configuration-files. Thanks to heavy integration for cluster scheduling systems, this allows for heavily parallelisation of jobs. Executing such a workflow on computation clusters is special case of this and elaborated further in the following section. For usage-examples, see \autoref{ap:usecase_snakemake}.
\end{description}
% 3 ways: Snakemake for shitton of param-combinations, invididual steps via the CLI for looking, debugging, creating, and 
% context-loading for jupyter to inspect and plot results - allowing load-context, where you can call eg. `print(ctx.display_output("embedding"))` of every component, read in several configs, iterate over them, re-create plots, allow for show-data-info showing where plots are first used, ...


\subsubsection*{Running on the \gls{sge}}

Due to a combinatorical explosion in the \gls{param}-space as well as the computational complexity of the algorithm, running the pipeline a sufficient amount of parameter-combinations would take several weeks on a single machine. As the \gls{ikw} at the \gls{uos} owns a dedicated computation grid\footnote{\url{https://doc.ikw.uni-osnabrueck.de/content/grid-computing}} with considerable modern hardware\footnote{Currently comprising, among many others, of 26 machines with an i7-11700 \gls{cpu} and 64 GB \gls{ram}} which uses the \gls{sge} as workload manager, which is supported by snakemake, it was the obvious candidate. Snakemake encodes special configurations for clusters using \emph{profiles},\footnote{\url{https://snakemake.readthedocs.io/en/stable/executing/cluster.html}} and there exists a profile for the Sun Grid engine.\footnote{\url{https://github.com/Snakemake-Profiles/sge}} Unfortunately, this default configuration does not take into account many of the pecularities of the \gls{ikw} grid and it needed to be heavily customized in order to work. Foremost, all available machines to \me have a runtime-limit of 90 minutes, which means all of the algorithm-steps that take longer than that must be able to be interrupted and gracefully shut down before getting killed and pick up the work on a new machine afterwards (including the job responsible for the workflow scheduling itself). Additionally, the arguments to request resources (such as \emph{memory} or \emph{parallel environments}) often differ from the documentation, and the \emph{accounting file} which keeps track if jobs succeeded is not available to users, so a custom one must be written. Resolving these and other issues required changing the available profile heavily, so the result was open-sourced.\footnote{The resuling Snakemake-Profile is available and documented at \url{https://github.com/cstenkamp/Snakemake-IKW-SGE-Profile} Note that it is heavily customized to the specific engine and thus includes explicit machine names or runtimes. This repository also contains convenience-terminal-commands to inspect failed pipeline-steps or to show the current progress of the current run. A sample output of the latter is presented in \autoref{lst:joblog}. Furthermore it contains \mytokensfnote{.sge}-files and shell-scripts to schedule or run a requested workflow (see \autoref{ap:usecase_snakemake})}. 


Scheduling on such engines interestingly unveils a whole new set of ``hyperparameters'' that have to be optimized to use the available hardware as efficiently as possible: there are limits of how many slots are available per user, there is a fixed walltime (and interrupting and restarting leads to overhead), and the effiency of multiprocessing is not linear in the number of threads per process. Thus, depending on the size of the dataset, resources must be divided among the steps with care. The required resources of the rules are accordingly dynamically allocated in the rule-descriptions of the workflow manager.

While the code required to scalably run on the \gls{ikw}-grid required much more work than expected, the result fulfills all demands perfectly, %TODO: WHAT demands
and the 64 allocated \emph{parallel environments} (slots) are maximally utilized, while most of the complexity of the scheduling system is abstracted away.\footnote{To the best of \my knowledge, no attempts going beyond simple \mytokensfnote{.sge}-files as job-descriptions were attempted on the IKW-grid before, and much of the available documentation turned out to be false information (as consultations with the grid's administrator have shown).} The workflow is installed and run with a single (well documented) command and can be customized using explicit configuration-files. A sample output of the custom-made watcher is listed in \autoref{lst:joblog}.  


\begin{widepage}
	\lstconsolestyle
	\lstinputlisting[
		caption={[Sample terminal output of the custom watcher]Sample terminal output of the custom watcher when running a full configuration on the \gls{ikw}-grid. The script lists the currently running jobs continously, including their progress and runtime and informs of finished jobs and failed jobs. There is another script that summarises the progress as per snakemake's dependency-graph.}, 
		label={lst:joblog},
		float,
		floatplacement=h!,
		xleftmargin=-0.5cm, 
		xrightmargin=-0.5cm,
		]{listings/joblog\_grid.txt}
	\lstdefaultstyle
\end{widepage}

\subsection{Conclusion}

It was originally unexpected, but implementing an appropriate architecture for the present codebase has been a major focus of work for this thesis, and the result fulfills all of the desired design criteria: 

% reproducibility alone is not enough to sustain the hours of work that scientists invest in crafting data analyses. Here, we outlined how the interplay of automation, scalabil- ity, portability, readability, traceability, and documentation can help to reach beyond reproducibility, making data analyses adaptable and transparent.

\begin{description}[style=unboxed]
	\item[Modularity] has been the main focus in the design, so exchanging components or running individual steps is easy and intuitive.
	\item[Scalability] is reached thanks to massive parallelisation wherever possible as well as a professional workflow management system that is perfectly adjusted to the available cluster engine but also highly customizable for other engines.
	\item[Reproducibility and Adaptability] are guaranteed by stringent encapsulation of components, completely automating the full data-analysis-pipeline, open-sourcing the code as proper package and containerization of the entire codebase for guaranteed and worry-free setup on any machine or compute cluster. The exact \gls{param}-combinations of \mainalgos are included (see \autoref{ap:yamls_for_origalgos}), allowing to re-create even the original papers using this code-base. Running the code on new datasets is documented and can be done in a matter of minutes. Extending or exchanging steps of the pipeline is seamless due to a consistent and understandable data schema, and pre-existing analysis-notebooks can easily create informative plots and figures.
	\item[Transparency and Understandability] are ensured due to rigorous documentation\footnote{\todoparagraph{TODO:} Link github-documentation!} (among others in this thesis) at any level of detail, from rough descriptions to concrete code-examples. Code, documentation and used data are publicly and easily available. Many analyses are included with the source-codes, for example allowing to visualise all steps of the process that can work with arbitrary numbers of dimension interactively in 3D. Code, data and configurations are clearly divided. All steps of the pipeline are very explicit about the used configurations and dependencies (making them traceable) and generate output at configurable levels of verbosity. All intermediate output can be re-accessed using helper commands.% (see \footnote{\todoparagraph{TODO:} ref the appendix with the show-info-command and a notebook with \mytokensfnote{create_svm("mathematik", embedding, dcm, pp_descriptions, highlight=["Informatik A: Algorithmen", "Informatik B: Grundlagen der Software-Entwicklung"])}}), including clear traces of the first usage of parameters (as \eg in a plot as depicted in \autoref{fig:dependency_graph})
	% it is crucial that the analysis code is as readable as possible such that it can be easily modified (looking at you, 40 unnamed cmd-args!)
	% code is readable and well-documented 
	% mit 2 Zeilen code kannst du dir in nem Jupyternotebook nen 3D-Plot anzeigen mit ner SVM die "Mathematik" von nicht-mathe trennt, mit gehighlighted ob "Informatik A" und "Informatik B" beeinander sind
\end{description}

% \includeMD{pandoc_generated_latex/3_3_architecture}


% \removeMe{

% \section{Workflow}
% \label{sec:workflow}


% \includeMD{pandoc_generated_latex/3_4_workflow}
% }


\section{Evaluation Metrics}
\label{sec:eval_metrics}
% Johannes 25.3.: das "How to Evaluate" ans ende von "Methods", und "Evaluation Metrics" nennen

Two goals were stated in this thesis' introduction: To implement a reliable software-architecture that successfully replicates the works of \textcite{Derrac2015}, and figure out if their methodology also works for the domain of educational resources. Given that there is not one single correct target that the algorithm needs to optimize for, there are also no obvious measurable metrics that can be applied straight-forward to test the performance of the algorithm and no obvious \textit{optimal results}. As the quality of the desired results is defined purely over its subjective appeal to humans\footnote{\cite[133]{Alshaikh2019}: \q{It does not seem possible (nor desirable) to formally define what constitutes a good facet, a typical problem in unsupervised learning}}, the only evaluation method that could quantify that would be a large-scale study with human evaluators, which unfortunately lies outside the scope of this thesis.

% hinleitung vom generellen "thesis goal" zu konkreten measruable sachen
Instead, one has to rely on qualitative analysis of certain produced features as well as proxy metrics. This section explains what kind of results have been chosen to represent the algorithm's performance as well as why these results are suitable proxies. Furthermore it is important to test if the architecture itself works reliably, which will be tested by generating results for the placetypes-dataset and comparing them with those of the literature.

Specifically, we are interested in the following questions:

\begin{enumerate}
	\item[\saveref{sec:results_placetypes}{1.}] Is the implementation correct? / Can it replicate the results of the original implementation?
	\item[\saveref{sec:results_datasetdiffs}{2.}] Is the algorithm able to cope with the Siddata-dataset despite it's different size and features?
	\item[\saveref{sec:results_siddata}{3.}] Does the general methodology work for the domain of educational resources?
	\item[\saveref{sec:results_params}{4.}] What combination of components and hyperparameters leads to the best results?
	% \item[\saveref{sec:results_architecture}{5.}] Does the architecture fulfill the features requested in \autoref{sec:goals_research_questions}?
\end{enumerate}

The first question is easily answered by applying the same evaluation that \mainalgos used and comparing their results with ours. To answer question two, we will compare the quantity and quality of some interim results of the algorithm being applied to placetypes with their counterparts for the Siddata-dataset. Question three and four rely on some quantifiable notion of what constitutes a \emph{good} result, so this is the first question that needs to be answered to know if the algorithm produces meaningful results.

\subsection{Proxies hinting at meaningful results}

% "To evaluate whether the discovered features are semantically meaningful, we test how similar they are to natural categories, by training depth-1 decision trees"
The goal of the algorithm is to find semantic directions in the data and re-embed the entities into a vector space spun up by these interpretable human concepts as its axes. Thus, to evaluate if the emerged features are semantically meaningful, it can be measured if any known \textit{natural categories} are among the detected semantic directions or similar to them. More specifically (as the algorithm works with \glspl{bow} which lose any information about synonymity and similarity of tokens) %TODO: irgendwo das quote von \cite{Le2014} über BoW einbringen und darauf hier linken!
it is enough to answer if \emph{terms accurately predicting} such are among the detected semantic directions. This method requires that the dataset is annotated with the respective concept, as that will be the respective classification target. The only easily obtainable target for the Siddata-dataset is a course's faculty, the targets for the datasets of \mainalgos are listed in \autoref{tab:all_datasets}.

\paragraph{Semantic Classifiers} This evaluation method is used extensively in \cite{Derrac2015}, who evaluated the practical usefulness of the created semantic directions by creating many different classifiers which only receive the entity's \gls{rank} \wrt the detected semantic direction as input. In line with the analogy of reasoning in conceptual spaces (see \ref{sec:reasoning}), all their created classifiers correspond to commonsense reasoning patterns and are accordingly linked to intuitive explanations, such as a 1-nearest-neighbor classifier linked to the explanation \textit{\q{Y is in the same class as X because Y is closest to Y}}. Using these, they evaluate if their derived relations are sufficiently accurate to allow for classification that is comparable to standard classification approaches while also allowing to give intuitive explanations that are compatible with high-level human reasoning such as interpolation and a fortiori inference.

\paragraph{Shallow Decision Trees} Instead of a full suite of commonsense reasoning based classifiers, \cite{Ager2018} and \cite{Alshaikh2020} train depth-limited \glspl{dt} \cite{breiman1984classification} on the entities' \glspl{fbr}. \Glspl{dt} are supervised learning models that predict the target value by inferring binary \textit{if-then-else} decision rules from the feature vector in a tree-like structure. The trees are constructed using the CART algorithm, which at every level finds the combination of attribute and threshold value that maximizes the prediction accuracy for the subset that falls into the respective tree's branch and partitions the attribute accordingly, to do the same on resulting subtrees (thus maximizing \emph{information gain}). %https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart 
Restricting the tree's depth ensures that only the features that are most important for the decision are selected. The tree then classifies by finding the most important features for the decision, thus finding the terms that most accurately predict the demanded concept in an intuitive and interpretable way.\footnote{Sample depth-1 decision trees are visualised in \autoref{fig:3dplot_mathe_infoab} The decision-rule of a depth-3 decision tree corresponds to checking if a 3D-vector falls into one of $2^3/2=4$ boxes, as visualised in \autoref{fig:boxes_rechtswis}.} Accordingly, we check if a depth-restricted tree that can accurately predict the demanded property reasonably well can be found. If so, that would provide evidence that the semantic directions correspond to actual human concepts. The performance is measured by checking their classification on a held-out test dataset seperately for each of the target's classes in a one-vs-rest fashion. To get an estimate for the maximally possible classification performance only on basis of the text corpus, it can be compared to state-of-the-art text classification techniques such as the one described in \autoref{sec:faculty_classifier}.

Unfortunately, this does not test if the names of the discovered semantic features are meaningful. It should further be noted that regardless of the classifier, only those features that predict the existing classificiation targets can be tested with this method, which in the case of the Siddata-dataset means that even if 10 features perfectly correspond to the respective faculties, all other features are still unaccounted for. In order to get additional targets, we will also compare the classification into the \gls{ddc}-categories as deteced by SidBERT (see \autoref{sec:sidbert}).

\paragraph{Recovering entities from salient directions} Re-embedding the entities into a conceptual space involves dimensionality reduction and thus necessarily goes along with a loss of information. Apart from testing the meaningfulness of the directions, it can be tested if any relevant information was lost. To account for that, we will test if it is possible to recover the original entities using only a subset of the most salient generated features. For that, we will also analyse under which conditions separate entities fall onto the same position in the semantic space. This also provides evidence about how far the samples are distributed in the vector-space. The linear classifiers used to create semantic directions profit from data that is split far apart instead of being on a compact hypersphere which due to the \textit{curse of dimensionality} fills only a negligible volume of the according vector-space.\footnote{cf. \url{https://en.wikipedia.org/wiki/Curse_of_dimensionality\#Distance_function}}

\subsection{Scientific Qualitative Analysis}
\label{sec:pre_qual_an}

Regardless of the performance of proxy metrics, the most important measure of the algorithm's quality is its subjective appeal, which can only be assessed by a qualitative analysis. Unfortunately, subjectively evaluating at a subset of the resulting features and samples is, even with the best intentions, also prone to  \textit{cherry-picking}, as the selected samples may not be representative. Despite of that, a qualitative analysis provides important insight into the algorithm. To alleviate these problems, it is reasonable to use the scientific method and ask the question: \textit{``Before looking at the data, seeing what kind of results would make us think the algorithm correctly does what we hope it does?''}

Specifically, we will try to find evidence for the following questions:

\begin{description}
	\removeMe{\item[Are the phrases making up the semantic-direction-clusters similar?] \textcite{Derrac2015} have shown that for example in the \textit{scary} dimensions, there were semantically related words such as \textit{horror} and \textit{gore}. Reversing this, we can look if things we know to be similar are actually in the same clusters, such as the names of different epochs hinting at culture-related courses, or names of languages hinting at language-courses. Additionally, we expect languages to co-occur with terms signifying language-levels such as \textit{A1}, \textit{A2} or \textit{B1}. Optimally, there are not too many words outside of this scope in the respective clusters. Furthermore it can be checked if the word-embeddings of words inside a cluster differ from their inter-cluster counterparts. As word-embeddings are known to also capture semantic information \cite{Mikolov:Regularities} as directions, the average cosine-distance of inter-cluster-embeddings is expected to be significantly lower.
	\item[Are there dimensions encoding the COVID-19 pandemic?] Exploratory analysis of the data has shown that many courses from 2020 on indicated their cancellation due to the pandemic. We can check if any of the dimensions captures this, and use the course's semester that is annotated as meta-information in the dataset to cross-check if primarily courses from 2020 on fall under the respective category. Furthermore it can be tested if reversing such a direction reliably points back to a previous iteration of the respective course before the pandemic, if it exists.}
	\item[Are intuitively appealing phrases among the semantic directions?] Given the task of manually embedding courses into a semantic space, there are some intuitive candidates one may think of that capture some important aspects of a course. For example, a word like \emph{"computer"} hinting at computer-science related courses. Other obvious candidates that will be checked include \emph{"mathe", "recht", "musik", "management", "literatur", "sprache", "psychologie", "wirtschaft", "geographie", "schule", "kultur", "wissenschaft", "sport"}.
	\item[Are top ranking courses for the directions convincing?] We can not only classify courses, but also rank them according to \textit{how much} they have a given property. To test if this is a good measure, will be to look at the courses that score highest for those dimensions that score highest for the directions that best encode one of the faculties and see if they are convincing \textit{extreme} examples of the respective faculty or property.
	\removeMe{\item[Is there a direction capturing \textit{advanced courses}?] In their semantic space of movies, \textcite{Derrac2015} detected a direction that encodes \textit{continuation}: The vector between \textit{Back to the Future 2} and \textit{Back to the Future} is roughly parallel to the vector from \textit{Terminator 2} and \textit{Terminator}. Equivalently it can be tested if the vectors between \eg \textit{Mathematik für Anwender 2} and \textit{Mathematik für Anwender} and \textit{Statistik und Datenanalyse 2} and \textit{Statistik und Datenanalyse} are also parallel. An optimal result would be that such courses differ in only a single vector-component.}
	\item[Are embeddings of known similar entities close?] An embedding that would put similar courses far apart from each other would not capture human intuition. Accordingly, the embeddings of exemplary courses such as \textit{Informatik A} and \textit{Informatik B} can be verified. In particular it can be checked if \gencite{Mikolov:Regularities} findings that vector-arithmetic corresponds of embeddings matches semantic content (see \autoref{eq:w2vregularity}) by checking if \textit{vec(Codierungstheorie und Kryptographie) - vec(mathe) + vec(informatik)} $\approx$ \textit{vec(Kryptographische Methoden in der Informatik)}
\end{description}



% \includeMD{pandoc_generated_latex/3_5_evaluation_metrics}
