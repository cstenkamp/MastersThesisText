\chapter{Methods}

%TODO: IST KLAR GENUG: "ich replizier diese 3 algorithmen, aus diesen 3 papern"

This chapter explains the methods that were used to achieve the two goals of replicating the algorihm of \textcite{Derrac2015} with the improvements of \cite{Ager2018,Alshaikh2020} with a sustainable architecture and applying it to the domain of educational resources.  

To do that, we will firstly look at the datasets \mainalgos used as well as our own dataset and compare key statistics. The subsequent section then explains the used algorithm broadly before looking at each of its steps in detail. Based on that, the specific architecture that was created as well as the workflow used to generate meaningful results will be introduced to demonstrate its process. The final section explains what kind of methods and metrics were used to generate and interpret the results that will follow in the next chapter.

Given that there were two research questions, one asking if the replicating algorithm can be applied to another domain and the other asking for a reliable application, it makes sense to both look at datasets and general algorithm on the one side, but also at the worflow and architecture. Writing about the latter while also open-sourcing the code-base is especially useful to ensure ease and speed of future replication, such that all claims can be independently tested with the exact same implementation without having to rely on ambiguous and incomplete verbal explanations\footnote{Just like this thesis could have been a lot less effort if \mainalgos did that instead of ambiguous incomplete descriptions. \todoparagraph{Scable, reproducible, open science! I wished it had been there, so I will do it.}}. 


%JOHANNES' KOMMENTARE TO WHAT I SENT HIM
% * Starker Unterschied zwischen Theoretisch (Dataset,..) und implementation unten in Terms of Verständlichkeit.
% => Implementation extrem verständlich, erster teil (dataset+algo) "teilweise unverstädnlich"
% * Struktur mit den Subsections so sinnvoll. Aber Streamlinen:
% * Verhältnis von Datensatz und Algo wird nicht klar und von den einzelnnen Datensätzen (am anfang sag ich ja es gibt das eine mit den 15.000 und beim zweiten sag ich goar nix dazu)


\section{Datasets}


%TODO: wird klar genug "Die 3 haupt-autoren nutzen halt diesen algo auf diesen datasets, ich will domänentransfer machen, daher macht es sinn zu gucken ob die grundlegend unterschiedlich sind"
% -> damit auch main-claim (Kommentar Johannes, es war sehr unklar welche datasets mich inwieweit interessieren und warum ich die beschreib die ich beschreib)
% * Verhältnis von Datensatz und Algorithmus wird nicht klar
% * Warum ich die einzelnen Datensätze erwähne wird nicht klar


Let us first elaborate on the datasets used both in the works that are being replicated as well as the dataset used in the scope of this thesis. Doing both is important because (as explained in \autoref{sec:howtoreplicate}), to really know if an algorithm is a good choice for a task, it must be ensured that it does not only work on a special kind of dataset due to a special property that this kind of data happens to have. So, we will now compare all datasets, to see if there are important differences between them, as for example in its structure, size or just general \emph{logic}.

\textcite{Derrac2015} used their algorithm to create conceptual spaces for three domains: \textit{movies}, \textit{placetypes} and \textit{wines}. Accordingly, they created corpora for these three domains. The two considered follow-up works both re-used the datasets for movies and placetypes and also created additional datasets respectively. An overview of all the corpora used in \mainalgos is provided in \autoref{tab:all_datasets}.

% NOW at first the ones from \mainalgos I am using

\subsection*{Comparing dataset properties}

This work applies the replicated algorithm to a new dataset, so it is important to check if the new dataset differs from the originally used ones. To ensure comparability in algorithm details, this implementation is also run on the \emph{placetypes}-dataset, and some of its statistics as well as that of the other two datasets made public by \cite{Derrac2015}\footnote{\url{https://www.cs.cf.ac.uk/semanticspaces/}}. Let us first compare some key statistics of them, before looking into two of them individually in the next section.

A very distinctive difference of the datasets is their size: the movies-dataset consists of 15.000 entities, place-types of 1383, and wines of only 330. Just as important as the number of entities is however also the length of their associated text-corpus. Figures \ref{fig:sid_wordsperdesc} and \ref{fig:placetypes_dist_unique} show a histogram of the distribution of text lengths for the Siddata-dataset and placetypes-dataset respectively, and \autoref{tab:corpussizes} summarize corpus size and distribution of text lengths per entity for all handled datasets. Note that the Siddata-dataset is listed twice, once including all available entities and once after filtering out those whose description was shorter than 80 words.

\begin{table}[H]
	\centering
	\begin{tabular}{r|l|lll|lll}
		&  \textbf{Entities}   & \multicolumn{6}{c}{\textbf{Words per Entity}}                                 \\
		&       & \multicolumn{3}{c}{\textbf{Unique}} & \multicolumn{3}{c}{\textbf{Non-Unique}} \\
	 &  & \textbf{5\textsuperscript{th}} & \textbf{Med} & \textbf{95\textsuperscript{th}} & \textbf{5\textsuperscript{th}} & \textbf{Med} & \textbf{95\textsuperscript{th}} \\ \midrule
	\textbf{movie reviews}                                                          & 13978 & 565       & 1358       & 5510       & 962        & 3179        & 38378        \\
	\textbf{place types}                                                          & 1383  & 159       & 2215       & 18117      & 2378       & 55422       & 886233       \\
	\textbf{Siddata (all)}     & 26346 & 19        & 60         & 169        & 22         & 71          & 239          \\
	\textbf{Siddata} ($\geq 80$ words) & 11601 & 63        & 99         & 211        & 83         & 124         & 323         
	\end{tabular}
	\slcaption{Sizes of the text-corpora of the Siddata-dataset and those of \cite{Derrac2015}. The columns \textit{Words per Entity} list the 5\textsuperscript{th}, 50\textsuperscript{th}, and 95\textsuperscript{th} percentile respectively.}
	\label{tab:corpussizes}
\end{table}

\begin{table}[h]
	\centering
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{rcccccccccc}
	\multicolumn{1}{l}{} &
	  \textbf{1} & \textbf{2} & \textbf{5} &  \textbf{25} &
	  \textbf{50} & \textbf{100} & \textbf{500} & \textbf{1000} &
	  \textbf{\acrshort{df}/10} & \textbf{\acrshort{df}/4} \\ \midrule
	\textbf{Siddata} & 163285 & 87840  & 40998  & 11619  & 6649   & 3722   & 765   & 317   & 60     & 8     \\
	\textbf{Places}  & 746180 & 746180 & 746180 & 313858 & 202713 & 126899 & 41108 & 24735 & 101867 & 53588 \\
	\textbf{Movies}  & 589727 & 392885 & 200470 & 83354  & 60952  & 44809  & 20314 & 13558 & 5317   & 2484 
	\end{tabular}
	}
	\slcaption{Words that exceed various term-frequency thresholds for the  Siddata, Places and Movies datasets. In \cite{Derrac2015}, the candidate-threshold for placetypes was 50, and the threshold for movies 100. Note that the last two columns are relative and depend on the dataset-size.}
	\label{tab:summed_unique_words}
\end{table}

It is obvious that even though the number of Siddata-entities is comparable to the bigger of \cite{Derrac2015}'s datasets, the number of unique words in the individual texts associated with the entities are two orders of magnitude shorter than both. As the algorithm does not rely on deep learning, this does not mean that the dataset is unsuited for it, but let us explore how the distance in their lengths can be explained and what consequences this has.

The works considered in this replication \mainalgos all use the movies- and the  placetypes-dataset to evaluate their methods. The former consists of the concatenation of all available reviews for 15.000 movies from IMDB\footnote{\url{https://www.imdb.com/}}, grouped by the movie the reviews are for. The placetypes-dataset is created from a collection of tags that belong to photos uploaded to the photo sharing platform Flickr\footnote{\url{https://www.flickr.com}} that co-occur with other tags that denote one of several placetypes. Other datasets considered in the works of \mainalgos include wine reviews, posts to certain newsgroups, and another dataset created from IMDB-reviews (see \tref{tab:all_datasets}). 
% "their algorithm is tailored to concatenated-reviews or concatenated-bags-of-tags"

All of these datasets have in common that they are made up from a collection of independent texts or tags, created by different people. This means, that the more obvious or distinct a property of the respective entity is, the more often words describing that property will be used as tag or as part of the review. For example, a movie that is \emph{scary} to a lot of people will lead to many reviews mentioning that, which means that the word scary (or other words commonly co-occuring with it) will have a high count in the concatenation of all these reviews. The algorithm from \cite{Derrac2015} heavily leans on this property by using the (relative) frequency of certain words as signal for the importance of the concept they may refer to. 

The Siddata-dataset unfortunately does not share this property, as the texts that belong to an entity are not collected from different independent texts, but solely from the description of that entity. It may of course be roughly the case that the more \emph{mathematical} a course is, the more often the word \emph{math} occurs in its description, but the correlation is likely not as prominent as in the aforementioned datasets. It should however be noted that \cite{Alshaikh2020} also used the algorithm on three datasets where the description for an entity was collected from its Wikipedia\footnote{\url{https://en.wikipedia.org/}}-article, which also does not contain the described duplication of words. \autoref{tab:all_datasets} shows a complete comparison of all datasets used in any of \mainalgos, including their origin, properties as well as associated pre-processing steps.

\input{include/dataset_table}

\subsection{SIDDATA-courses}

\label{sec:dataset_siddata}

% Generelle Analyse Datensatz mit Plots (Längenverteilung der Descriptions, ...)
% ich würde SIDDATA immer in caps schreiben wenn du das Projekt meinst, wenn du die software meinst kannst du die ja auch klein introducen bzw die auch mit digital study assistant (DSA) abkürzen

\begin{table}[H]
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{lllllrrrr}
	\toprule
	 &  &  &  &  & \textbf{$\geq$20} & \textbf{$\geq$50} & \textbf{$\geq$200} & \textbf{$\geq$500} \\
	\textbf{Source} & \textbf{Type} & \textbf{Format} & \textbf{Uni} & \textbf{Lang.} &  &  &  &  \\
	\midrule
	\multirow[t]{3}{*}{\textbf{2021 Dump}} & \multirow[t]{3}{*}{\textbf{Stud.IP}} & \multirow[t]{3}{*}{\textbf{Course}} & \multirow[t]{3}{*}{\textbf{\acrshort{uos}}} & \textbf{de} & 18203 & 14551 & 2671 & 100 \\
	 &  &  &  & \textbf{en} & 1950 & 1773 & 616 & 53 \\
	 &  &  &  & \textbf{other} & 681 & 612 & 100 & 1 \\
	\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5}
	\multirow[t]{6}{*}{\textbf{2022 Dump}} & \multirow[t]{6}{*}{\textbf{Stud.IP}} & \multirow[t]{6}{*}{\textbf{Course}} & \multirow[t]{3}{*}{\textbf{\acrshort{uos}}} & \textbf{de} & 1009 & 807 & 188 & 5 \\
	 &  &  &  & \textbf{en} & 132 & 123 & 70 & 13 \\
	 &  &  &  & \textbf{other} & 20 & 14 & 1 & - \\
	\cline{4-5}
	 &  &  & \multirow[t]{3}{*}{\textbf{other}} & \textbf{de} & 5246 & 4353 & 1246 & 53 \\
	 &  &  &  & \textbf{en} & 771 & 622 & 260 & 25 \\
	 &  &  &  & \textbf{other} & 63 & 46 & 9 & 2 \\
	\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5}
	\multirow[t]{15}{*}{\specialcell[t]{\textbf{Educational}\\ \textbf{Resources}}} & \multirow[t]{4}{*}{\textbf{OER}} & \multirow[t]{2}{*}{\textbf{PDF}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{de} & 51 & 22 & 2 & - \\
	 &  &  &  & \textbf{en} & 1 & - & - & - \\
	\cline{3-5} \cline{4-5}
	 &  & \multirow[t]{2}{*}{\textbf{unknown}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{de} & 125 & 50 & 6 & - \\
	 &  &  &  & \textbf{en} & 7 & 4 & - & - \\
	\cline{2-5} \cline{3-5} \cline{4-5}
	 & \multirow[t]{6}{*}{\textbf{Stud.IP}} & \multirow[t]{6}{*}{\textbf{Course}} & \multirow[t]{3}{*}{\textbf{\acrshort{uos}}} & \textbf{de} & 993 & 684 & 121 & 5 \\
	 &  &  &  & \textbf{en} & 236 & 163 & 53 & 2 \\
	 &  &  &  & \textbf{other} & 168 & 130 & 91 & 25 \\
	\cline{4-5}
	 &  &  & \multirow[t]{3}{*}{\textbf{other}} & \textbf{de} & 737 & 514 & 145 & 23 \\
	 &  &  &  & \textbf{en} & 303 & 258 & 89 & 23 \\
	 &  &  &  & \textbf{other} & 33 & 23 & 3 & - \\
	\cline{2-5} \cline{3-5} \cline{4-5}
	 & \multirow[t]{3}{*}{\specialcell[t]{\textbf{Udemy-}\\ \textbf{\acrshort{mooc}}}} & \multirow[t]{2}{*}{\textbf{Course}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{en} & 22 & - & - & - \\
	 &  &  &  & \textbf{other} & 3 & - & - & - \\
	\cline{3-5} \cline{4-5}
	 &  & \textbf{\acrshort{mooc}} & \textbf{other} & \textbf{en} & 8 & - & - & - \\
	\cline{2-5} \cline{3-5} \cline{4-5}
	 & \multirow[t]{2}{*}{\textbf{web}} & \multirow[t]{2}{*}{\textbf{unknown}} & \multirow[t]{2}{*}{\textbf{other}} & \textbf{de} & 48 & 3 & - & - \\
	 &  &  &  & \textbf{en} & 1 & - & - & - \\
	\cline{5-9}
	 &  &  &  & \textbf{sum} & 30811 & 24752 & 5671 & 330 \\
	\bottomrule
	\end{tabular}
	\caption[Metadata of the Siddata-Dataset]{Metadata of the Siddata-Dataset. Languages are reported as detected (see \ref{ap:translating}), other metadata as it was available in the dumps. The individual columns is the number of entities whose description has at least 20, 50, 200 or 500 words. \label{tab:siddata_metadata}}
	}
\end{table}

\begin{figure}[H]
	\includegraphics[width=1.15\textwidth,center]{graphics/dataset_new/statistics_bars.pdf}
	\caption{Distribution of metadata in the raw Siddata-dataset. Languages are reported as detected (see \ref{ap:translating}), other metadata as it was available in the dumps.}
	\label{fig:sid_statistics}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{graphics/dataset_new/words_per_desc.pdf}
	\caption{Distribution of description-lengths of the Siddata-dataset. The rightmost bar represents the longest 2\% of descriptions.}
	\label{fig:sid_wordsperdesc}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{graphics/dataset_new/courses_per_faculty.pdf}
	\caption{Distribution of Faculty per Course for those Courses at University of Osnabrück.}
	\label{fig:courses_per_faculty}
\end{figure}


\includeMD{pandoc_generated_latex/3_0_1_datasets_siddata}

\todoparagraph{Ref auf die Jupyter-plots, wie der sunburst-plot!}


\subsection{Place-Types}
\label{sec:dataset_placetypes}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{graphics/figures/placetypes_dist_unique.pdf}
	\caption{Distribution of unique words per entity for the place-types dataset.}
	\label{fig:placetypes_dist_unique}
\end{figure}




Because the Siddata-dataset differs in many regards from the datasets used in \mainalgos, %AS stated above (in siddta-section, in the first part of datasecs-section, in workflow-section, ...)
it does make sense to compare the results of the given pipeline on that dataset to the results of \mainalgos. To be able to compare the results achieved here with theirs, one of the datasets used by all of their papers was considered as well. Comparing on a original dataset also has the benefit of allowing to sanity-check if the implenetation is correct. If the performance achieved on this dataset is comparable to the performances of \mainalgos whereas the peformance for the Siddata-dataset is considerably worse, there is strong indication that the quality or quantity of the dataset is insufficient for acheiving high-quality representations. Converesely, if our algorithm applied to the dataset used in \mainalgos produces results that fall considerably short of those reported in literature, it is likely that there is a fault in our implementation.

There are two datasets used by all three authors: place-types and movie-reviews (see \autoref{tab:all_datasets}). Both of the datasets are available in preprocessed form\footnote{\url{https://www.cs.cf.ac.uk/semanticspaces/}} \cite{Derrac2015}. It was originally planned to use both of the datasets as basis for comparison, however unfortunately it is impossible to recover the form of it as originally used by \textcite{Derrac2015}: The original texts of the reviews are not available, but only their respective bag-of-1-grams (\glsentrylong{bow}) - even though the authors explicitly state that they worked with variable-length-n-grams. Even though the provided list of \glspl{cand} contains \glspl{ngram}, it is impossible to recover which of the \glspl{entity} contained it. While the algorithm can still be run only for the 1-grams, the results are not comparable with the original ones anymore\footnote{As all of the \mainalgos share an author, it is assumed that \cite{Alshaikh2020,Ager2018} had access to the full version of the dataset and did not share the stated problem.}.

\includeMD{pandoc_generated_latex/3_0_2_datasets_placetypes}

\subsection{Other Datasets}


\includeMD{pandoc_generated_latex/3_0_3_datasets_other}

\section{Algorithm}


\input{Chapters/sections/methods_algorithms}


\section{Architecture}
\label{sec:architecture}

As elaborated in \autoref{sec:reproducibility}, one of the main motivations for this thesis was to create a publicly available \textit{open-source} version of the algorithm that is easily \textit{understood} and \textit{reproduced}, \textit{adaptable} for other datasets and methods, as well as fast and \textit{scalable}, meaning it can be run maximally efficient on single machines but also on compute clusters, such as the \acrshort{ikw} Grid.
%TODO: Hier schon eindeutig sagen dass es auf ner single machine infeasibly lange läuft und deswegen der ganze Bums fürs Grid nötig war!!

% Main goal: BETTER ARCHITECTURE. Most important things for that: scalability, modularity, transparency, reproducibility, understandability, objectiveness, systematicacy, sustainability, adaptability
% describing this because I want to encourage extending the code etc and for that not only the algorithm but also the architecture should be described 
% and I think that was successful: This codebase contains everything and finally fulfills code-standards! 

This section will outline the architecture that was developed in order to achieve the aforementioned results. The resulting pipeline is the result of a lot of trial-end-error, but fulfills all of the aformentioned criteria, dealing with vastly differing sizes and kinds of datasets, minimizing runtime wherever feasible and allowing for a multitude of parameters at every step of the process. %TODO: don't like this paragraph, lieber später nohcmal auf die design principles eingehen und sagen dass sie alle fulfilled sind.

The rest of this section will go into further detail regarding the architecture of the resulting code-base. \todoparagraph{it will start with xyz and then asdf and then yaddayadda}

\subsection{Implementation}

The associated program is written by the author of this work and licensed under the \emph{GNU General Public License} (GNU GPLv3). The source code is written in the Python Programming Language and available digitally on GitHub\footnote{Source code: \url{https://github.com/cstenkamp/derive_conceptualspaces/}\\Source of this Document: \url{https://github.com/cstenkamp/MastersThesisText/}\\Compiled Document: \url{https://nightly.link/cstenkamp/MastersThesisText/workflows/create_pdf_artifact/master/Thesis.zip}}. In order to ensure that no work after the deadline is considered, it is referred to the signed commits \todoparagraph{COMMIT} and \todoparagraph{COMMIT}. 

The code is a proper python-package that can be installed into any Python 3.10 environment using for example python's default package manager pip:\\ \mytokens{pip install git+https://github.com/cstenkamp/derive_conceptualspaces.git@main}~ .\\ It can then be run using \mytokens{python -m derive_conceptualspace <COMMAND>} \footnote{The command \mytokensfnote{python -m derive_conceptualspace --help} gives a peak into what sub-commands can be used}. For more information on how to invoke the code base with these commands it is referred to \autoref{ap:usecase_click}

To guarantee reusability of this code-base, there is also a \emph{Dockerfile}\footnote{{\url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/Dockerfile}}} that allows to easily create a \emph{Docker-Container\footnote{\url{https://www.docker.com/resources/what}}} from it\footnote{A Container can be thought of as a lightweight virtual operating system, in which the codebase is bundled together with all required dependencies, libraries and configurations, enabling users install this software on any system without having to download or install anything besides this container, irrespective of operating system or software versions on the host \acrshort{os}. For more info about the container, it is referred to \url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/doc/docker_intro.md}.}.

\subsection{Modularity}

The developed algorithm consists of clearly divisible components (as demonstrated in \autoref{fig:dependency_graph}), where the runtime for each of the steps is roughly in the same order of magnitude. All of the aforementioned (\autoref{sec:algorithm_steps}) steps are itself algorithms with many \gls{param} each. Furthermore, the framework described here does not even require particular algorithms for the individual components, but rather a classes of algorithms like \emph{dimensionality reduction techniques}. This means that in practice, there is a combinatorical explosion of settings and \glspl{param} that must be experimented with in order to find the best-performing one. Because of the clear modularity of the algorithm however, many of these become only relevant in a later step of the pipeline. Due to this, it is reasonable to make the architecture as modular as possible, storing interim results before every step, such that two parameter-combinations that differ only in \eg the fourth step of the pipeline can share the intermediate results up to that point, keeping the required computation to a minimum. 

The design principle of maximal modularity is the cornerstone of the developed pipeline. All of the interim results store the configurations that were required for the respective algorithm (and forward the ones of the input-files they transformed), as well as the created output and plots. When there are different possible algorithms for a step, it is ensured that its result are of the same format, as required by the next step. Many of the individual steps generate additional plots that can be used as sanity-checks to quickly inspect if the results so far are reasonable.

\subsubsection{Workflow Management}

A pipeline where multiple intermediate files for different parameter-combinations are created introduces the problem of \emph{dependency resolution}: Ultimately, there is supposed to be one final file for every combination. This file however relies on intermediate files, which in turn rely on intermediate files. To resolve these dependencies, there are many existing \textbf{Workflow Management Systems}. For this thesis, \textbf{Snakemake}\footnote{\url{https://snakemake.readthedocs.io/en/stable/}} \cite{Molder2021a} seemed the right choice.

Snakemake defines a small comprehensible domain-specific language ontop of python. With this, a workflow is described in terms of individual \textbf{rules}, each of which defining how an \textbf{output} is generated from several \textbf{inputs} using code or shell-commands. Through \textbf{wildcards}, these rules can be generalized and hyperparameters introduced \cite{Molder2021a}. The job of Snakemake is to infer a \gls{dag} from these, finding for every rule in the dependency tree for the demanded file an output that generates the required inputs, and to create jobs for all required instanciations of the wildcards if the required files are not already present. Importantly, Snakemake then also handles the inevitable scheduling problem: Due to (explicitly specified) restrictions of \acrshort{cpu} and \acrshort{ram} and the nature of the unresolved dependencies, not all jobs of the workflow can be executed simultaneously. Its scheduler favors maximal utilization of \acrshort{cpu} and parallelisation for minimal execution time \cite{Molder2021a}. Especially relevant was also that it allows to schedule these jobs on high performance clusters and computation grids, and supports among others the scheduling system \gls{sge} which is used to orchestrate jobs at the \gls{ikw} grid. Configurations for the grid, like the maximal runtime or the amount of \gls{ram} and \glspl{cpu} to request, can be specified per-rule as well as in special configuration files.
%TODO: gibt noch 1-2 buzzwords from paper, ich kann schonmal aufs Grid hinaus und dann halt der wann-ist-snakemake-sinnvoll-und-wann-nicht.

Snakemake was chosen because it is a lightweight system ontop of python, adding only a few lines of code to specify what inputs and outputs are created ontop of the \gls{cli} that is necessary to run and debug individual steps anyway. It is a useful tool if the workflow can be divided into rougly equally long steps which can run independently and heavily parallelized (possibly on multiple machines) with an optimal usage of resources. Its file-centric dependency resolution system allows to fill in missing steps seemlessly when working on specific configurations for later step, but on the other hand requires unintuitive customization if instead configuration-files with explicit parameter-choices declare the demanded output for dynamically generated filenames. Also it unfortunately doesn't allow debugging and has a comparably small community\footnote{As of \DTMdisplaydate{2022}{03}{16}{-1}, there are only 1256 question tagged ``snakemake'' on StackOverflow (\url{https://stackoverflow.com/questions/tagged/snakemake})}. \autoref{ap:usecase_snakemake} shows the different ways the full pipeline can be invoked using Snakemake.

%wenn viele parameter die an gwissen punkten relevant werden und später nicht mehr, wenn viele param-kombis, it's main thing is the automatic dependency resolvement (which means I can just tell it "hey I need this file" (automatically creating missing stuff), but with config-files you're abusing it. good for optimal CPU/RAM usage. Good if independent parallelzed steps, not if one main step. Have to abuse it for configs, no good way to debug, small comunities, nondynamic I need nondynamic filenames that are set from the start of the execution 


\subsection{Modes of Execution / Use-Cases}

It is possible to run the full pipeline for individual files as well as for a set of \gls{param}-configurations specified via configuration files, but also possible to run individual steps to inspect or debug the respective steps. To inspect and compare results it is possible to load all available parameter-configurations, as well as the complete history for a certain combination, listing the generated outputs and metrics. Further, individual configurations can be loaded in \emph{Jupyter Notebooks} to generate and export plots and tables from them (like the ones used in this text). The three main ways of exectution are:
%TODO: deutlicher drauf eingehen dass man wegen dem ganzen bums mit intermediate files undso speziell drauf achten muss dass 
% * keine plots/prints verloren gehen
% * man mitschreibt wann welche configs genutzt werden
% * immer eindeutig drauf geachtet wird dass dependencies für genau die konfigurationen as demanded verwendet werden!! 

\begin{description}[style=unboxed]
	\item[Running individual Steps per \gls{cli}] is the mode of choice when working on custom steps, as it allows to attach debuggers and executes in the main thread. If a later step is executed, it is also possible to automatically generated its required dependencies using the workflow-definition. Passing configurations is possible using configuration-files, command-line-arguments or enviroment-files/-variables. For usage-examples, see \autoref{ap:usecase_click}.
	\item[Loading existing Configurations for inspection] especially in Notebooks, allowing to easily load a complete configuration including all its dependencies to inspect and plot (intermediate) previously created results and outputs, also allowing to iterate over several configurations to compare their results\footnote{The tables used in thesis are also automatically exported as \LaTeX- code from the functions available there, as specified in their respetive references.}. For usage-examples, see \autoref{ap:usecase_notebook}.
	\item[Running/Scheduling a Workflow] This mode is used to execute several \gls{param}-combinations at once, specified via configuration-files. Thanks to heavy integration for cluster scheduling systems, this allows for heavily parallelisation of jobs. Executing such a workflow on computation clusters is special case of this and elaborated further in the following section. For usage-examples, see \autoref{ap:usecase_snakemake}.
\end{description}
% 3 ways: Snakemake for shitton of param-combinations, invididual steps via the CLI for looking, debugging, creating, and 
% context-loading for jupyter to inspect and plot results - allowing load-context, where you can call eg. `print(ctx.display_output("embedding"))` of every component, read in several configs, iterate over them, re-create plots, allow for show-data-info showing where plots are first used, ...


\subsubsection*{Running on the \gls{sge}}

Due to a combinatorical explosion in the \gls{param}-space as well as the computational complexity of the algorithm, running the pipeline a sufficient amount of parameter-combinations would take several weeks on a single machine\footnote{\todoparagraph{Give a few examples!}}. As the \gls{ikw} at the \gls{uos} owns a dedicated computation grid\footnote{\url{https://doc.ikw.uni-osnabrueck.de/content/grid-computing}} with considerable modern hardware\footnote{Currently comprising, among many others, of 26 machines with an i7-11700 \gls{cpu} and 64 GB \gls{ram}} which uses the \gls{sge} as workload manager, which is supported by snakemake, it was the obvious candidate. Snakemake encodes special configurations for clusters using \emph{profiles}\footnote{\url{https://snakemake.readthedocs.io/en/stable/executing/cluster.html}}, and there exists a profile for the Sun Grid engine\footnote{\url{https://github.com/Snakemake-Profiles/sge}}. Unfortunately, this default configuration does not take into account many of the pecularities of the \gls{ikw} grid and it needed to be heavily customized in order to work. Foremost, all available machines to \me have a runtime-limit of 90 minutes, which means all of the algorithm-steps that take longer than that must be able to be interrupted and gracefully shut down before getting killed and pick up the work on a new machine afterwards (including the job responsible for the workflow scheduling itself). Additionally, the arguments to request resources (such as \emph{memory} or \emph{parallel environments}) often differ from the documentation, and the \emph{accounting file} which keeps track if jobs succeeded is not available to users, so a custom one must be written. Resolving these and other issues required changing the available profile heavily, so the result was open-sourced\footnote{The resuling Snakemake-Profile is available and documented at \url{https://github.com/cstenkamp/Snakemake-IKW-SGE-Profile}. Note that it is heavily customized to the specific engine and thus includes explicit machine names or runtimes. This repository also contains convenience-terminal-commands to inspect failed pipeline-steps or to show the current progress of the current run. A sample output of the latter is presented in \autoref{lst:joblog}. Furthermore it contains \mytokensfnote{.sge}-files and shell-scripts to schedule or run a requested workflow (see \autoref{ap:usecase_snakemake})}. 


Scheduling on such engines interestingly unveils a whole new set of ``hyperparameters'' that have to be optimized to use the available hardware as efficiently as possible: there are limits of how many slots are available per user, there is a fixed walltime (and interrupting and restarting leads to overhead), and the effiency of multiprocessing is not linear in the number of threads per process. Thus, depending on the size of the dataset, resources must be divided among the steps with care. The required resources of the rules are accordingly dynamically allocated in the rule-descriptions of the workflow manager.

While the code required to scalably run on the \gls{ikw}-grid required much more work than expected, the result fulfills all demands perfectly, %TODO: WHAT demands
and the 64 allocated \emph{parallel environments} (slots) are maximally utilized, while most of the complexity of the scheduling system is abstracted away\footnote{To the best of \my knowledge, no attempts going beyond simple \mytokensfnote{.sge}-files as job-descriptions were attempted on the IKW-grid before, and much of the available documentation turned out to be false information (as consultations with the grid's administrator have shown).}. The workflow is installed and run with a single (well documented) command and can be customized using explicit configuration-files. A sample output of the custom-made watcher is listed in \autoref{lst:joblog}.  


\begin{widepage}
	\lstconsolestyle
	\lstinputlisting[
		caption={[Sample terminal output of the custom watcher]Sample terminal output of the custom watcher when running a full configuration on the \gls{ikw}-grid. The script lists the currently running jobs continously, including their progress and runtime and informs of finished jobs and failed jobs. There is another script that summarizes the progress as per snakemake's dependency-graph.}, 
		label={lst:joblog},
		float,
		floatplacement=h!,
		xleftmargin=-0.5cm, 
		xrightmargin=-0.5cm,
		]{listings/joblog\_grid.txt}
	\lstdefaultstyle
\end{widepage}

\subsection{Conclusion}

It was originally unexpected, but implementing an appropriate architecture for the present codebase has been a major focus of work for this thesis, and the result fulfills all of the desired design criteria: 

% reproducibility alone is not enough to sustain the hours of work that scientists invest in crafting data analyses. Here, we outlined how the interplay of automation, scalabil- ity, portability, readability, traceability, and documentation can help to reach beyond reproducibility, making data analyses adaptable and transparent.

\begin{description}[style=unboxed]
	\item[Modularity] has been the main focus in the design, so exchanging components or running individual steps is easy and intuitive.
	\item[Scalability] is reached thanks to massive parallelisation wherever possible as well as a professional workflow management system that is perfectly adjusted to the available cluster engine but also highly customizable for other engines.
	\item[Reproducibility and Adaptability] are guaranteed by stringent encapsulation of components, completely automating the full data-analysis-pipeline, open-sourcing the code as proper package and containerization of the entire codebase for guaranteed and worry-free setup on any machine or compute cluster. The exact \gls{param}-combinations of \mainalgos are included (see \autoref{ap:yamls_for_origalgos}), allowing to re-create even the original papers using this code-base. Running the code on new datasets is extensively documented\footnote{\todoparagraph{TODO:} link to that} and a matter of minutes. Extending or exchanging steps of the pipeline is seamless due to a consistent and understandable data schema, and pre-existing analysis-notebooks can easily create informative plots and figures.
	\item[Transparency and Understandability] are ensured due to rigorous documentation\footnote{\todoparagraph{TODO:} Link github-documentation!} (among others in this thesis) at any level of detail, from rough descriptions to concrete code-examples. Code, documentation and used data are publicly and easily available. Many analyses are included with the source-codes, for example allowing to visualize all steps of the process that can work with arbitrary numbers of dimension interactively in 3D. Code, data and configurations are clearly divided. All steps of the pipeline are very explicit about the used configurations and dependencies (making them traceable) and generate output at configurable levels of verbosity. All intermediate output can be re-accessed using helper commands (see \footnote{\todoparagraph{TODO:} ref the appendix with the show-info-command and a notebook with \mytokensfnote{create_svm("mathematik", embedding, dcm, pp_descriptions, highlight=["Informatik A: Algorithmen", "Informatik B: Grundlagen der Software-Entwicklung"])}}), including clear traces of the first usage of parameters (as \eg in a plot as depicted in \autoref{fig:dependency_graph})
	% it is crucial that the analysis code is as readable as possible such that it can be easily modified (looking at you, 40 unnamed cmd-args!)
	% code is readable and well-documented 
	% mit 2 Zeilen code kannst du dir in nem Jupyternotebook nen 3D-Plot anzeigen mit ner SVM die "Mathematik" von nicht-mathe trennt, mit gehighlighted ob "Informatik A" und "Informatik B" beeinander sind
\end{description}

\includeMD{pandoc_generated_latex/3_3_architecture}


\section{Workflow}
\label{sec:workflow}


\includeMD{pandoc_generated_latex/3_4_workflow}



\section{Evaluation Metrics}
\label{sec:eval_metrics}


\includeMD{pandoc_generated_latex/3_5_evaluation_metrics}
