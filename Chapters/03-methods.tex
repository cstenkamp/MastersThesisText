\chapter{Methods}

Direkt am Anfang schreiben dass ich halt auf den main algorithmus eingehe und das laut meiner research diese 3 paper am besten den main algo beschreiben (bzw sinnvoll erweitern) - was nicht heißt dass das die einzigen in dem kontext sind, Alshaikh2019 bspw nutzen ja den main algorithmus, aber ja nur als komponente, und haben andere Ziele was sie dann damit machen

Im folgenden gibt es neben Datasets 2 main sections: algoritm and architecture. Dass Algorithm und Architecture 2 subsection von methods sind ist halt "Der allgemeine Algorithmus und die spezifische Anwendung" Warum Architecture section? es kostet extrem viel zeit die schwammigen formulierungen in den papern genau zu verstehen, man probiert super oft falsche parameter-kombinationen aus etc etc, es ist halt ein riesiger ewig langer lernprozess den man von vorne machen müsste wenn man es nachimplementieren möchte, ich hätte mir gewünscht die authors hätten darüber mehr worte verloren, and also the scalable reproducible open-science part. And also - it took me a shitton of time, way more than working on the algorithm (but NOW it can run so easily on the grid and all param-combis simultaneously, ..), so this is what you'll get.


\section{Datasets}

All considered algorithms \mainalgos use a dataset of 15.000 movies and their reviews on IMDB, as well as a placetypes-dataset to evaluate their methods. The former consists of the concatenation of all available reviews for movies from IMDB\footnote{\url{https://www.imdb.com/}}, whereas the latter a collection of tags from photos uploaded to Flickr\footnote{\url{https://www.flickr.com}} that co-occur with a certain placetype. Other considered datasets are wine reviews, posts to certain newsgroups and another IMDB-review-dataset (see \tref{tab:all_datasets}). 

All of these datasets have in common that they are made up from a collection of independent texts or tags, created by different people. This means, that the more obvious or distinct a property of the respective entity is, the more often words describing that property will be used as tag or as part of the review. For example, a movie that is \emph{scary} to a lot of people will lead to many reviews mentioning that, which means that the word scary (or other words commonly co-occuring with it) will have a high count in the concatenation of that review. The algorithm from \cite{Derrac2015} heavily leans on this property by using the (relative) frequency of certain words as signal for the importance of the concept they may refer to. 

The main considered dataset of this thesis unfortunately does not share this property, as the texts that belong to an entity are not collected from different independent texts, but solely from the description of that entity - while it may be the case that the more \emph{mathematical} a course is, the more often the word \emph{math} occurs in it description, but the correlation is likely not as prominent as in the aforementioned datasets. Interestingly, \cite{Alshaikh2020} also used three datasets that only use a sort of description for an entity: its Wikipedia\footnote{\url{https://en.wikipedia.org/}}-article.


\subsection{SIDDATA-courses}

% TODO: bei dataset section darauf verweisen dass große teile des siddata-datasets mit gtranslate übersetzt wurden und auf den entsprechenden anhang verweiseen

% * Steht ja schon woanders dass mein Datensatz anders ist als concatenated-movie-reviews und ich deswegen nicht einfach "je öfter 'scary' desco scarier" machen kann. Da gibt's several ways mit umzugehen
	% * Das sich-die-richtigen-wörter-per-candidate-svm-bootstrappen
	% * Mit LSI rausfinden welche Terme genausogut in dem Text hätten vorkommen können (hab ich auch irgendwo schon)
	% * Explizit einfach zu gucken "Welche Terme kommen oft in den gleichen dokumenten vor" (und das inverse (steht iwo im code)), und dann ne candidate SVM für grouped terms anstelle von einzelterms machen (auch schon iwo als code)
	% * Mit Wordnet hypernyms/hyponyns und synonyms zu finden damit ebenfalls zu arbeiten (kann man wit wordnet angeben welches abstraktionsniveau ich haben will?)
	%     * Abstraktionsniveau gibt's nicht in wordnet, das heißt das richtige layer zu finden ist schwer. Was man auf jeden Fall machen kann ist die Terme zu den bases ihrer synsets umzuwandeln (dadurch wird aus "math" und "mathematics" das gleiche), aber in anderen Fällen ist es halt so dass ich die Candidate-Terms schon vorher brauche und nur sagen kann "diese entity enhält X wörter die halt hyponyms von dem Term sind"

their algorithm is tailored to concatenated-reviews or concatenated-bags-of-tags. Take their success-metric for the SVMs splitting the embedding. The more often the word "scary" comes in the concatenated reviews, the more scary the movie is. Sounds legit. The more often the people that took pictures at a particular place mentioned the "nature" of that, the more relevant "nature" is to that place. Also legit. But in the descriptions for courses that involve a lot of mathematics, it is not necessarily the case that the term "mathematics" occurs often. So due to the different nature of my dataset I have to go beyond their algorithm at some points - in this case it is probably the case that different kinds of mathematical terms actually do occur more often, so I'd need calculate these kinds of kappas not based oon a single term but ALREADY on a cluster of terms (... and I can bootstrap my way there, because after I do this I get more words to add to my cluster, rinse and repeat!)


% * Dass man theoretisch sich den task einfacher machen kann indem man nur die correctly-classified Kurse des fb-classifiers verwendet
% * MEINEN DATENSATZ mal mit den anderen vergleichen!! 
% 	* die Plots die schon drin sind beschreiben und warum der Datensatz whack ist.
% * Den ganzen "wo ist mein dataset anders als deren" Kram (teilweise schon im text, teilweise very old)
% * Woher der Datensatz kommt, dass es ja version 2 des Kurs-Datensatzes von Johannes ist (kommt von: /home/chris/Documents/UNI_neu/Masterarbeit/OTHER/study_behavior_analysis/src/data/course_data/db_dump_new/course_dump_new.csv)
% * Meine Pre-Preprocessing Schritte die da ja auch noch gut rumfiltern und rummergen beschreiben
% * Candidate-Word-Threshold: movies has samples-to-threshold value of 100, placetypes has 35, 20newsgrups has 614, so for 8000 courses any threshold from 2 to 25 seems reasonable => \cite{Derrac2015} say they intentionally kept the number of candidateterms approximate equal (at around 22.000), so to do the same I'd need a threshold of [TODO: optimal value]
% * [AGKR18] use a dataset that has fucking scipy preprocessing
% * ausrechnen "um so gut zu sein wie die, müsste der datensatz größe xyz haben"
% * Die standard-whackities des datensatzes, dass halt viele nur sind "Tutoren sind: Susi Sorglos Willi Wacker", oder "Findet statt in Raum XYZ", oder dass alle Sprachkurse die gleichen Beschreibung haben (beispiel. `....len([i for i in descriptions._descriptions if "kompetenzen entwickelt befahigen akademischen berufstypischen" in i.processed_as_string()]) == 25  ... weil es genau 25 exakt gleiche Beschreibungen gibt, für die Fremdsprachkurse. Deswegen ist up to jede 5-wort-kombination davon ein extracted keyword`)
% * Der Kappa-Score der rankigns vergleicht ist für mich ne kack metric weil ich ebennicht reviews nehme und more-occurences better-candidate heißen -> gucken wie ich stattdessen gute dimensionen und cluster finde (klingt doch so als sei accuracy/f1/... doch wichtig)

\input{include/dataset_table}


% Empirie, auch specifics über den Datensatz

%To write:
% * where does the data come from
% * what size is the data, what is the distribution, ...
% * Preliminary analysis (if I delete all that are shorter than X, it are |Y|..)
% * Does it cluster and look nice?
% * Verteilung der Sprachen
% * Preprocessing in kurzem Fließtext beschreiben - "After throwing out all descriptions shorter than xyz chars, 2323 courses where left. 223 of these were ..."
% * That the type of dataset differs from DESC15 and followups - mainly used movie-dataset consists of concatenated reviews (which means relevant words occur more often!) 
%     (TODO: look/think was die anderen auszeichnet - bei dem placetypedataset ists ja gar kein fließtext sondern direkt ein bag-of-tags)
% Dass mein Datensatz kleiin ist! Bei keinem sonderlichen min-word-per-desc threshold hab ich halt 7588 samples, bei 50 schon nur noch 4123, das ist wirklich little
% Dass auch die Descriptions echt kurz sind! Ich hab rund 8k samples, um das selbe samples-to-threshold verhältnis zu haben wie DESC15 wäre rechnerisch ein wert von 2 bis 25 sinnvoll (wobei man beachten muss das 2 schon richtig kacke ist weil dann die SVM 2 vs 8000 klassifizieren muss and that will never work -> 25 ist minimum), ABER wenn ich dann 25 nehme hab ich nur 2.4k candidates statt the 22k DESC15 aimed at, which also sucks!! --> CONCLUSION: Datensatz scheint zu klein.

The main goal of this thesis was to create a conceptual space of courses, automatically generated by course descriptions.


For that, a dataset of courses and their descriptions was obtained as export from the Stud.IP system as used at the universities of Osnabrück, Hannover and Bremen.
%TODO wait, woher kam der datensatz überhaupt? Tobias hat mir den geschickt, aber kam er zustande im Rahmen von Siddata?

The dataset comes from Johannes' Repo at \url{https://git.siddata.de/jschrumpf/study_behavior_analysis} (requires authentification over UOS!)

\begin{figure}[H]
	\centering
	\includegraphics[width=\figwidth]{graphics/figures/courses_language_distribution.png}
	\slcaption{
		\label{fig:courses_language_distribution}
		Distribution of languages of course descriptions.
		%TODO figure if this is the correct amount of preprocessing/throwout to have done
		Of the 21337 courses left after preprocessing, 18,679 were in german language according to the \textit{langdetect} python-package (for details, see \aref{ap:translating}).
		}
\end{figure}


The faculty is easily obtainable from the dataset, as the first one or two digits of the course ID correspond to it. The distribution of the faculties is depicted in figure \ref{fig:faculty_plot}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\figwidth]{graphics/figures/faculty_plot.png}
	\slcaption{
		\label{fig:faculty_plot}
		Distribution of faculties in the courses
		}
\end{figure}

The purpose of the Neural Network classifier is to check if it is anyhow possible to extract meaningful information from the descriptions: If it is possible to train a classifier on the data that can reasonably predict a qualitative feature, there is enough structure in the data such that the algorithm I'm about to produce can work.
Also, we have a lower bound for useful data: we can just throw away data that cannot be classified!
%TODO: train a second classifier on something else and throw away data that gets classified by neither and inspect it

(-> 91\% test accuracy)

% =============== Besonderheiten vom Siddata-datensatz

....len([i for i in descriptions._descriptions if "kompetenzen entwickelt befahigen akademischen berufstypischen" in i.processed_as_string()]) == 25  ... weil es genau 25 exakt gleiche Beschreibungen gibt, für die Fremdsprachkurse. Deswegen ist up to jede 5-wort-kombination davon ein extracted keyword
(und das obwohl sie verschiedene Namen haben! merging them doesn't make sense but they are almost equal)

% =============== Schreiben zum Thema Datensatz-Vergleich:

...ist es richtig dass nur 6000 verschiedene Terms >= 25 mal vorkommen?! 6000?!
=> auch in groß ist mein datensatz ja noch deutlich kleiner als placetypes, die haben immerhin 22k candidates
--> n-docs: 7596
--> 1-grams >= 25 times: 5054, 1-5-grams >= 25 times: 6717
--> unique 1-grams: 106235

bei placetypes sind es 
* unique 1-grams: 746180, davon 41320 >= 25 mal und 21833 >= 50 mal (their threshold)

--> das verhältnis Anzahl Texte zu Länge Texte ist bei mir halt komplett off 

% =============== 


\subsection{Place-Types}

\begin{itemize}
	\item Took it to be able to compare my results to the ones of \mainalgos
	\item Did NOT do the movies-dataset (also used by all \mainalgos, see \tref{tab:all_datasets}), because version available online does not contain n-grams so it will not be comparable
	\item Also took it to be able to sanity-check if my implementation was correct, which was extremely helpful
	\item Didn't do the openCYC taxonomy bc they say that they don't use one level of the taxonomy consistently but also never explain where they go to which level
\end{itemize}

%TODO: write IN THE ALGORITHM & ARCHITECTURE SECTIONS that I of course tried the placetypes-dataset as sanity-check to find errors - for that dataset, stuff like the good-candidates is known so as long as I don't reach their performances for that dataset I know my code is the problem, but as soon as I reach their performance I can savely say that the actual algorithm is correct and if it's still bad on the siddata dataset it's just not applicable to this kind of data

So, infos from \cite{Derrac2015}:
\begin{itemize}
	\item GeoNames has 667 place-types in 9 categories (403 used)
	\item Foursquare has 435 place-types in 9 top-level categories (391 used))
	\item content: tags of Flickr photos. Photos assumed to be of a type if one of the tags is the name of that type (so they queried for photos with that tag), and then all other tags of that picture make up the BoW.
	\item 22816139 photos, types with less than 1000 photos removed.
\end{itemize}


Also tried the Plactypes-Dataset used by all main-paper-authors. When doing so I noticed that there are definitely duplicates (which are consistently recognized as closest-terms in embedding):
  abandoned rail road and abandoned railroad
  boat yard and boatyard
  coral reef and reef
  court house and courthouse
  grass land and grassland
  sheep fold and sheepfold
  skate park and skatepark
  steak house and steakhouse
  water fall and waterfall
  wind mill and windmill

Next to that, the embedding however also sees very similar ones as very similar, which is a nice sanity-check, eg.

  abandoned farm and abandoned home
  airfield and airport
  airport and airport terminal
  ancient site and archaeological site
  arch and arch bridge
  art gallery and art museum
  coffee house and coffee shop
  aircraft cabin and airplane cabin
  apartment and apartment building
  bank and bank building
  field hockey field and hockey field

\subsection{Other Datasets}

Also tried a dataset of 100.000 coursera course reviews from \url{https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset}. Why? Because it's also eduactional resources, but as it's reviews it seems closer to the movies dataset
See \url{https://www.kaggle.com/roshansharma/coursera-course-reviews} for exploratory analysis of the dataset (there he also has another dataset he writes about, but you cannot merge them unfortunately, so besides course name the only possible task is the rating)
%TODO: I could try to merge it with this one https://www.kaggle.com/siddharthm1698/coursera-course-dataset or another one (see https://www.kaggle.com/mihirs16/coursera-course-data which links names to links, https://www.kaggle.com/search?q=coursera+in%3Adatasets for other places)

Also, there's the Large Movie Review Dataset\footnote{\url{http://ai.stanford.edu/~amaas/data/sentiment/}, \url{https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html}}, also used by \cite{Ager2018}.




\section{Algorithm}

\input{Chapters/sections/methods_algorithms}


\section{Architecture}
\label{sec:architecture}

As elaborated in \autoref{sec:reproducibility}, one of the main motivations for this thesis was to create a publicly available \textit{open-source} version of the algorithm that is easily \textit{understood} and \textit{reproduced}, \textit{adaptable} for other datasets and methods, as well as fast and \textit{scalable}, meaning it can be run maximally efficient on single machines but also on compute clusters, such as the \acrshort{ikw} Grid.
%TODO: Hier schon eindeutig sagen dass es auf ner single machine infeasibly lange läuft und deswegen der ganze Bums fürs Grid nötig war!!

% Main goal: BETTER ARCHITECTURE. Most important things for that: scalability, modularity, transparency, reproducibility, understandability, objectiveness, systematicacy, sustainability, adaptability
% describing this because I want to encourage extending the code etc and for that not only the algorithm but also the architecture should be described 
% and I think that was successful: This codebase contains everything and finally fulfills code-standards! 

This section will outline the architecture that was developed in order to achieve the aforementioned results. The resulting pipeline is the result of a lot of trial-end-error, but fulfills all of the aformentioned criteria, dealing with vastly differing sizes and kinds of datasets, minimizing runtime wherever feasible and allowing for a multitude of parameters at every step of the process. %TODO: don't like this paragraph, lieber später nohcmal auf die design principles eingehen und sagen dass sie alle fulfilled sind.

The rest of this section will go into further detail regarding the architecture of the resulting code-base. \todoparagraph{it will start with xyz and then asdf and then yaddayadda}

\subsection{Implementation}

The associated program is written by the author of this work and licensed under the \emph{GNU General Public License} (GNU GPLv3). The source code is written in the Python Programming Language and available digitally on GitHub\footnote{Source code: \url{https://github.com/cstenkamp/derive_conceptualspaces/}\\Source of this Document: \url{https://github.com/cstenkamp/MastersThesisText/}\\Compiled Document: \url{https://nightly.link/cstenkamp/MastersThesisText/workflows/create_pdf_artifact/master/Thesis.zip}}. In order to ensure that no work after the deadline is considered, it is referred to the signed commits \todoparagraph{COMMIT} and \todoparagraph{COMMIT}. 

The code is a proper python-package that can be installed into any Python 3.10 environment using for example python's default package manager pip:\\ \mytokens{pip install git+https://github.com/cstenkamp/derive_conceptualspaces.git@main}~ .\\ It can then be run using \mytokens{python -m derive_conceptualspace <COMMAND>} \footnote{The command \mytokensfnote{python -m derive_conceptualspace --help} gives a peak into what sub-commands can be used}. For more information on how to invoke the code base with these commands \todoparagraph{it is referred to} \autoref{AppendixA} %TODO: once that appendix is not autoconverted markdown ref to the section of appendix a

To guarantee reusability of this code-base, there is also a \emph{Dockerfile}\footnote{{\url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/Dockerfile}}} that allows to easily create a \emph{Docker-Container\footnote{\url{https://www.docker.com/resources/what}}} from it\footnote{A Container can be thought of as a lightweight virtual operating system, in which the codebase is bundled together with all required dependencies, libraries and configurations, enabling users install this software on any system without having to download or install anything besides this container, irrespective of operating system or software versions on the host \acrshort{os}. For more info about the container, it is referred to \url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/doc/docker_intro.md}.}.

\subsection{Modularity}

The developed algorithm consists of clearly divisible components (as demonstrated in \autoref{fig:dependency_graph}), where the runtime for each of the steps is roughly in the same order of magnitude. All of the aforementioned (\autoref{sec:algorithm_steps}) steps are itself algorithms with many hyperparameters each. Furthermore, the framework described here does not even require particular algorithms for the individual components, but rather a classes of algorithms like \emph{dimensionality reduction techniques}. This means that in practice, there is a combinatorical explosion of settings and hyperparameters that must be experimented with in order to find the best-performing one. Because of the clear modularity of the algorithm however, many of these become only relevant in a later step of the pipeline. Due to this, it is reasonable to make the architecture as modular as possible, storing interim results before every step, such that two parameter-combinations that differ only in \eg the fourth step of the pipeline can share the intermediate results up to that point, keeping the required computation to a minimum. 

The design principle of maximal modularity is the cornerstone of the developed pipeline. All of the interim results store the configurations that were required for the respective algorithm (and forward the ones of the input-files they transformed), as well as the created output and plots. When there are different possible algorithms for a step, it is ensured that its result are of the same format, as required by the next step. Many of the individual steps generate additional plots that can be used as sanity-checks to quickly inspect if the results so far are reasonable.

\subsubsection{Workflow Management}

A pipeline where multiple intermediate files for different parameter-combinations are created introduces the problem of \emph{dependency resolution}: Ultimately, there is supposed to be one final file for every combination. This file however relies on intermediate files, which in turn rely on intermediate files. To resolve these dependencies, there are many existing \textbf{Workflow Management Systems}. For this thesis, \textbf{Snakemake}\footnote{\url{https://snakemake.readthedocs.io/en/stable/}} \cite{Molder2021a} seemed the right choice.

Snakemake defines a domain-specific language used to describe workflows, which extends the santax of python. Using this (easily comprehensible), the workflow is described in terms of individual \textbf{rules}, which consist of several 


\subsection{Modes of Execution}

It is possible to run the full pipeline for individual files as well as for a set of parameter-configurations specified via configuration files, but also possible to run individual steps to inspect or debug the respective steps. To inspect and compare results it is possible to load all available parameter-configurations, as well as the complete history for a certain combination, listing the generated outputs and metrics. Further, individual configurations can be loaded in \emph{Jupyter Notebooks} to generate and export plots and tables from them (like the ones used in this text).




\begin{widepage}
	\lstconsolestyle
	\lstinputlisting[
		caption={Sample terminal output when running it on the grid}, 
		label={lst:joblog},
		float,
		floatplacement=h!,
		]{listings/joblog\_grid.txt}
	\lstdefaultstyle
\end{widepage}

% \includeMD{pandoc_generated_latex/chapter_methods_section_architecture}
