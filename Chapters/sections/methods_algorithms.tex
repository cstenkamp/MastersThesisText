% * Dataset + Algo Section "teilweise unveständlich"
% * Verhältnis von Datensatz und Algorithmus wird nicht klar

\todoparagraph{If not explicitly stated otherwise, hyperparameter also refers to exchangeable algorithm components such as which quantification is used}

This section will go into detail about this main algorithm, plus the two other papers that according to my research are good additions to it (in terms of helping to understand it as well as improving on some stuff in it). That we are focusing on only those three papers should by no means imply that they are the only ones in that regard\footnote{see \autoref{sec:otherwork} - \cite{VISR12} and their tag genome, the fact that the algorithm detailed here is basically only one step in \cite{Alshaikh2019}, Gärdenfors himself suggested that one may use self-organizing maps instead of classical AI/NLP algorithms.}.

\todoparagraph{Regarding ager and alshaikh:}

The implementation of this thesis replicates and extends the algorithm proposed by \textcite{Derrac2015} with some novel contributions to deal with the given dataset. Further, some improvements from the works of \textcite{Ager2018} and \textcite{Alshaikh2020} are incorporated, who also replicated and improved the original algorithm\footnote{All three of these works share Prof Steven Schokaert as last author.}.

\subsection{Core Idea}

% The core idea of the algorithm is to unsupervisedly find a a set of features which can be modelled as directions for a vector-space representation of the respetive entities.
The main goal of the algorithm is to unsupervisedly use the considered text-corpora associated with the respective entities from a certain domain % descriptions, reviews, ...
to embed the entities into a vector-space where the axes correspond to human concepts, % "concepts" meaning attributes and what-was-the-other-again, according to CS lingo corresponding to nouns and adjectives, TODO: see above where I described that already.
allowing a \textit{feature-based} representation of them - a high-dimensional vector that numerically encodes the degree % protypicality
to which the entity corresponds to a number of appropriate dimensions. % Das haupt-ziel der algorithmen ist es, am ende die entities "feature-based" darstellen zu können, also als high-dim-vector mit floats per human-interpretable dimension. 
% Alshaikh2020 hat dafür wegen den subfeatures noch kleine specials
%TODO: Johannes said "Consider breaking this sentence into multiple sentences to enhance readability."
\newline

The general idea to achieve that is as follows: First, the entities %TODO: at this point already defined that entities = texts = descriptions/concatenatedreviews/... 
are embedded as fixed-dimensional vectors\footnote{This already softens the definition of a conceptual space, as the considered entities are modelled as vectors instead of regions.}. % entities were supposed to be regions, however here we assume vectors because it's computationally a lot easier (TODO: are the reasons mentioned elsewhere? also the distiction between types and tokens?)
To allow for the types of reasoning mentioned in Section \ref{sec:cs_reasoning}, %TODO: section where the entire mapping from logic-reasoning to CS-reasoning is explained, mostly from DESC15 and Gärdenfors himself
it is embedded into metric spaces where the concepts of direction and distance are well-defined. \gencite{Derrac2015} original algorithm uses MDS (see \ref{MDS}) for this matter, which enforces equal distances\footnote{however as shown by \textcite{Mikolov:Regularities}, such things are also possible with neural word-embedding techniques such as word2vec \cite{Mikolov2013}.}%TODO: link the distance-matrix-notepad FROM MY REPO in nbviewer+binder
. Additionally, words or phrases from the text are extracted as candidates for the names of the semantic dimensions. The underlying assumption is that \q{words describing semantically meaningful features can be identified by learning for each candidate word $w$ a linear classifier which separates the embeddings of entities that have $w$ in their description from the others} \cite[3574]{Alshaikh2020}. The better the performance of that classifier according to a chosen metric, the more evidence there is that $w$ describes a semantically meaningful feature. 
% * from Alshaikh2020: "Their core assumption is that words describing semantically meaningful features can be identified by learning for each candi- date word w a linear classifier which separates the embeddings of entities that have w in their description from the others. The performance of the classifier for w then tells us to what extent w describes a semantically meaningful feature"
% TODO: make the direct quote shorter, such things should only be done "to support an argument", which I don't here.
In a final step, the candidate-words are clustered according to their similarity to find a fixed set of \emph{semantic directions}. A representative term for the directions is selected, and the entities are re-embedded into a new space comprised of these dimensions, where the individual vector-components correspond to the ranking of an entity with respect to these dimensions.
% * from Alshaikh2020: "The learned vectors will be referred to as feature directions to emphasize the fact that only the ordering induced by the dot product d_i · e matters"
\newline


Many of the mentioned components don't refer to a specific algorithms, and the existing implementations \mainalgos differ in many of these implementations. The rest of this section goes into further detail for each of these components, and an overview of the components supported in the respective implementations is given in \tref{tab:compared_algos}. Further, configuration files to enable exactly the respective components of the papers \mainalgos for the codebase of this thesis are listed in \aref{ap:yamls_for_origalgos}.


% #########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

\subsection{Steps of the algorithm}

\label{sec:algorithm_steps}

% TODO: THIS from \cite{Turney2010} is a very fucking good explanation!!!

% After the text has been tokenized and (optionally) normalized and annotated, the first step is to generate a matrix of frequencies. Second, we may want to adjust the weights of the elements in the matrix, because common words will have high frequencies, yet they are less informative than rare words. Third, we may want to smooth the matrix, to reduce the amount of random noise and to fill in some of the zero elements in a sparse matrix. Fourth, there are many different ways to measure the similarity of two vectors. Lowe (2001) gives a good summary of mathematical processing for word–context VSMs.
% He decomposes VSM construction into a similar four-step process: calculate the frequencies, transform the raw frequency counts, smooth the space (dimensionality reduction), then calculate the similarities.

\todoparagraph{CONSIDER JOHANNES' NOTES}
% TODO: Johannes' Notes:
% Consider restructuring the first two paragraphs: You first introduce the first two steps but then do not follow up with the next steps (Unless they are combined under the mentioned Lowe framework in which case you need to make this explicit).
% I like the structured step listing later on, maybe you could build this section around that.

The considered algorithm first embeds the collection of texts into a vector space, to afterwards extract important features from this space using linear classifiers. The second step is an original idea of \cite{Derrac2015}, however creating vector space embeddings from texts is a very popular technique, used for many tasks in computational linguistics \cite{Mikolov:Regularities,Mikolov2013a,Guo,Lowe,Turney2010}. %TODO: irgendwo bin ich ja noch auf word2vec und den ganzen bums eingegangen blabla, nochmal schreiben dass es hier um classical AI geht und ebennicht die sahcen wie word2vec und glove
%TODO: glove und fasttext cite?!

 \textcite{Lowe} conceived a general framework to construct vector spaces from texts, splitting the process into the steps of first counting the token frequencies, then transforming the raw counts into more useful \gls{quant} measures (see \ref{sec:word_count_techniques}) %adjust the weights of the elements in the matrix, because common words will have high frequencies yet are less informative than rare words.
 %TODO: talk about a fucking DOC-TERM-MATRIX, A MATRIX OF FREQUENCIES!!!
  and smoothing the space using dimensionality reduction, before calculating the similarities on the resulting embedding. While the considered algorithm requires more steps before calculating the similarities to allow for domain-specific and more complex forms of reasoning, it also adheres to this structure, as will be shown in section~\nameparanref{sec:generate_vectorspaces}.

The core idea of the algorithm is to (unsupervised, data-driven) find a a set of features which can be modelled as directions for a vector-space representation of the respective entities. For that, the steps are:

\label{sec:algorithmsteps}
\begin{enumerate}
	\item[\saveref{sec:algo_preproc}{1.}] preprocess the descriptions using default techniques and create a bag-of-words representation for the texts
	\item[\saveref{sec:extract_cands}{2.}] extract candidate features from these texts (easist variant is to just consider each sufficiently frequent word as candidate)
	\item[\saveref{sec:generate_vectorspaces}{3.}] create a fixed-dimensional embedding for the texts (optimally a metric space, optimally based on their dissimilarity)
	\item[\saveref{sec:svm_filter_cands}{4.}] for each candidate term, train a linear classifier to seperate the vector representations of the entities that contain the term vs those that don't. If some metric for this classifier is sufficiently high, assume that the candidate term captures a salient feature - it's direction is then characterized by the normal vector for that hyperplane (for an intuition see \ref{fig:3d_hyperplane_ortho})
	\item[\saveref{sec:generate_vectorspaces}{5.}] Cluster the candidates and re-calculate the directions
	\item[\saveref{sec:generate_vectorspaces}{6.}] (optionally) Post-process the clusters 
	\item[\saveref{sec:generate_vectorspaces}{7.}] Calculate the direction of the cluster from the directions of it's contents to create a new space to embed the entities into
\end{enumerate}

The algorithm is also given as pseudocode in \autoref{ap:algorithm_pseudo}.

% MENTION IN STEPS
%   3) fixed-dimensional embedding: 
%		* RaZb20 tried Doc2Vec instead of MDS and it performed worse!
%   5) if metric, assume salient:
%  		* [AGKS18] haben den candidate-filter-teil konfigurierbar (und sagen bei denen ist accuracy even better than kappa)

\autoref{fig:dependency_graph} shows an automatically exported dependency-graph, displaying the individual steps of the algorithm as done in the codebase of this thesis, also showing where selected important parameters are first used. As explained in \autoref{sec:architecture}, the modularity of the provided architecture allows individual components to be exchanged as needed.
% \todoparagraph{I should also refer to} \autoref{tab:compared_algos} here!


\begin{figure}[htp]
	\begin{center}
	  \includegraphics[width=0.9\textwidth]{dependency_graph.pdf}
	  \caption[Dependency-Graph of the Algorithm]{Dependency-Graph of the Algorithm, displaying the individual steps of the algorithm as well as their dependencies and where selected important parameters are first used. \todoparagraph{Generated using command XYZ}}
	  \label{fig:dependency_graph}
	\end{center}
\end{figure}


\input{include/algorithm_compare_table}

Before looking at the steps in turn, it should be noted that even the preprocessing does not work on completely raw data, but on curated and processed corpora. This processing is however not considered part of the algorithm, as it is very specific to the respective datasets and manual dataset exploration, tweaking settings such that they are best for the respective corpora\footnote{Because of this, it was done in sepearate Jupyter Notebooks, such as \url{https://github.com/cstenkamp/derive_conceptualspaces/blob/main/notebooks/create_datasets/Preprocess_Siddata2022.ipynb} }. %TODO: instead link to nbviewer/binder!!


\subsubsection{Preprocessing\arrowref{sec:algorithmsteps}}

\label{sec:algo_preproc}

A common prerequisit for NLP algorithms is to pre-process the text corpus. The preprocessing itself consists of multiple independent components chained after each other. Which components are necessary also depends on the processed dataset - as for example the \emph{placetypes-dataset} consists of a collection of tags instead of full sentences, tokenizing sentences or removing \glspl{stopword} becomes irrelevant. Other datasets may require additional cleaning or are already available in preprocessed form.
% * some parts of this depend on the dataset 
%     * Some may just require additional cleaning (remove HTML-Tags and such things)
%     * Some are already mostly preprocessed (placetypes is already a bag-of-words so there's not much to do there, and we skip almost all preprocessing-steps)
%     * In course-descriptions, I want some parts of the pre-preprocessing be part of the pipeline, like how we merge descriptions of different iterations of the same course that overlap to a high degree (sentwise-merge vs relative-term-frequencies)

As the main considered dataset of university-courses is highly multilingual (see \autoref{fig:sid_statistics}), one of the first questions that needs to be addressed is how entities of different langauges are handled. The algorithm consists of classical language processing algorithms such as comparing \gls{bow} representation of the entities, which means that the same text in two different languages may result in maximally different representations\footnote{\todoparagraph{another reason that makes checking out word2vec worth it!}}. Because of this, before any other processing, the languages of the entities is checked, such that entities of languages other than the demanded may be either translated, left out or used anyway\footnote{For details about the translation-step, it is referred to Appendix \ref{ap:translating}. It should be noted that professional automatic translation is costly and thus not all texts are available in all languages.}.


The following components are developed for the preprocessing, every one of which can be individually enabled or disabled:
\begin{itemize}
	\item Prepend title and/or subtitle to the entities' associated text \itemtext{useful for the SIDDATA-Dataset, as the titles are often quite long and more descriptive than their descriptions}
	\item Remove HTML-Tags from texts 
	\itemtext{useful for the SIDDATA-Dataset, as it includes descriptions for \glspl{mooc}, which may contain such}
	\item Tokenize sentences 
	\itemtext{such that \glspl{ngram} across sentences are not considered}
	\item Lower-case all words
	\item Remove stop-words
	\itemtext{\todoparagraph{grains of salt for stopwords! Alshaikh2019 just removed too-high-termfreq!}}
	\item Tokenize words
	\item \Gls{lemma}tize words
	\itemtext{\todoparagraph{TODO: Write about details  and HanTa for German etc!}}
	\item Remove diacritics\footnote{\emph{Diacritics} are glyphs added to basic letters, such as accents or German \emph{Umlaute}. Removing them converts for example the letter \emph{ä} to an \emph{a}}
	\item Remove punctuation 
\end{itemize}

The above can be done either be done with proprietary code for all of these steps\footnote{Mostly relying on the python package \emph{nltk} \cite{bird2009natural}}, or using \codeother{sklearn}\footnote{\url{https://scikit-learn.org/stable/}}s \codeother{CountVectorizer} (which is faster, but less configurable), as did \cite{Ager2018}.


% TODO: Regarding Stop-words:

% There are issues with using stop-word lists, see \cite{nothman-etal-2018-stop} ( SkLearn references this paper why their own/stopwordslists in general suck)

% from \cite{Turney2010}: Computing the similarity between all pairs of vectors, described in Section 4.4, is a computationally intensive task. However, only vectors that share a non-zero coordinate must be compared (i.e., two vectors that do not share a coordinate are dissimilar). Very frequent context words, such as the word the, unfortunately result in most vectors matching a non-zero coordinate. Such words are precisely the contexts that have little semantic discrimination power

... The preprocessing of the algorithms of \mainalgos is dependent on the dataset - \autoref{tab:all_datasets} lists the respectively used preprocessing per dataset and author.


\subsubsection{Extract Candidate Terms\arrowref{sec:algorithmsteps}}
\label{sec:extract_cands}
% Section 4.2.1 of Derrac2015

The final result of the algorithm is a metric space in which the individual dimensions (\emph{\glspl{feature}}) correspond to natural-language concepts and attributes (\q{Interpretable directions} \cite{Derrac2015}). The candidates for these features are verbatim phrases extracted from the text-corpus of the \glspl{entity}, which will subsequently be filtered and merged.

In \gencite{Derrac2015} original paper, the selection of phrases to be extracted depends on the dataset: For placetypes-dataset, all sufficiently frequent\footnote{\label{fnote:cand_thresholds}The respective thresholds are listed in \autoref{tab:all_datasets} as ``candidate word threshold''.} 1-grams\footnote{Note that in the case of the place-types dataset, a 1-gram corresponds to all merged words of a tag.} were considered. For the other two datasets, they applied a \gls{pos}-tagger that extracted all sufficiently frequent\footnoteref{fnote:cand_thresholds} \textbf{adjectives, nouns, adjective phrases} and \textbf{noun phrases}, assuming that adjectives would correspond to gradual properties (\eg \textit{violent, funny}) and nouns to topics (\eg the \textit{genre}) \cite[Sec. 4.2.1]{Derrac2015}.

% Their method depended on the dataset - as their placetypes-dataset was just a collection of tags and the number of tags with term-freq >= XYZ (docfreq>2?! hä?) corresponded to their desired number of candidates anyway (around 22k), they just took all of these as candidates. For their movie-reviews-dataset, they considered all nouns, adjectives, nounphrases and adjective-phrases as detected by a POS-tagger. Doing something similar in the scope of this thesis led to suboptimal results, which is why alternative methods were developed
For this step, the implementation of this thesis differs from the original algorithm, as both taking all words as candidate and running a \gls{pos}-tagger led to suboptimal results in previous experiments of me\footnote{To ensure comparability, in the case of the placetypes-dataset the original method of taking all words with a term-frequency of at least 50 was used.}. 
% I would however argue that the difference here doesn't make a relevant difference (TODO: argue that lol)
% Also, while in the paper they claim to use n-grams as candidates, the dataset they publicly published does not contain n-grams (which is why I didn't use that one).

In the codebase the candidate-extraction is split into four consequently excecuted substeps, because depending on the algorithm used to extract the candidates the runtime of the individual components is comparably long and some settings are only relevant in later substeps. The substeps are:
\begin{itemize}
	\item Extracting Candidate Terms
	\item Postprocessing the Candidates
	\item Creating the \gls{doctermmat} for the candidates
	\item Filtering the \gls{doctermmat}
\end{itemize}

As visualized in \autoref{fig:dependency_graph}, these substeps only depend on the preprocessed descriptions, which means they can be run in parallel to the creation of the embedding\footnote{\todoparagraph{Another good reason for cluster exceution!}}.

% This can be done either based on the frequency (meaning all terms with a minimal term-frequency), based on some notion of *importance* (based on scores like tf-idf or ppmi), or by more complex means of figuring out *important* keywords and keyphrases. An example of the latter would be KeyBERT
Three main techniques are implemented to extract candidates from the text-corpus: 
\begin{description}
	\item[By frequency:] consider all phrases that exceed a specified term-frequency (like \cite{Derrac2015}).
	\item[By a \gls{quant}:] consider all phrases that exceed a specified score (\gls{ppmi}, \gls{tf-idf} or pure term-frequency).
	\item[Using \emph{KeyBERT}\cite{grootendorst2020keybert}:] consider phrases whose BERT-embedding \cite{Devlin2019} is most similar to the text they are in\footnote{For more details on KeyBERT, see Appendix~\ref{ap:details_keybert}}.
\end{description}



\subsubsection{Generating Vector Space Embeddings\arrowref{sec:algorithmsteps}}

\label{sec:generate_vectorspaces}
%TODO: in \cite{Turney2010}, this is "Building the frequency matrix", 

In this step, the individual \glspl{entity} are embedded into a fixed-dimensional vector space, making up a \emph{frequency matrix}. It should be noted that while this matrix is a \gls{doctermmat}, it is only an interim result in the algorithm and the calculation of distances and directions will be done on another matrix from a later step.

Embedding words, \glspl{ngram}/phrases or other tokens, as depicted by \cite{Turney2010,Lowe}, generally involves counting the token frequencies, transforming them to get relative frequencies, and performing dimensionality reduction on the resulting matrix. As elaborated upon in \autoref{sec:reasoning}, for \gencite{Derrac2015} algorithm it was a requirement that the space is Euclidian. Another requirement is that the number of dimensions is decidable as hyperparameter to the algorithm. Because of these two requirements, they selected \gls{mds} for dimensionality reduction.

In their algorithm, the \gls{dissimmat} is created using the \emph{normalized angular distances} of the \glspl{bow} of the respective entities. If the strict requirement for a metric space is dropped however, many different algorithms may instead be used at this point - not only different dimensionality reduction methods for the embedding, but also embeddings that do not rely on the distance matrix or even the \gls{bow} at all, like document-embedding-techniques such as \gls{doc2vec} \cite{Le2014} (as \eg used by \cite{Alshaikh2020}).

% TODO: komplettes 4 von \cite{Turney2010}, außerdem der ganze bums von Deerwester!

normalized angular distances according to \cite{Derrac2015}:

\begin{align}
	ang(e_i, e_j) &= \frac{2}{\pi} * \arccos \left( \frac{\vec[m]{v_{e_i}} * \vec[m]{v_{e_j}}} { \lVert \vec[m]{v_{e_i}} \rVert * \lVert \vec[m]{v_{e_j}} \rVert }  \right)  \label{eq:norm_ang_dist} \\
	&= \frac{2}{\pi} * \arccos(1-\cos(\vec[m]{v_{e_i}},\vec[m]{v_{e_j}})) \text{, where $\cos$ is the default cosine-distance} \nonumber
\end{align}



\subsubsection{Filter Candidates by Classifier Performance\arrowref{sec:algorithmsteps}}

\label{sec:svm_filter_cands}

\todoparagraph{In methods muss eindeutig stehen} "wir measuren ja die faithfulness der representation by how good the kappa is. Derrac2015 schreibt dass die kappa nutzen, geben aber keine details, allwoing for a shitton of different ways how to implement that. As that's highly relevant, we try many diffeent values and report them in the results. Details what means what in the ipmlementation details" -> und da steht dann kurze erklärung in ner tabelle


\subsubsection{Clustering the extracted candidates}

An analysis of \cite{Carmel2009} showed that a statistical method to extract features from clustered text corpora identified the labels of human annotators as one of the top five most important terms in only 15\% of cases, implying ``that human labels are not necessarily significant from a statistical perspective" \cite[139]{Carmel2009}
%TODO: die eigentliche Methode (JSD) mehr erklären!!
%(the JSD method for feature selection identifies human labels as “significant” (appearing in the top five most important terms) for only 15% of the categories. This result implies that human labels are not necessarily significant from a statistical perspective.z)





\includeMD{pandoc_generated_latex/3_1_algorithm}